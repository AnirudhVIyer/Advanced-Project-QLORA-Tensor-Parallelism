{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "In this section we'll import the requisite libraries and instantiate a number of objects and variables to configure our training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker                                        # SageMaker Python SDK\n",
    "from sagemaker.pytorch import PyTorch                   # PyTorch Estimator for running pytorch training jobs\n",
    "from sagemaker.debugger import TensorBoardOutputConfig  # Debugger TensorBoard config to log training metrics to TensorBoard\n",
    "import boto3                                            # AWS SDK for Python\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()   # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_key_prefix = \"7-bill\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl -O data/dialogsum.train.jsonl\n",
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.test.jsonl -O data/dialogsum.test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accelerate launch` command has two key parts, the `config.yml` file and the `train.py` script. The `config.yml` file is used to configure the distributed training job. The `train.py` script is the training script that will be launched by the launcher. In this example, we'll use the [ds_zero3.yml](src/train/ds_zero3.yaml) configuration file. The config file enables [DeepSpeed ZeRo Stage3](#https://www.deepspeed.ai/tutorials/zero/) and a number of other optimizations to enable training of large scale models. This file was generated by running `accelerate config --config_file ds_zero3.yml` and then following the on-screen prompts. \n",
    "The [train.py](src/train/train.py) makes use of a number of key libraries to enable training of large models with minimal code changes:\n",
    "- 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) - Configures the distributed training environment and adapts training objects (data loaders, models, optimizers) to the distributed environment\n",
    "- 🤗 [Transformers](https://huggingface.co/docs/transformers/index) - Provides a number of pre-trained models and utilities for training and evaluating models\n",
    "- 🤗 [PEFT](https://github.com/huggingface/peft) - Provides a number of methods for Parameter Efficient Finetuning(PEFT) of large language models. The [LoRA](https://arxiv.org/pdf/2106.09685.pdf) method will be used to finetune the model\n",
    "- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Provides a number of optimizations to enable training of large models. In this example, we'll use DeepSpeed ZeRO Stage3 to enable training of models with over 1B parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the tesnorboard output directly to S3\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://{bucket}/{s3_key_prefix}/tensorboard\"\n",
    ")\n",
    "\n",
    "image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker'\n",
    "estimator1 = PyTorch(\n",
    "    source_dir = \"src/train\",\n",
    "    entry_point=\"acc_launcher.py\",\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    framework_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    hyperparameters = {\n",
    "    \"training_script\": \"train_qlora.py\",\n",
    "    \"config_file\": \"qlora.yaml\",\n",
    "    \"seed\": 100,\n",
    "    \"model_name_or_path\": \"NousResearch/Llama-2-7b-hf\",\n",
    "    \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
    "    \"chat_template_format\": \"chatml\",\n",
    "    \"add_special_tokens\": False,\n",
    "    \"append_concat_token\": False,\n",
    "    \"splits\": \"train,test\",\n",
    "    \"max_seq_len\": 2048,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"bf16\": True,\n",
    "    \"packing\": True,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_ratio\": 0.0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"use_reentrant\": True,\n",
    "    \"dataset_text_field\": \"content\",\n",
    "    \"use_flash_attn\": False,\n",
    "    \"use_peft_lora\": True,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": \"all-linear\",\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_nested_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
    "    \"report_to\":\"none\",\n",
    "\n",
    "},\n",
    "\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-04-12-00-16-38-903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 00:16:39 Starting - Starting the training job...\n",
      "2024-04-12 00:16:48 Pending - Training job waiting for capacity...\n",
      "2024-04-12 00:17:14 Pending - Preparing the instances for training........................\n",
      "2024-04-12 00:21:36 Downloading - Downloading input data...\n",
      "2024-04-12 00:22:06 Downloading - Downloading the training image........................\n",
      "2024-04-12 00:25:57 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-12 00:27:09,184 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-12 00:27:09,279 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-12 00:27:09,286 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-12 00:27:09,288 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-12 00:27:10,644 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers to /tmp/pip-req-build-exql0hla\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-exql0hla\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers to commit 0bd58f1ce0573c0e3269de4215a17d318add49b9\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/accelerate (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-6ms261t9\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-6ms261t9\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/accelerate to commit c0a37015e3e4f2ab78a35efd6462e5b5140cf46e\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/peft (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/peft to /tmp/pip-req-build-rwivsyu6\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-rwivsyu6\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/peft to commit 31c884e93469dd1391bb54eb1468311c38bbccac\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/trl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/trl to /tmp/pip-req-build-2wq7f4jz\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-2wq7f4jz\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/trl to commit 995f1174da89da4dc0ad04c45de11d67b6d06274\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/datatrove.git (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/datatrove.git to /tmp/pip-req-build-8grhwi4c\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/datatrove.git /tmp/pip-req-build-8grhwi4c\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/datatrove.git to commit 40a3a1b0eb9e000abba1a0e1fd67873b49fdad65\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting unsloth[conda]@ git+https://github.com/unslothai/unsloth.git (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-hv7gmkmi/unsloth_7a7313277bd545c9b19bcde15757a617\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-hv7gmkmi/unsloth_7a7313277bd545c9b19bcde15757a617\u001b[0m\n",
      "\u001b[34mResolved https://github.com/unslothai/unsloth.git to commit 4606443b77f98a624896d4ca50710255d8436d86\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting PyGithub (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyGithub-2.3.0-py3-none-any.whl (354 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.4/354.4 kB 23.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 60.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 68.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.6.1)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 104.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 88.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (3.7.2)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 100.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 99.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xformers (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.5/222.5 MB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting hf_transfer (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 108.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 70.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 108.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 61.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl==0.8.3.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.0/102.0 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.12.2 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 37.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting humanize (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.9.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.8/126.8 kB 27.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting loguru>=0.7.0 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 13.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 62.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynacl>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.5.0)\u001b[0m\n",
      "\u001b[34mCollecting pyjwt[crypto]>=2.4.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.26.15)\u001b[0m\n",
      "\u001b[34mCollecting Deprecated (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 42.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 11)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 12)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 92.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (8.1.4)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 42.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 267.1/267.1 kB 50.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 115.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 113.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (4.41.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 23)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting torch>=1.10.0 (from accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 1.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.0.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mUsing cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 76.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 81.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 16.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 25.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting triton==2.2.0 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 11.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 82.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting fsspec[http]>=2021.05.0 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 39.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 50.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 37.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 50.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub->-r requirements.txt (line 8)) (41.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4)) (13.4.2)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 16)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10 (from Deprecated->PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 19.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wheel>=0.42.0 (from unsloth[conda]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.3.dev0->-r requirements.txt (line 4)) (0.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, accelerate, peft, trl, datatrove, unsloth\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8888190 sha256=7aba95ffef8aa1ad325b0ed70cec10d1123db245c43893b33f6f164cc6d11099\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for accelerate: filename=accelerate-0.30.0.dev0-py3-none-any.whl size=297653 sha256=17834a781d2946b162875ee9d5c32dc7879b2276e22f2e9c7dc4177f5805b730\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/f6/c7/9d/1b8a5ca8353d9307733bc719107acb67acdc95063bba749f26\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for peft: filename=peft-0.10.1.dev0-py3-none-any.whl size=202303 sha256=2c2422a7959b0d655c63b18d4ad90ec276ddf2691e5e49048003afb43bd593a0\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for trl: filename=trl-0.8.3.dev0-py3-none-any.whl size=244192 sha256=8302ee807f866cdab97eeaaf2d45e8d93c7a6b9cfdb1bfe93bf3b5c9ae984a5d\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/6a/aa/56/d64d9ae3521350622f9325fdc3bccb4dd3d3ec1c1d8e917400\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for datatrove: filename=datatrove-0.0.1-py3-none-any.whl size=16639941 sha256=20bcbea1d16dd0d12b36b53b0aa296760752878ece9496229889e664b318cd5c\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/8a/74/96/5af76f3e0504a1ed76602ea64ffadeb493297b90d375394cd0\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for unsloth: filename=unsloth-2024.4-py3-none-any.whl size=96655 sha256=ca927c2f3f74a830480a1f3ee8a7e6d7cf0f86b14f0abb3c46ae466fbc610503\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-z130u2d2/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers accelerate peft trl datatrove unsloth\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, appdirs, xxhash, wrapt, wheel, unsloth, typing-extensions, triton, tensorboard-data-server, smmap, shtab, setproctitle, sentry-sdk, safetensors, regex, pyjwt, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown, loguru, humanize, hf_transfer, grpcio, fsspec, frozenlist, docstring-parser, docker-pycreds, async-timeout, absl-py, yarl, tiktoken, tensorboard, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, huggingface-hub, gitdb, Deprecated, aiosignal, tyro, tokenizers, nvidia-cusolver-cu12, GitPython, datatrove, aiohttp, wandb, transformers, torch, PyGithub, xformers, datasets, bitsandbytes, accelerate, trl, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: wheel\u001b[0m\n",
      "\u001b[34mFound existing installation: wheel 0.40.0\u001b[0m\n",
      "\u001b[34mUninstalling wheel-0.40.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled wheel-0.40.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.24.4\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.24.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.24.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.6.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.6.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.6.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.20.3\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Deprecated-1.2.14 GitPython-3.1.43 PyGithub-2.3.0 absl-py-2.1.0 accelerate-0.30.0.dev0 aiohttp-3.9.4 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.18.0 datatrove-0.0.1 docker-pycreds-0.4.0 docstring-parser-0.16 evaluate-0.4.1 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 grpcio-1.62.1 hf_transfer-0.1.6 huggingface-hub-0.22.2 humanize-4.9.0 loguru-0.7.2 markdown-3.6 multidict-6.0.5 nltk-3.8.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 peft-0.10.1.dev0 pyarrow-hotfix-0.6 pyjwt-2.8.0 regex-2023.12.25 responses-0.18.0 safetensors-0.4.2 sentencepiece-0.2.0 sentry-sdk-1.45.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.2 transformers-4.40.0.dev0 triton-2.2.0 trl-0.8.3.dev0 typing-extensions-4.11.0 tyro-0.8.3 unsloth-2024.4 wandb-0.16.6 wheel-0.43.0 wrapt-1.16.0 xformers-0.0.25.post1 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,481 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,481 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,610 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,713 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,815 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,824 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_special_tokens\": false,\n",
      "        \"append_concat_token\": false,\n",
      "        \"bf16\": true,\n",
      "        \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "        \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
      "        \"chat_template_format\": \"chatml\",\n",
      "        \"config_file\": \"qlora.yaml\",\n",
      "        \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
      "        \"dataset_text_field\": \"content\",\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 8,\n",
      "        \"lora_target_modules\": \"all-linear\",\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 1.0,\n",
      "        \"max_seq_len\": 2048,\n",
      "        \"model_name_or_path\": \"NousResearch/Llama-2-7b-hf\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
      "        \"packing\": true,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"report_to\": \"none\",\n",
      "        \"seed\": 100,\n",
      "        \"splits\": \"train,test\",\n",
      "        \"training_script\": \"train_qlora.py\",\n",
      "        \"use_4bit_quantization\": false,\n",
      "        \"use_flash_attn\": false,\n",
      "        \"use_nested_quant\": true,\n",
      "        \"use_peft_lora\": true,\n",
      "        \"use_reentrant\": true,\n",
      "        \"warmup_ratio\": 0.0,\n",
      "        \"weight_decay\": 0.0001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-12-00-16-38-903\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-12-00-16-38-903/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"acc_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"acc_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-7b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":false,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":true,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=acc_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=acc_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-12-00-16-38-903/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-7b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":false,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":true,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-12-00-16-38-903\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-12-00-16-38-903/source/sourcedir.tar.gz\",\"module_name\":\"acc_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"acc_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_special_tokens\",\"False\",\"--append_concat_token\",\"False\",\"--bf16\",\"True\",\"--bnb_4bit_compute_dtype\",\"bfloat16\",\"--bnb_4bit_quant_storage_dtype\",\"bfloat16\",\"--chat_template_format\",\"chatml\",\"--config_file\",\"qlora.yaml\",\"--dataset_name\",\"smangrul/ultrachat-10k-chatml\",\"--dataset_text_field\",\"content\",\"--evaluation_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0001\",\"--logging_strategy\",\"steps\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"8\",\"--lora_target_modules\",\"all-linear\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"1.0\",\"--max_seq_len\",\"2048\",\"--model_name_or_path\",\"NousResearch/Llama-2-7b-hf\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"llama-sft-qlora-dsz3\",\"--packing\",\"True\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--report_to\",\"none\",\"--seed\",\"100\",\"--splits\",\"train,test\",\"--training_script\",\"train_qlora.py\",\"--use_4bit_quantization\",\"False\",\"--use_flash_attn\",\"False\",\"--use_nested_quant\",\"True\",\"--use_peft_lora\",\"True\",\"--use_reentrant\",\"True\",\"--warmup_ratio\",\"0.0\",\"--weight_decay\",\"0.0001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_SPECIAL_TOKENS=false\u001b[0m\n",
      "\u001b[34mSM_HP_APPEND_CONCAT_TOKEN=false\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_COMPUTE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_QUANT_STORAGE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE_FORMAT=chatml\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=qlora.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=smangrul/ultrachat-10k-chatml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_TEXT_FIELD=content\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_TARGET_MODULES=all-linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=NousResearch/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=llama-sft-qlora-dsz3\u001b[0m\n",
      "\u001b[34mSM_HP_PACKING=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=none\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=100\u001b[0m\n",
      "\u001b[34mSM_HP_SPLITS=train,test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=train_qlora.py\u001b[0m\n",
      "\u001b[34mSM_HP_USE_4BIT_QUANTIZATION=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_NESTED_QUANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_PEFT_LORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_REENTRANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 acc_launcher.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --config_file qlora.yaml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-7b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --training_script train_qlora.py --use_4bit_quantization False --use_flash_attn False --use_nested_quant True --use_peft_lora True --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m2024-04-12 00:29:30,854 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcommand = accelerate launch --config_file qlora.yaml train_qlora.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-7b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --use_4bit_quantization False --use_flash_attn False --use_nested_quant True --use_peft_lora True --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:33,566] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:33,566] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:33,566] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:33,566] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 34.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 4.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mUsing cached pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mUsing cached pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mUsing cached pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400401 sha256=32cec83ac15fe8db3789f11bdb4cccf7c6f5570eeaccc824dca38052d610cbb0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400401 sha256=8b9d887499dbbc131ad3a96cacef35c4418231e37e09eac021beb0d2b0ea13ed\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400406 sha256=77fce2d9a6226d2ec943612dd3bf7fdb7b082101ac2fa37ec1b7d5141392e321\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400400 sha256=1906da8896b97e760be207454c3396a22afd9f73a323f56a7c6b05d4453442a8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mERROR: Can't roll back deepspeed; was not uninstalled\u001b[0m\n",
      "\u001b[34mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/deepspeed/compression/basic_layer.py'\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mError installing packages: Command '['/opt/conda/bin/python3.10', '-m', 'pip', 'install', '--upgrade', 'deepspeed']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.3.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:54,416] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.3.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:54,619] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:54,927] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,058] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,058] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.3.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.3.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,293] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,348] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,593] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:29:55,715] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:39<00:39, 39.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:39<00:39, 39.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:39<00:39, 39.94s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:39<00:39, 39.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 24.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:53<00:00, 24.64s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:53<00:00, 26.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:53<00:00, 24.61s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:53<00:00, 26.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 24.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.15s/it]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m[2024-04-12 00:30:51,600] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.44s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr12_00-29-54_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=3,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr12_00-29-55_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=2,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr12_00-29-54_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr12_00-29-55_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/524 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|██████████| 524/524 [00:00<00:00, 936kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/35.2M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 10.5M/35.2M [00:00<00:00, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 35.2M/35.2M [00:00<00:00, 157MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/7.08M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 7.08M/7.08M [00:00<00:00, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 7.08M/7.08M [00:00<00:00, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  60%|██████    | 6000/10000 [00:00<00:00, 49194.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 100%|██████████| 10000/10000 [00:00<00:00, 51108.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 100%|██████████| 2000/2000 [00:00<00:00, 57266.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|█         | 1000/10000 [00:00<00:01, 8307.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|█         | 1000/10000 [00:00<00:01, 8164.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|███       | 3000/10000 [00:00<00:00, 9896.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|███       | 3000/10000 [00:00<00:00, 9736.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 5000/10000 [00:00<00:00, 10297.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 5000/10000 [00:00<00:00, 10131.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 7000/10000 [00:00<00:00, 10524.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 7000/10000 [00:00<00:00, 10314.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 9000/10000 [00:00<00:00, 10694.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|█         | 1000/10000 [00:00<00:01, 8186.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10000/10000 [00:00<00:00, 10505.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 9000/10000 [00:00<00:00, 10461.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|███       | 3000/10000 [00:00<00:00, 9950.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10000/10000 [00:00<00:00, 10286.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11261.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11159.80 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 5000/10000 [00:00<00:00, 10573.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11147.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11049.26 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 7000/10000 [00:00<00:00, 10915.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 9000/10000 [00:00<00:00, 11174.43 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10000/10000 [00:00<00:00, 10800.17 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  2.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 353 examples [00:00, 879.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 710 examples [00:00, 1584.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:01, 833.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1376 examples [00:01, 1252.35 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1755 examples [00:01, 1682.43 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2187 examples [00:01, 1220.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2495 examples [00:02, 1444.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2836 examples [00:02, 1222.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3187 examples [00:02, 1338.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3535 examples [00:03, 1153.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3772 examples [00:03, 1306.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4000 examples [00:03, 1306.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4375 examples [00:03, 1707.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4604 examples [00:03, 1251.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4975 examples [00:03, 1637.65 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5295 examples [00:04, 830.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5666 examples [00:04, 1123.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6000 examples [00:04, 1237.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6291 examples [00:05, 1118.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6667 examples [00:05, 1461.76 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6848 examples [00:05, 1232.40 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  3.59 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 371 examples [00:00, 1256.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 748 examples [00:00, 2074.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1188 examples [00:00, 1339.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1368 examples [00:01, 1332.63 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mPeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32008, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mtrainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mtrainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054\u001b[0m\n",
      "\u001b[34mtrainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mtrainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 20254720 in 513 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/428 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/428 [00:10<1:12:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/428 [00:14<48:27,  6.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/428 [00:19<40:48,  5.76s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/428 [00:23<37:11,  5.26s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/428 [00:28<35:09,  4.99s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6/428 [00:32<33:54,  4.82s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/428 [00:37<33:05,  4.72s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/428 [00:41<32:31,  4.65s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/428 [00:46<32:07,  4.60s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/428 [00:50<31:50,  4.57s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/428 [00:55<31:36,  4.55s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/428 [00:59<31:26,  4.53s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/428 [01:04<31:17,  4.52s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14/428 [01:08<31:10,  4.52s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 15/428 [01:13<31:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 16/428 [01:17<30:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17/428 [01:22<30:53,  4.51s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18/428 [01:26<30:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 19/428 [01:31<30:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 20/428 [01:35<30:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 21/428 [01:40<30:34,  4.51s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 22/428 [01:44<30:31,  4.51s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 23/428 [01:49<30:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 24/428 [01:53<30:23,  4.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 25/428 [01:58<30:19,  4.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 26/428 [02:02<30:15,  4.52s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 27/428 [02:07<30:10,  4.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/428 [02:11<30:06,  4.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 29/428 [02:16<30:02,  4.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 30/428 [02:20<29:57,  4.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 31/428 [02:25<29:53,  4.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 32/428 [02:29<29:49,  4.52s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 33/428 [02:34<29:44,  4.52s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 34/428 [02:38<29:39,  4.52s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 35/428 [02:43<29:34,  4.52s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 36/428 [02:47<29:30,  4.52s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 37/428 [02:52<29:24,  4.51s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 38/428 [02:57<29:32,  4.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 39/428 [03:01<29:23,  4.53s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 40/428 [03:06<29:16,  4.53s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 41/428 [03:10<29:09,  4.52s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 42/428 [03:15<29:03,  4.52s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 43/428 [03:19<29:07,  4.54s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 44/428 [03:24<28:59,  4.53s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 45/428 [03:28<28:52,  4.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 46/428 [03:33<28:46,  4.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 47/428 [03:37<28:40,  4.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 48/428 [03:42<28:35,  4.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 49/428 [03:46<28:30,  4.51s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 50/428 [03:51<28:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 51/428 [03:55<28:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 52/428 [04:00<28:16,  4.51s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 53/428 [04:04<28:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 54/428 [04:09<28:08,  4.52s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 55/428 [04:13<28:04,  4.52s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 56/428 [04:18<28:00,  4.52s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 57/428 [04:22<27:56,  4.52s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 58/428 [04:27<27:51,  4.52s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 59/428 [04:31<27:47,  4.52s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 60/428 [04:36<27:43,  4.52s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 61/428 [04:40<27:37,  4.52s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 62/428 [04:45<27:32,  4.52s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 63/428 [04:49<27:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 64/428 [04:54<27:22,  4.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 65/428 [04:59<27:18,  4.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 66/428 [05:03<27:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 67/428 [05:08<27:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 68/428 [05:12<27:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 69/428 [05:17<26:59,  4.51s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 70/428 [05:21<26:56,  4.51s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 71/428 [05:26<26:51,  4.51s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 72/428 [05:30<26:47,  4.51s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 73/428 [05:35<26:42,  4.51s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 74/428 [05:39<26:38,  4.52s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 75/428 [05:44<26:34,  4.52s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 76/428 [05:48<26:29,  4.52s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 77/428 [05:53<26:25,  4.52s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 78/428 [05:57<26:20,  4.52s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 79/428 [06:02<26:16,  4.52s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 80/428 [06:06<26:11,  4.52s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 81/428 [06:11<26:07,  4.52s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 82/428 [06:15<26:02,  4.51s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 83/428 [06:20<25:57,  4.52s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 84/428 [06:24<25:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 85/428 [06:29<25:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 86/428 [06:33<25:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 87/428 [06:38<25:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 88/428 [06:42<25:34,  4.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 89/428 [06:47<25:29,  4.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 90/428 [06:51<25:24,  4.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 91/428 [06:56<25:20,  4.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 92/428 [07:00<25:16,  4.51s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 93/428 [07:05<25:11,  4.51s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 94/428 [07:09<25:07,  4.51s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 95/428 [07:14<25:03,  4.51s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 96/428 [07:18<24:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 97/428 [07:23<24:54,  4.52s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 98/428 [07:27<24:49,  4.51s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 99/428 [07:32<24:45,  4.51s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 100/428 [07:37<24:40,  4.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 101/428 [07:41<24:36,  4.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 102/428 [07:46<24:31,  4.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 103/428 [07:50<24:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 104/428 [07:55<24:22,  4.51s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 105/428 [07:59<24:18,  4.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 106/428 [08:04<24:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 107/428 [08:08<24:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 108/428 [08:13<24:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 109/428 [08:17<23:59,  4.51s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 110/428 [08:22<23:55,  4.51s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 111/428 [08:26<23:50,  4.51s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 112/428 [08:31<23:46,  4.51s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 113/428 [08:35<23:42,  4.51s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 114/428 [08:40<23:37,  4.51s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 115/428 [08:44<23:32,  4.51s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 116/428 [08:49<23:28,  4.51s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 117/428 [08:53<23:24,  4.51s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 118/428 [08:58<23:19,  4.51s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 119/428 [09:02<23:15,  4.52s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 120/428 [09:07<23:10,  4.52s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 121/428 [09:11<23:06,  4.52s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 122/428 [09:16<23:01,  4.51s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 123/428 [09:20<22:56,  4.51s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 124/428 [09:25<22:51,  4.51s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 125/428 [09:29<22:47,  4.51s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 126/428 [09:34<22:42,  4.51s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 127/428 [09:38<22:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 128/428 [09:43<22:33,  4.51s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 129/428 [09:47<22:28,  4.51s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 130/428 [09:52<22:24,  4.51s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 131/428 [09:56<22:19,  4.51s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 132/428 [10:01<22:15,  4.51s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 133/428 [10:05<22:10,  4.51s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 134/428 [10:10<22:05,  4.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 135/428 [10:14<22:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 136/428 [10:19<21:56,  4.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 137/428 [10:23<21:51,  4.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 138/428 [10:28<21:47,  4.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 139/428 [10:32<21:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 140/428 [10:37<21:40,  4.52s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 141/428 [10:42<21:36,  4.52s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 142/428 [10:46<21:31,  4.52s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 143/428 [10:51<21:27,  4.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 144/428 [10:55<21:22,  4.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 145/428 [11:00<21:17,  4.51s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 146/428 [11:04<21:12,  4.51s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 147/428 [11:09<21:07,  4.51s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 148/428 [11:13<21:08,  4.53s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 149/428 [11:18<21:08,  4.55s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 150/428 [11:22<20:59,  4.53s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 151/428 [11:27<20:52,  4.52s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 152/428 [11:31<20:45,  4.51s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 153/428 [11:36<20:39,  4.51s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 154/428 [11:40<20:39,  4.53s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 155/428 [11:45<20:32,  4.52s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 156/428 [11:49<20:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 157/428 [11:54<20:20,  4.50s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 158/428 [11:58<20:15,  4.50s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 159/428 [12:03<20:09,  4.50s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 160/428 [12:07<20:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 161/428 [12:12<19:59,  4.49s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 162/428 [12:16<19:55,  4.49s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 163/428 [12:21<19:50,  4.49s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 164/428 [12:25<19:46,  4.49s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 165/428 [12:30<19:41,  4.49s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 166/428 [12:34<19:37,  4.49s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 167/428 [12:39<19:33,  4.50s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 168/428 [12:43<19:29,  4.50s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 169/428 [12:48<19:24,  4.50s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 170/428 [12:52<19:19,  4.49s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 171/428 [12:57<19:15,  4.49s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 172/428 [13:01<19:10,  4.49s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 173/428 [13:06<19:06,  4.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 174/428 [13:10<19:01,  4.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 175/428 [13:15<18:57,  4.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 176/428 [13:19<18:52,  4.50s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 177/428 [13:24<18:48,  4.50s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 178/428 [13:28<18:44,  4.50s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 179/428 [13:33<18:39,  4.50s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 180/428 [13:37<18:36,  4.50s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 181/428 [13:42<18:31,  4.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 182/428 [13:46<18:27,  4.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 183/428 [13:51<18:22,  4.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 184/428 [13:55<18:18,  4.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 185/428 [14:00<18:13,  4.50s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 186/428 [14:04<18:08,  4.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 187/428 [14:09<18:04,  4.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 188/428 [14:13<18:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 189/428 [14:18<17:55,  4.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 190/428 [14:22<17:51,  4.50s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 191/428 [14:27<17:47,  4.50s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 192/428 [14:31<17:42,  4.50s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 193/428 [14:36<17:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 194/428 [14:40<17:34,  4.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 195/428 [14:45<17:30,  4.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 196/428 [14:49<17:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 197/428 [14:54<17:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 198/428 [14:58<17:17,  4.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 199/428 [15:03<17:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 200/428 [15:07<17:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 201/428 [15:12<17:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 202/428 [15:16<17:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 203/428 [15:21<16:55,  4.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 204/428 [15:25<16:51,  4.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 205/428 [15:30<16:46,  4.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 206/428 [15:34<16:41,  4.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 207/428 [15:39<16:37,  4.51s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 208/428 [15:43<16:32,  4.51s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 209/428 [15:48<16:28,  4.51s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 210/428 [15:52<16:23,  4.51s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 211/428 [15:57<16:19,  4.51s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 212/428 [16:01<16:14,  4.51s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 213/428 [16:06<16:10,  4.51s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 214/428 [16:10<16:05,  4.51s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 215/428 [16:15<16:01,  4.51s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 216/428 [16:20<15:56,  4.51s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 217/428 [16:24<15:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 218/428 [16:29<15:47,  4.51s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 219/428 [16:33<15:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 220/428 [16:38<15:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 221/428 [16:42<15:34,  4.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 222/428 [16:47<15:30,  4.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 223/428 [16:51<15:25,  4.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 224/428 [16:56<15:20,  4.51s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 225/428 [17:00<15:16,  4.51s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 226/428 [17:05<15:11,  4.51s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 227/428 [17:09<15:06,  4.51s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 228/428 [17:14<15:02,  4.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 229/428 [17:18<14:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 230/428 [17:23<14:53,  4.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 231/428 [17:27<14:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 232/428 [17:32<14:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 233/428 [17:36<14:40,  4.51s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 234/428 [17:41<14:35,  4.51s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 235/428 [17:45<14:30,  4.51s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 236/428 [17:50<14:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 237/428 [17:54<14:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 238/428 [17:59<14:17,  4.51s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 239/428 [18:03<14:12,  4.51s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 240/428 [18:08<14:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 241/428 [18:12<14:03,  4.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 242/428 [18:17<13:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 243/428 [18:21<13:54,  4.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 244/428 [18:26<13:49,  4.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 245/428 [18:30<13:45,  4.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 246/428 [18:35<13:41,  4.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 247/428 [18:39<13:36,  4.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 248/428 [18:44<13:32,  4.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 249/428 [18:48<13:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 250/428 [18:53<13:22,  4.51s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 251/428 [18:57<13:18,  4.51s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 252/428 [19:02<13:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 253/428 [19:06<13:09,  4.51s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 254/428 [19:11<13:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 255/428 [19:15<13:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 256/428 [19:20<12:55,  4.51s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 257/428 [19:25<12:54,  4.53s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 258/428 [19:29<12:53,  4.55s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 259/428 [19:34<12:46,  4.54s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 260/428 [19:38<12:41,  4.53s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 261/428 [19:43<12:35,  4.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 262/428 [19:47<12:30,  4.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 263/428 [19:52<12:26,  4.52s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 264/428 [19:56<12:20,  4.52s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 265/428 [20:01<12:15,  4.52s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 266/428 [20:05<12:11,  4.51s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 267/428 [20:10<12:06,  4.51s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 268/428 [20:14<12:01,  4.51s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 269/428 [20:19<11:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 270/428 [20:23<11:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 271/428 [20:28<11:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 272/428 [20:32<11:44,  4.51s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 273/428 [20:37<11:39,  4.51s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 274/428 [20:41<11:35,  4.51s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 275/428 [20:46<11:30,  4.51s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 276/428 [20:50<11:25,  4.51s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 277/428 [20:55<11:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 278/428 [20:59<11:16,  4.51s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 279/428 [21:04<11:11,  4.51s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 280/428 [21:08<11:07,  4.51s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 281/428 [21:13<11:02,  4.51s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 282/428 [21:17<10:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 283/428 [21:22<10:54,  4.51s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 284/428 [21:26<10:49,  4.51s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 285/428 [21:31<10:44,  4.51s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 286/428 [21:35<10:40,  4.51s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 287/428 [21:40<10:36,  4.51s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 288/428 [21:44<10:31,  4.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 289/428 [21:49<10:27,  4.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 290/428 [21:54<10:22,  4.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 291/428 [21:58<10:18,  4.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 292/428 [22:03<10:13,  4.51s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 293/428 [22:07<10:09,  4.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 294/428 [22:12<10:04,  4.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 295/428 [22:16<10:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 296/428 [22:21<09:55,  4.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 297/428 [22:25<09:51,  4.51s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 298/428 [22:30<09:46,  4.51s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 299/428 [22:34<09:42,  4.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 300/428 [22:39<09:37,  4.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 301/428 [22:43<09:33,  4.51s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 302/428 [22:48<09:29,  4.52s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 303/428 [22:52<09:24,  4.51s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 304/428 [22:57<09:19,  4.51s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 305/428 [23:01<09:15,  4.51s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 306/428 [23:06<09:10,  4.51s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 307/428 [23:10<09:05,  4.51s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 308/428 [23:15<09:01,  4.51s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 309/428 [23:19<08:56,  4.51s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 310/428 [23:24<08:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 311/428 [23:28<08:47,  4.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 312/428 [23:33<08:43,  4.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 313/428 [23:37<08:38,  4.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 314/428 [23:42<08:34,  4.51s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 315/428 [23:46<08:29,  4.51s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 316/428 [23:51<08:25,  4.51s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 317/428 [23:55<08:20,  4.51s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 318/428 [24:00<08:16,  4.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 319/428 [24:04<08:11,  4.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 320/428 [24:09<08:07,  4.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 321/428 [24:13<08:02,  4.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 322/428 [24:18<07:58,  4.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 323/428 [24:22<07:53,  4.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 324/428 [24:27<07:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 325/428 [24:31<07:44,  4.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 326/428 [24:36<07:40,  4.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 327/428 [24:40<07:35,  4.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 328/428 [24:45<07:31,  4.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 329/428 [24:49<07:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 330/428 [24:54<07:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 331/428 [24:58<07:17,  4.51s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 332/428 [25:03<07:12,  4.51s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 333/428 [25:07<07:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 334/428 [25:12<07:03,  4.51s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 335/428 [25:17<06:59,  4.51s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 336/428 [25:21<06:54,  4.50s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 337/428 [25:25<06:49,  4.50s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 338/428 [25:30<06:44,  4.50s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 339/428 [25:34<06:40,  4.50s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 340/428 [25:39<06:35,  4.50s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 341/428 [25:43<06:30,  4.49s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 342/428 [25:48<06:26,  4.49s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 343/428 [25:52<06:21,  4.49s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 344/428 [25:57<06:16,  4.49s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 345/428 [26:01<06:12,  4.49s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 346/428 [26:06<06:07,  4.49s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 347/428 [26:10<06:03,  4.49s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 348/428 [26:15<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 349/428 [26:19<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 350/428 [26:24<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 351/428 [26:28<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 352/428 [26:33<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 353/428 [26:37<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 354/428 [26:42<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 355/428 [26:46<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 356/428 [26:51<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 357/428 [26:55<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 358/428 [27:00<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 359/428 [27:04<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 360/428 [27:09<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 361/428 [27:13<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 362/428 [27:18<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 363/428 [27:22<04:51,  4.49s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 364/428 [27:27<04:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 365/428 [27:31<04:42,  4.49s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 366/428 [27:36<04:39,  4.51s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 367/428 [27:40<04:36,  4.53s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 368/428 [27:45<04:31,  4.52s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 369/428 [27:49<04:26,  4.51s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 370/428 [27:54<04:21,  4.51s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 371/428 [27:58<04:16,  4.50s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 372/428 [28:03<04:13,  4.52s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 373/428 [28:07<04:08,  4.51s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 374/428 [28:12<04:03,  4.51s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 375/428 [28:16<03:58,  4.50s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 376/428 [28:21<03:53,  4.50s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 377/428 [28:25<03:49,  4.50s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 378/428 [28:30<03:44,  4.50s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 379/428 [28:34<03:40,  4.50s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 380/428 [28:39<03:35,  4.50s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 381/428 [28:43<03:31,  4.50s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 382/428 [28:48<03:26,  4.50s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 383/428 [28:52<03:22,  4.50s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 384/428 [28:57<03:17,  4.50s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 385/428 [29:01<03:13,  4.50s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 386/428 [29:06<03:08,  4.50s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 387/428 [29:10<03:04,  4.50s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 388/428 [29:15<02:59,  4.50s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 389/428 [29:19<02:55,  4.50s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 390/428 [29:24<02:50,  4.50s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 391/428 [29:28<02:46,  4.50s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 392/428 [29:33<02:42,  4.50s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 393/428 [29:37<02:37,  4.51s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 394/428 [29:42<02:33,  4.52s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 395/428 [29:46<02:29,  4.53s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 396/428 [29:51<02:25,  4.53s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 397/428 [29:55<02:20,  4.53s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 398/428 [30:00<02:16,  4.54s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 399/428 [30:04<02:11,  4.54s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 400/428 [30:09<02:06,  4.53s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 401/428 [30:14<02:02,  4.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 402/428 [30:18<01:57,  4.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 403/428 [30:23<01:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 404/428 [30:27<01:48,  4.51s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 405/428 [30:32<01:43,  4.50s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 406/428 [30:36<01:39,  4.50s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 407/428 [30:40<01:34,  4.50s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 408/428 [30:45<01:29,  4.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 409/428 [30:49<01:25,  4.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 410/428 [30:54<01:20,  4.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 411/428 [30:58<01:16,  4.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 412/428 [31:03<01:11,  4.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 413/428 [31:07<01:07,  4.49s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 414/428 [31:12<01:02,  4.49s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 415/428 [31:16<00:58,  4.49s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 416/428 [31:21<00:53,  4.49s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 417/428 [31:25<00:49,  4.49s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 418/428 [31:30<00:44,  4.49s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 419/428 [31:34<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 420/428 [31:39<00:35,  4.49s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 421/428 [31:43<00:31,  4.49s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 422/428 [31:48<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 423/428 [31:52<00:22,  4.49s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 424/428 [31:57<00:17,  4.49s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 425/428 [32:01<00:13,  4.49s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 426/428 [32:06<00:08,  4.49s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 427/428 [32:10<00:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 428/428 [32:15<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/171 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m1%|          | 2/171 [00:01<02:40,  1.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/171 [00:02<02:05,  1.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/171 [00:03<02:00,  1.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/171 [00:03<01:57,  1.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 6/171 [00:04<01:55,  1.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/171 [00:05<01:53,  1.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 8/171 [00:05<01:52,  1.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 9/171 [00:06<01:51,  1.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 10/171 [00:07<01:50,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 11/171 [00:07<01:49,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/171 [00:08<01:48,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/171 [00:09<01:48,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 14/171 [00:09<01:47,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 15/171 [00:10<01:46,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 16/171 [00:11<01:45,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 17/171 [00:11<01:45,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 18/171 [00:12<01:44,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 19/171 [00:13<01:43,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 20/171 [00:13<01:43,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 21/171 [00:14<01:42,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 22/171 [00:15<01:41,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 23/171 [00:16<01:41,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 24/171 [00:16<01:40,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 25/171 [00:17<01:39,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 26/171 [00:18<01:39,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 27/171 [00:18<01:38,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▋        | 28/171 [00:19<01:37,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 29/171 [00:20<01:36,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 30/171 [00:20<01:36,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 31/171 [00:21<01:35,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 32/171 [00:22<01:34,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 33/171 [00:22<01:34,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 34/171 [00:23<01:33,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 35/171 [00:24<01:32,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 36/171 [00:24<01:32,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 37/171 [00:25<01:31,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 38/171 [00:26<01:30,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 39/171 [00:26<01:30,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 40/171 [00:27<01:29,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 41/171 [00:28<01:28,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▍       | 42/171 [00:28<01:28,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 43/171 [00:29<01:27,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 44/171 [00:30<01:26,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 45/171 [00:31<01:25,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 46/171 [00:31<01:25,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 47/171 [00:32<01:24,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 48/171 [00:33<01:24,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 49/171 [00:33<01:23,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 50/171 [00:34<01:22,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 51/171 [00:35<01:21,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 52/171 [00:35<01:21,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 53/171 [00:36<01:20,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 54/171 [00:37<01:19,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 55/171 [00:37<01:19,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 56/171 [00:38<01:18,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 57/171 [00:39<01:17,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 58/171 [00:39<01:17,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 59/171 [00:40<01:16,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 60/171 [00:41<01:15,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 61/171 [00:41<01:15,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▋      | 62/171 [00:42<01:14,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 63/171 [00:43<01:13,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 64/171 [00:44<01:13,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 65/171 [00:44<01:12,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 66/171 [00:45<01:11,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 67/171 [00:46<01:11,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|███▉      | 68/171 [00:46<01:10,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 69/171 [00:47<01:09,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 70/171 [00:48<01:08,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 71/171 [00:48<01:08,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 72/171 [00:49<01:07,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 73/171 [00:50<01:06,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 74/171 [00:50<01:06,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 75/171 [00:51<01:05,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 76/171 [00:52<01:04,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 77/171 [00:52<01:04,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 78/171 [00:53<01:03,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 79/171 [00:54<01:02,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 80/171 [00:54<01:02,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 81/171 [00:55<01:01,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 82/171 [00:56<01:00,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 83/171 [00:56<01:00,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 84/171 [00:57<00:59,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|████▉     | 85/171 [00:58<00:58,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 86/171 [00:59<00:58,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 87/171 [00:59<00:57,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 88/171 [01:00<00:56,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 89/171 [01:01<00:55,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 90/171 [01:01<00:55,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 91/171 [01:02<00:54,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 92/171 [01:03<00:53,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 93/171 [01:03<00:53,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 94/171 [01:04<00:52,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 95/171 [01:05<00:51,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 96/171 [01:05<00:51,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 97/171 [01:06<00:50,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 98/171 [01:07<00:49,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 99/171 [01:07<00:49,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 100/171 [01:08<00:48,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 101/171 [01:09<00:47,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 102/171 [01:09<00:47,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 103/171 [01:10<00:46,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 104/171 [01:11<00:45,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 105/171 [01:12<00:45,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 106/171 [01:12<00:44,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 107/171 [01:13<00:43,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 108/171 [01:14<00:43,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 109/171 [01:14<00:42,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 110/171 [01:15<00:41,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 111/171 [01:16<00:41,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 112/171 [01:16<00:40,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 113/171 [01:17<00:39,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 114/171 [01:18<00:38,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 115/171 [01:18<00:38,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 116/171 [01:19<00:37,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 117/171 [01:20<00:36,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 118/171 [01:20<00:36,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 119/171 [01:21<00:35,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 120/171 [01:22<00:34,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 121/171 [01:22<00:34,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 122/171 [01:23<00:33,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 123/171 [01:24<00:32,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 124/171 [01:25<00:32,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 125/171 [01:25<00:31,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 126/171 [01:26<00:30,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 127/171 [01:27<00:30,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 128/171 [01:27<00:29,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 129/171 [01:28<00:28,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 130/171 [01:29<00:28,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 131/171 [01:29<00:27,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 132/171 [01:30<00:26,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 133/171 [01:31<00:25,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 134/171 [01:31<00:25,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 135/171 [01:32<00:24,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 136/171 [01:33<00:23,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 137/171 [01:33<00:23,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 138/171 [01:34<00:22,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 139/171 [01:35<00:21,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 140/171 [01:35<00:21,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 141/171 [01:36<00:20,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 142/171 [01:37<00:19,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 143/171 [01:38<00:19,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 144/171 [01:38<00:18,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 145/171 [01:39<00:17,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 146/171 [01:40<00:17,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 147/171 [01:40<00:16,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 148/171 [01:41<00:15,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 149/171 [01:42<00:15,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 150/171 [01:42<00:14,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 151/171 [01:43<00:13,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 152/171 [01:44<00:13,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 153/171 [01:44<00:12,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 154/171 [01:45<00:11,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 155/171 [01:46<00:10,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 156/171 [01:46<00:10,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 157/171 [01:47<00:09,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 158/171 [01:48<00:08,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 159/171 [01:48<00:08,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 160/171 [01:49<00:07,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 161/171 [01:50<00:06,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 162/171 [01:51<00:06,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 163/171 [01:51<00:05,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 164/171 [01:52<00:04,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 165/171 [01:53<00:04,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 166/171 [01:53<00:03,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 167/171 [01:54<00:02,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 168/171 [01:55<00:02,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 169/171 [01:55<00:01,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 170/171 [01:56<00:00,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 171/171 [01:57<00:00,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.0194367170333862, 'eval_runtime': 118.1417, 'eval_samples_per_second': 11.579, 'eval_steps_per_second': 1.447, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 428/428 [34:13<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 171/171 [01:57<00:00,  1.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2053.3329, 'train_samples_per_second': 3.335, 'train_steps_per_second': 0.208, 'train_loss': 1.029766546231564, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 428/428 [34:13<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 428/428 [34:13<00:00,  4.80s/it]\u001b[0m\n",
      "\u001b[34m2024-04-12 01:05:46,101 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-12 01:05:46,101 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-12 01:05:46,102 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-12 01:06:02 Uploading - Uploading generated training model\n",
      "2024-04-12 01:06:02 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 2665\n",
      "Billable seconds: 2665\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Section\n",
    "#### Monitor the training with TensorBoard\n",
    "**Note: You have to wait a few minutes for the job to launch before seeing any logs**\n",
    "\n",
    "We can use [TensorBoard](https://www.tensorflow.org/tensorboard), a visualization toolkit for analyzing deep learning models to monitor the progress of the training. Instructions for using TensorBoard with SageMaker Studio can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html). Instructions for accessing TensorBoard in SageMaker Studio are provided below:\n",
    "1. Open a new terminal in SageMaker Studio by navigating to <em>File->New->Terminal <br> ![](./image/OpenTerminal.JPG)\n",
    "2. Run the following command in the terminal `pip install tensorboard boto3 tensorflow_io`\n",
    "3. Run the notebook cell below to generate a command to launch TensorBoard\n",
    "3. Copy the command and paste it into the terminal and hit Enter\n",
    "4. Return to the notebook an click the link provided in the bellow cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "cur_dir = os.getcwd().replace(os.environ[\"HOME\"],\"\")\n",
    "HTML(f'''1. Paste the following command into the Studio Terminal <code style=\"background-color:gray;\">tensorboard --logdir {tensorboard_output_config.s3_output_path}</code><br>\n",
    "2. Click <a href='/jupyter/default/proxy/6006/'>here</a> to open TensorBoard''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "In this section we'll deploy our model to a SageMaker Endpoint. We'll then use the endpoint to generate summaries for random examples from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to wait for the job to finish before we can deploy the model \n",
    "estimator.latest_training_job.wait(logs=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job has completed, we can deploy the model to a SageMaker Endpoint.\n",
    "We will use a [Deep learning container for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) for deployment which is optimized for serving large models in excess of 100B parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need a few additional imports for model deployment\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the docker image that will be used for inference\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a model deploymnent packages which will be used to deploy our model to a SageMaker Endpoint. The model deployment package is a tarball that contains the model artifacts, [inference code](src/inference/model.py), and any [additional dependencies](src/inference/requirements.txt) required to run the inference code. We'll go through the following steps to create the model deployment package:\n",
    "1. Download the trained model artifact from S3 to the local filesystem\n",
    "2. Cretae a `serving.properties` file that will configure our hosting environment\n",
    "3. Combine the trained model, the inference code, and the `serving.properties` file into a tarball with the following structure:\n",
    "```\n",
    "|-- model.py         # inference code\n",
    "|-- requirements.txt    # additional dependencies\n",
    "|-- serving.properties  # configuration file\n",
    "|-- <model_id>\\         # model artifacts\n",
    "    |-- config.json\n",
    "    |-- pytorch_model.bin\n",
    "    |-- special_tokens_map.json\n",
    "    |-- tokenizer_config.json\n",
    "    |-- tokenizer.json\n",
    "    |-- vocab.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model was trained using Low Rank Adaptation (LoRA), and as a result the model artifact is small (~10Mb) allowing us to repackage it along with our inference code. At deployment time, the base model will be downloaded from Hugging Face Hub and the LoRA weights will be applied to the base model. For deployment of larger models with LoRA weights, it is recommended to store the based model weights in your own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model artifacts into the inference code directory \n",
    "with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "    contents = tar.getnames()\n",
    "    model_id = os.path.dirname(contents[-1]) # model id is the name of the folder containing the model files as generated by the training job\n",
    "    tar.extractall(\"src/inference/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the serving.properties file\n",
    "# We'll use the python engine for inference and specify the model_id for the base model we want to use\n",
    "with open(\"src/inference/serving.properties\", \"w\") as f:\n",
    "    f.write(\n",
    "f\"\"\"engine=Python\n",
    "option.model_id={model_id}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything needed to create the model package. We'll combine the contents of the `src/inference` directory with the model artifact and create a tarball. We'll then upload the tarball to S3 and use the S3 URI to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd src/\n",
    "tar czvf model.tar.gz inference/\n",
    "mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, f\"{s3_key_prefix}/model\")\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {hf_s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(image_uri, model_data, role, endpoint_name, instance_type, sagemaker_session):\n",
    "    \"\"\"Helper function to create the SageMaker Endpoint resources and return a predictor\"\"\"\n",
    "    model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "    model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name)\n",
    "\n",
    "    # our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=serializers.JSONSerializer(),        # will convert python dict to json\n",
    "        deserializer=deserializers.JSONDeserializer(),  # will convert json to python dict\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique endpoint name\n",
    "hf_endpoint_name = sagemaker.utils.name_from_base(\"t5-summarization\")\n",
    "print(f\"Our endpoint will be called {hf_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment will take 5 to 10 minutes\n",
    "hf_predictor = deploy_model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=hf_s3_code_artifact,\n",
    "    role=role,\n",
    "    endpoint_name=hf_endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the endpoint deployed, we can generate summaries on dialogues from the test dataset. We'll randomly select an examples and generate summaries. You can also provide your own dialogue to generate summaries just be sure to use the same format as the examples in the train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dialogue_idx = randint(0, test_data.shape[0])\n",
    "random_dialogue = test_data[\"dialogue\"][random_dialogue_idx]\n",
    "\n",
    "output = hf_predictor.predict({\"inputs\": [random_dialogue], \"parameters\":{\"max_length\": 100}})\n",
    "output_summary = output[\"outputs\"][0][\"summary_text\"]\n",
    "\n",
    "print(\"#####DIALOGUE######\\n\", random_dialogue)\n",
    "print(\"\\n#####GENERATED SUMMARY######\\n\", output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint when finished experimenting\n",
    "hf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

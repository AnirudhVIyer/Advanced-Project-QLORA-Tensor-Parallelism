{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "In this section we'll import the requisite libraries and instantiate a number of objects and variables to configure our training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                        # SageMaker Python SDK\n",
    "from sagemaker.pytorch import PyTorch                   # PyTorch Estimator for running pytorch training jobs\n",
    "from sagemaker.debugger import TensorBoardOutputConfig  # Debugger TensorBoard config to log training metrics to TensorBoard\n",
    "import boto3                                            # AWS SDK for Python\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()   # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_key_prefix = \"13-bill\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl -O data/dialogsum.train.jsonl\n",
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.test.jsonl -O data/dialogsum.test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accelerate launch` command has two key parts, the `config.yml` file and the `train.py` script. The `config.yml` file is used to configure the distributed training job. The `train.py` script is the training script that will be launched by the launcher. In this example, we'll use the [ds_zero3.yml](src/train/ds_zero3.yaml) configuration file. The config file enables [DeepSpeed ZeRo Stage3](#https://www.deepspeed.ai/tutorials/zero/) and a number of other optimizations to enable training of large scale models. This file was generated by running `accelerate config --config_file ds_zero3.yml` and then following the on-screen prompts. \n",
    "The [train.py](src/train/train.py) makes use of a number of key libraries to enable training of large models with minimal code changes:\n",
    "- 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) - Configures the distributed training environment and adapts training objects (data loaders, models, optimizers) to the distributed environment\n",
    "- 🤗 [Transformers](https://huggingface.co/docs/transformers/index) - Provides a number of pre-trained models and utilities for training and evaluating models\n",
    "- 🤗 [PEFT](https://github.com/huggingface/peft) - Provides a number of methods for Parameter Efficient Finetuning(PEFT) of large language models. The [LoRA](https://arxiv.org/pdf/2106.09685.pdf) method will be used to finetune the model\n",
    "- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Provides a number of optimizations to enable training of large models. In this example, we'll use DeepSpeed ZeRO Stage3 to enable training of models with over 1B parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the tesnorboard output directly to S3\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://{bucket}/{s3_key_prefix}/tensorboard\"\n",
    ")\n",
    "\n",
    "image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker'\n",
    "estimator1 = PyTorch(\n",
    "    source_dir = \"src/train\",\n",
    "    entry_point=\"acc_launcher.py\",\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    framework_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    hyperparameters = {\n",
    "    \"training_script\": \"train_qlora.py\",\n",
    "    \"config_file\": \"qlora.yaml\",\n",
    "    \"seed\": 100,\n",
    "    \"model_name_or_path\": \"NousResearch/Llama-2-13b-hf\",\n",
    "    \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
    "    \"chat_template_format\": \"chatml\",\n",
    "    \"add_special_tokens\": False,\n",
    "    \"append_concat_token\": False,\n",
    "    \"splits\": \"train,test\",\n",
    "    \"max_seq_len\": 2048,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"bf16\": True,\n",
    "    \"packing\": True,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_ratio\": 0.0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"use_reentrant\": True,\n",
    "    \"dataset_text_field\": \"content\",\n",
    "    \"use_flash_attn\": False,\n",
    "    \"use_peft_lora\": True,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": \"all-linear\",\n",
    "    \"use_4bit_quantization\": True,\n",
    "    \"use_nested_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
    "    \"report_to\":\"none\",\n",
    "\n",
    "},\n",
    "\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-04-14-16-55-37-214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 16:55:37 Starting - Starting the training job\n",
      "2024-04-14 16:55:37 Pending - Training job waiting for capacity......\n",
      "2024-04-14 16:56:10 Pending - Preparing the instances for training........................\n",
      "2024-04-14 17:00:23 Downloading - Downloading input data...\n",
      "2024-04-14 17:00:48 Downloading - Downloading the training image...............\n",
      "2024-04-14 17:03:39 Training - Training image download completed. Training in progress..........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-14 17:04:56,463 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-14 17:04:56,559 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-14 17:04:56,566 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-14 17:04:56,568 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-14 17:04:57,945 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers to /tmp/pip-req-build-aatnvboi\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-aatnvboi\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers to commit b109257f4fb8b1166e7c53cc5418632014ed53a5\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/accelerate (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-scmr1ikz\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-scmr1ikz\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/accelerate to commit 5ca095a34fede7c988af8c193eb0c0d199750845\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/peft (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/peft to /tmp/pip-req-build-ahg73mu1\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-ahg73mu1\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/peft to commit c8974c5880b28a913e35f050e82402e34d181c63\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/trl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/trl to /tmp/pip-req-build-1haa01an\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-1haa01an\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/trl to commit 1c0d8bca159b5f416323b51cf34c8a92ba6a05d5\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/datatrove.git (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/datatrove.git to /tmp/pip-req-build-08ppj331\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/datatrove.git /tmp/pip-req-build-08ppj331\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/datatrove.git to commit 209ebec293cb2407c8e6fc691bfeb358ba030b09\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting unsloth[conda]@ git+https://github.com/unslothai/unsloth.git (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-lf7tw66u/unsloth_daa955d3dccc4d33a62f927de9942e28\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-lf7tw66u/unsloth_daa955d3dccc4d33a62f927de9942e28\u001b[0m\n",
      "\u001b[34mResolved https://github.com/unslothai/unsloth.git to commit 4606443b77f98a624896d4ca50710255d8436d86\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting PyGithub (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyGithub-2.3.0-py3-none-any.whl (354 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.4/354.4 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 35.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 45.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.6.1)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 92.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 92.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 99.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (3.7.2)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 100.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 112.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xformers (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.5/222.5 MB 8.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting hf_transfer (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 88.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 73.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 91.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 100.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl==0.8.4.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.0/102.0 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.12.2 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting humanize (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.9.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.8/126.8 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting loguru>=0.7.0 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 18.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 69.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynacl>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.5.0)\u001b[0m\n",
      "\u001b[34mCollecting pyjwt[crypto]>=2.4.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.26.15)\u001b[0m\n",
      "\u001b[34mCollecting Deprecated (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 40.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 11)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 12)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 88.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (8.1.4)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 34.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 267.1/267.1 kB 50.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 107.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 109.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (4.41.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 23)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting torch>=1.10.0 (from accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 1.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.0.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mUsing cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 52.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 76.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 88.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 26.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 8.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 10.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 28.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting triton==2.2.0 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 10.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 75.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting fsspec[http]>=2021.05.0 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 34.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 55.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 12.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub->-r requirements.txt (line 8)) (41.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4)) (13.4.2)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 16)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10 (from Deprecated->PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wheel>=0.42.0 (from unsloth[conda]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.4.dev0->-r requirements.txt (line 4)) (0.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, accelerate, peft, trl, datatrove, unsloth\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8888292 sha256=7aa9cc822959fed89e262789acaf5fea63abc32b563d4449e768d5a421039250\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for accelerate: filename=accelerate-0.30.0.dev0-py3-none-any.whl size=297660 sha256=7227bc7384b13f452833de5900590ad2da77ed3d189b633164ef7d8585392228\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/f6/c7/9d/1b8a5ca8353d9307733bc719107acb67acdc95063bba749f26\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for peft: filename=peft-0.10.1.dev0-py3-none-any.whl size=218552 sha256=e9f8bd48d75ac3b6fe898325f26c219e208ea11a8bc3d73dda3d0e9e446df23e\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for trl: filename=trl-0.8.4.dev0-py3-none-any.whl size=244261 sha256=6390f28f143fa5a136bd945b0fea5c19ce0b6bf69c31717549f08a8ec02b1751\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/6a/aa/56/d64d9ae3521350622f9325fdc3bccb4dd3d3ec1c1d8e917400\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for datatrove: filename=datatrove-0.0.1-py3-none-any.whl size=16644320 sha256=e3d5042a94ead026630c4cebb3a5c178442059a8067e035ecc3b6b807cd27f7c\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/8a/74/96/5af76f3e0504a1ed76602ea64ffadeb493297b90d375394cd0\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for unsloth: filename=unsloth-2024.4-py3-none-any.whl size=96655 sha256=237edd22dec5b1528f3368288f421bae2a8bd7ac1e507e6a8fefd94fb8640cfb\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-4o4q3wzs/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers accelerate peft trl datatrove unsloth\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, appdirs, xxhash, wrapt, wheel, unsloth, typing-extensions, triton, tensorboard-data-server, smmap, shtab, setproctitle, sentry-sdk, safetensors, regex, pyjwt, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown, loguru, humanize, hf_transfer, grpcio, fsspec, frozenlist, docstring-parser, docker-pycreds, async-timeout, absl-py, yarl, tiktoken, tensorboard, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, huggingface-hub, gitdb, Deprecated, aiosignal, tyro, tokenizers, nvidia-cusolver-cu12, GitPython, datatrove, aiohttp, wandb, transformers, torch, PyGithub, xformers, datasets, bitsandbytes, accelerate, trl, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: wheel\u001b[0m\n",
      "\u001b[34mFound existing installation: wheel 0.40.0\u001b[0m\n",
      "\u001b[34mUninstalling wheel-0.40.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled wheel-0.40.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.24.4\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.24.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.24.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.6.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.6.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.6.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.20.3\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Deprecated-1.2.14 GitPython-3.1.43 PyGithub-2.3.0 absl-py-2.1.0 accelerate-0.30.0.dev0 aiohttp-3.9.4 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.18.0 datatrove-0.0.1 docker-pycreds-0.4.0 docstring-parser-0.16 evaluate-0.4.1 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 grpcio-1.62.1 hf_transfer-0.1.6 huggingface-hub-0.22.2 humanize-4.9.0 loguru-0.7.2 markdown-3.6 multidict-6.0.5 nltk-3.8.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 peft-0.10.1.dev0 pyarrow-hotfix-0.6 pyjwt-2.8.0 regex-2023.12.25 responses-0.18.0 safetensors-0.4.2 sentencepiece-0.2.0 sentry-sdk-1.45.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.2 transformers-4.40.0.dev0 triton-2.2.0 trl-0.8.4.dev0 typing-extensions-4.11.0 tyro-0.8.3 unsloth-2024.4 wandb-0.16.6 wheel-0.43.0 wrapt-1.16.0 xformers-0.0.25.post1 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,158 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,158 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,288 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,391 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,497 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,505 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_special_tokens\": false,\n",
      "        \"append_concat_token\": false,\n",
      "        \"bf16\": true,\n",
      "        \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "        \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
      "        \"chat_template_format\": \"chatml\",\n",
      "        \"config_file\": \"qlora.yaml\",\n",
      "        \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
      "        \"dataset_text_field\": \"content\",\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 8,\n",
      "        \"lora_target_modules\": \"all-linear\",\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 1.0,\n",
      "        \"max_seq_len\": 2048,\n",
      "        \"model_name_or_path\": \"NousResearch/Llama-2-13b-hf\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
      "        \"packing\": true,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"report_to\": \"none\",\n",
      "        \"seed\": 100,\n",
      "        \"splits\": \"train,test\",\n",
      "        \"training_script\": \"train_qlora.py\",\n",
      "        \"use_4bit_quantization\": true,\n",
      "        \"use_flash_attn\": false,\n",
      "        \"use_nested_quant\": true,\n",
      "        \"use_peft_lora\": true,\n",
      "        \"use_reentrant\": true,\n",
      "        \"warmup_ratio\": 0.0,\n",
      "        \"weight_decay\": 0.0001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-14-16-55-37-214\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-14-16-55-37-214/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"acc_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"acc_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-13b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":true,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":true,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=acc_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=acc_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-14-16-55-37-214/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-13b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":true,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":true,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-14-16-55-37-214\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-14-16-55-37-214/source/sourcedir.tar.gz\",\"module_name\":\"acc_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"acc_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_special_tokens\",\"False\",\"--append_concat_token\",\"False\",\"--bf16\",\"True\",\"--bnb_4bit_compute_dtype\",\"bfloat16\",\"--bnb_4bit_quant_storage_dtype\",\"bfloat16\",\"--chat_template_format\",\"chatml\",\"--config_file\",\"qlora.yaml\",\"--dataset_name\",\"smangrul/ultrachat-10k-chatml\",\"--dataset_text_field\",\"content\",\"--evaluation_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0001\",\"--logging_strategy\",\"steps\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"8\",\"--lora_target_modules\",\"all-linear\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"1.0\",\"--max_seq_len\",\"2048\",\"--model_name_or_path\",\"NousResearch/Llama-2-13b-hf\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"llama-sft-qlora-dsz3\",\"--packing\",\"True\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--report_to\",\"none\",\"--seed\",\"100\",\"--splits\",\"train,test\",\"--training_script\",\"train_qlora.py\",\"--use_4bit_quantization\",\"True\",\"--use_flash_attn\",\"False\",\"--use_nested_quant\",\"True\",\"--use_peft_lora\",\"True\",\"--use_reentrant\",\"True\",\"--warmup_ratio\",\"0.0\",\"--weight_decay\",\"0.0001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_SPECIAL_TOKENS=false\u001b[0m\n",
      "\u001b[34mSM_HP_APPEND_CONCAT_TOKEN=false\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_COMPUTE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_QUANT_STORAGE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE_FORMAT=chatml\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=qlora.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=smangrul/ultrachat-10k-chatml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_TEXT_FIELD=content\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_TARGET_MODULES=all-linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=NousResearch/Llama-2-13b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=llama-sft-qlora-dsz3\u001b[0m\n",
      "\u001b[34mSM_HP_PACKING=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=none\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=100\u001b[0m\n",
      "\u001b[34mSM_HP_SPLITS=train,test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=train_qlora.py\u001b[0m\n",
      "\u001b[34mSM_HP_USE_4BIT_QUANTIZATION=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_NESTED_QUANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_PEFT_LORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_REENTRANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 acc_launcher.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --config_file qlora.yaml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-13b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --training_script train_qlora.py --use_4bit_quantization True --use_flash_attn False --use_nested_quant True --use_peft_lora True --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m2024-04-14 17:07:20,535 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcommand = accelerate launch --config_file qlora.yaml train_qlora.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-13b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --use_4bit_quantization True --use_flash_attn False --use_nested_quant True --use_peft_lora True --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:23,269] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:23,269] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:23,269] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:23,269] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 43.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 51.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 899.9 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mUsing cached pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400395 sha256=9ade891b9d49afe76b7e972e3c5269b18699239202e3ff3514d2e9b6a4adf932\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400397 sha256=36390c17cae30a97cc5cc384274814baa5ecad3198138f6816112ddcce01e8a5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtrl 0.8.4.dev0\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:44,110] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.4\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtrl 0.8.4.dev0\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:44,429] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:44,429] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:44,446] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34m[2024-04-14 17:07:44,752] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 1/3 [00:22<00:44, 22.04s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 1/3 [00:21<00:43, 21.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.64s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [00:57<00:00, 18.26s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [00:57<00:00, 19.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [00:57<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 3/3 [00:57<00:00, 19.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:15<00:30, 15.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:16<00:32, 16.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:29<00:14, 14.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:32<00:16, 16.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:39<00:00, 12.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:39<00:00, 13.09s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr14_17-07-44_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/524 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|██████████| 524/524 [00:00<00:00, 937kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/35.2M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 10.5M/35.2M [00:00<00:00, 78.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 35.2M/35.2M [00:00<00:00, 171MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/7.08M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 7.08M/7.08M [00:00<00:00, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 7.08M/7.08M [00:00<00:00, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  60%|██████    | 6000/10000 [00:00<00:00, 48513.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 100%|██████████| 10000/10000 [00:00<00:00, 50132.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 100%|██████████| 2000/2000 [00:00<00:00, 56176.47 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:42<00:00, 13.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:42<00:00, 14.21s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|█         | 1000/10000 [00:00<00:01, 8361.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|███       | 3000/10000 [00:00<00:00, 10048.07 examples/s]\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr14_17-07-44_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 5000/10000 [00:00<00:00, 10650.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 7000/10000 [00:00<00:00, 11045.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 9000/10000 [00:00<00:00, 11302.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10000/10000 [00:00<00:00, 10976.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11581.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2000/2000 [00:00<00:00, 11484.93 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  2.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 362 examples [00:00, 913.13 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 720 examples [00:00, 1618.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:01, 854.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1377 examples [00:01, 1283.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1759 examples [00:01, 1722.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2187 examples [00:01, 1200.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2564 examples [00:02, 1540.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3000 examples [00:02, 1176.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3373 examples [00:02, 1478.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3724 examples [00:03, 959.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4000 examples [00:03, 1062.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4374 examples [00:03, 1377.81 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4649 examples [00:04, 1075.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5000 examples [00:04, 1230.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5295 examples [00:04, 1080.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5665 examples [00:04, 1408.04 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6000 examples [00:04, 1430.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6287 examples [00:05, 1248.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6662 examples [00:05, 1604.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6848 examples [00:05, 1248.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  3.72 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 372 examples [00:00, 1288.82 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 750 examples [00:00, 2112.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1185 examples [00:00, 1374.45 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1368 examples [00:01, 1365.68 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mPeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32008, 5120)\n",
      "        (layers): ModuleList(\n",
      "          (0-39): 40 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=32008, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mtrainable params: 31,293,440 || all params: 13,047,239,680 || trainable%: 0.2398472072830044\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mtrainable params: 31,293,440 || all params: 13,047,239,680 || trainable%: 0.2398472072830044\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 18437120 in 521 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/856 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/856 [00:18<4:28:20, 18.83s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/856 [00:26<2:52:34, 12.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/856 [00:33<2:21:48,  9.98s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/856 [00:41<2:07:14,  8.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/856 [00:48<1:59:13,  8.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/856 [00:55<1:54:22,  8.07s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/856 [01:03<1:51:17,  7.87s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/856 [01:10<1:49:12,  7.73s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/856 [01:18<1:47:45,  7.63s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/856 [01:25<1:46:44,  7.57s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/856 [01:33<1:46:01,  7.53s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 12/856 [01:40<1:45:31,  7.50s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/856 [01:47<1:45:03,  7.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/856 [01:55<1:44:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/856 [02:02<1:44:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/856 [02:10<1:44:18,  7.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/856 [02:17<1:44:04,  7.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/856 [02:25<1:43:58,  7.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/856 [02:32<1:43:48,  7.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/856 [02:40<1:43:44,  7.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 21/856 [02:47<1:43:38,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/856 [02:54<1:43:32,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/856 [03:02<1:43:28,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/856 [03:09<1:43:19,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/856 [03:17<1:43:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/856 [03:24<1:43:03,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/856 [03:32<1:42:58,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 28/856 [03:39<1:42:51,  7.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 29/856 [03:47<1:42:44,  7.45s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 30/856 [03:54<1:42:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 31/856 [04:02<1:42:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 32/856 [04:09<1:42:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/856 [04:16<1:42:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/856 [04:24<1:42:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/856 [04:31<1:42:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 36/856 [04:39<1:42:20,  7.49s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 37/856 [04:46<1:42:03,  7.48s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 38/856 [04:54<1:41:51,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/856 [05:01<1:41:38,  7.46s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 40/856 [05:09<1:41:48,  7.49s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 41/856 [05:16<1:41:35,  7.48s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 42/856 [05:24<1:41:23,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/856 [05:31<1:41:13,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 44/856 [05:39<1:41:03,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 45/856 [05:46<1:40:55,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 46/856 [05:54<1:40:47,  7.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 47/856 [06:01<1:40:39,  7.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/856 [06:09<1:40:32,  7.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 49/856 [06:16<1:40:24,  7.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 50/856 [06:23<1:40:16,  7.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 51/856 [06:31<1:40:08,  7.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 52/856 [06:38<1:40:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 53/856 [06:46<1:39:52,  7.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 54/856 [06:53<1:39:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 55/856 [07:01<1:39:37,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 56/856 [07:08<1:39:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 57/856 [07:16<1:39:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 58/856 [07:23<1:39:17,  7.47s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 59/856 [07:31<1:39:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 60/856 [07:38<1:39:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 61/856 [07:46<1:38:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 62/856 [07:53<1:38:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 63/856 [08:00<1:38:31,  7.45s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 64/856 [08:08<1:38:22,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 65/856 [08:15<1:38:11,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 66/856 [08:23<1:38:06,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 67/856 [08:30<1:38:01,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 68/856 [08:38<1:37:51,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 69/856 [08:45<1:37:45,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 70/856 [08:53<1:37:37,  7.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 71/856 [09:00<1:37:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 72/856 [09:08<1:37:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 73/856 [09:15<1:37:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 74/856 [09:22<1:37:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 75/856 [09:30<1:37:04,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 76/856 [09:37<1:36:59,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 77/856 [09:45<1:36:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 78/856 [09:52<1:36:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 79/856 [10:00<1:36:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 80/856 [10:07<1:36:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 81/856 [10:15<1:36:27,  7.47s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 82/856 [10:22<1:36:21,  7.47s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 83/856 [10:30<1:36:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 84/856 [10:37<1:35:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 85/856 [10:45<1:35:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 86/856 [10:52<1:35:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 87/856 [10:59<1:35:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 88/856 [11:07<1:35:26,  7.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 89/856 [11:14<1:35:16,  7.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 90/856 [11:22<1:35:09,  7.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 91/856 [11:29<1:35:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 92/856 [11:37<1:34:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 93/856 [11:44<1:34:46,  7.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 94/856 [11:52<1:34:40,  7.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 95/856 [11:59<1:34:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 96/856 [12:07<1:34:25,  7.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 97/856 [12:14<1:34:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 98/856 [12:21<1:34:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 99/856 [12:29<1:34:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 100/856 [12:36<1:34:02,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 101/856 [12:44<1:33:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 102/856 [12:51<1:33:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 103/856 [12:59<1:33:38,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 104/856 [13:06<1:33:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 105/856 [13:14<1:33:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 106/856 [13:21<1:33:12,  7.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 107/856 [13:29<1:33:04,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 108/856 [13:36<1:32:55,  7.45s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 109/856 [13:44<1:32:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 110/856 [13:51<1:32:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 111/856 [13:58<1:32:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 112/856 [14:06<1:32:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 113/856 [14:13<1:32:17,  7.45s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 114/856 [14:21<1:32:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 115/856 [14:28<1:32:06,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 116/856 [14:36<1:32:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 117/856 [14:43<1:31:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 118/856 [14:51<1:31:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 119/856 [14:58<1:31:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 120/856 [15:06<1:31:32,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 121/856 [15:13<1:31:25,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 122/856 [15:21<1:31:20,  7.47s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 123/856 [15:28<1:31:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 124/856 [15:35<1:31:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 125/856 [15:43<1:30:56,  7.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 126/856 [15:50<1:30:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 127/856 [15:58<1:30:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 128/856 [16:05<1:30:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 129/856 [16:13<1:30:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 130/856 [16:20<1:30:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 131/856 [16:28<1:30:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 132/856 [16:35<1:30:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 133/856 [16:43<1:29:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 134/856 [16:50<1:29:44,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 135/856 [16:58<1:29:36,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 136/856 [17:05<1:29:26,  7.45s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 137/856 [17:12<1:29:20,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 138/856 [17:20<1:29:14,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 139/856 [17:27<1:29:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 140/856 [17:35<1:29:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 141/856 [17:42<1:28:54,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 142/856 [17:50<1:28:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 143/856 [17:57<1:28:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 144/856 [18:05<1:28:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 145/856 [18:12<1:28:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 146/856 [18:20<1:28:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 147/856 [18:27<1:28:05,  7.46s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 148/856 [18:34<1:27:57,  7.45s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 149/856 [18:42<1:27:51,  7.46s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 150/856 [18:49<1:27:44,  7.46s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 151/856 [18:57<1:27:34,  7.45s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 152/856 [19:04<1:27:28,  7.45s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 153/856 [19:12<1:27:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 154/856 [19:19<1:27:12,  7.45s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 155/856 [19:27<1:27:21,  7.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 156/856 [19:34<1:27:08,  7.47s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 157/856 [19:42<1:26:59,  7.47s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 158/856 [19:49<1:26:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 159/856 [19:57<1:26:53,  7.48s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 160/856 [20:04<1:26:41,  7.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 161/856 [20:12<1:26:30,  7.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 162/856 [20:19<1:26:25,  7.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 163/856 [20:26<1:26:17,  7.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 164/856 [20:34<1:26:05,  7.46s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 165/856 [20:41<1:25:58,  7.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 166/856 [20:49<1:25:51,  7.47s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 167/856 [20:56<1:25:44,  7.47s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 168/856 [21:04<1:25:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 169/856 [21:11<1:25:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 170/856 [21:19<1:25:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 171/856 [21:26<1:25:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 172/856 [21:34<1:25:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 173/856 [21:41<1:24:52,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 174/856 [21:49<1:24:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 175/856 [21:56<1:24:34,  7.45s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 176/856 [22:03<1:24:18,  7.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 177/856 [22:11<1:24:09,  7.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 178/856 [22:18<1:24:01,  7.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 179/856 [22:26<1:23:53,  7.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 180/856 [22:33<1:23:45,  7.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 181/856 [22:41<1:23:36,  7.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 182/856 [22:48<1:23:30,  7.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 183/856 [22:55<1:23:22,  7.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 184/856 [23:03<1:23:15,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 185/856 [23:10<1:23:07,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 186/856 [23:18<1:23:00,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 187/856 [23:25<1:22:52,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 188/856 [23:33<1:22:44,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 189/856 [23:40<1:22:36,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 190/856 [23:47<1:22:29,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 191/856 [23:55<1:22:21,  7.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 192/856 [24:02<1:22:16,  7.43s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 193/856 [24:10<1:22:11,  7.44s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 194/856 [24:17<1:22:07,  7.44s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 195/856 [24:25<1:22:01,  7.45s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 196/856 [24:32<1:21:54,  7.45s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 197/856 [24:40<1:21:47,  7.45s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 198/856 [24:47<1:21:38,  7.44s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 199/856 [24:54<1:21:34,  7.45s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 200/856 [25:02<1:21:29,  7.45s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 201/856 [25:09<1:21:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 202/856 [25:17<1:21:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 203/856 [25:24<1:21:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 204/856 [25:32<1:21:04,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 205/856 [25:39<1:20:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 206/856 [25:47<1:20:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 207/856 [25:54<1:20:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 208/856 [26:02<1:20:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 209/856 [26:09<1:20:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 210/856 [26:17<1:20:22,  7.47s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 211/856 [26:24<1:20:15,  7.47s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 212/856 [26:31<1:20:06,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 213/856 [26:39<1:19:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 214/856 [26:46<1:19:48,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 215/856 [26:54<1:19:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 216/856 [27:01<1:19:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 217/856 [27:09<1:19:26,  7.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 218/856 [27:16<1:19:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 219/856 [27:24<1:19:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 220/856 [27:31<1:19:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 221/856 [27:39<1:18:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 222/856 [27:46<1:18:48,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 223/856 [27:54<1:18:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 224/856 [28:01<1:18:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 225/856 [28:08<1:18:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 226/856 [28:16<1:18:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 227/856 [28:23<1:18:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 228/856 [28:31<1:18:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 229/856 [28:38<1:18:01,  7.47s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 230/856 [28:46<1:17:53,  7.47s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 231/856 [28:53<1:17:46,  7.47s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 232/856 [29:01<1:17:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 233/856 [29:08<1:17:25,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 234/856 [29:16<1:17:17,  7.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 235/856 [29:23<1:17:08,  7.45s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 236/856 [29:30<1:16:56,  7.45s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 237/856 [29:38<1:16:48,  7.44s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 238/856 [29:45<1:16:35,  7.44s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 239/856 [29:53<1:16:26,  7.43s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 240/856 [30:00<1:16:19,  7.43s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 241/856 [30:08<1:16:12,  7.44s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 242/856 [30:15<1:16:05,  7.44s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 243/856 [30:22<1:15:55,  7.43s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 244/856 [30:30<1:15:48,  7.43s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 245/856 [30:37<1:15:42,  7.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 246/856 [30:45<1:15:35,  7.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 247/856 [30:52<1:15:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 248/856 [31:00<1:15:21,  7.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 249/856 [31:07<1:15:16,  7.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 250/856 [31:15<1:15:12,  7.45s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 251/856 [31:22<1:15:05,  7.45s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 252/856 [31:29<1:14:56,  7.44s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 253/856 [31:37<1:14:49,  7.45s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 254/856 [31:44<1:14:41,  7.44s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 255/856 [31:52<1:14:36,  7.45s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 256/856 [31:59<1:14:29,  7.45s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 257/856 [32:07<1:14:24,  7.45s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 258/856 [32:14<1:14:16,  7.45s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 259/856 [32:22<1:14:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 260/856 [32:29<1:14:04,  7.46s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 261/856 [32:37<1:13:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 262/856 [32:44<1:13:51,  7.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 263/856 [32:51<1:13:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 264/856 [32:59<1:13:41,  7.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 265/856 [33:06<1:13:34,  7.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 266/856 [33:14<1:13:26,  7.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 267/856 [33:21<1:13:19,  7.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 268/856 [33:29<1:13:08,  7.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 269/856 [33:36<1:13:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 270/856 [33:44<1:12:51,  7.46s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 271/856 [33:51<1:12:40,  7.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 272/856 [33:59<1:12:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 273/856 [34:06<1:12:25,  7.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 274/856 [34:14<1:12:17,  7.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 275/856 [34:21<1:12:22,  7.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 276/856 [34:29<1:12:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 277/856 [34:36<1:11:54,  7.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 278/856 [34:43<1:11:44,  7.45s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 279/856 [34:51<1:11:48,  7.47s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 280/856 [34:58<1:11:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 281/856 [35:06<1:11:20,  7.44s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 282/856 [35:13<1:11:10,  7.44s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 283/856 [35:21<1:10:59,  7.43s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 284/856 [35:28<1:10:51,  7.43s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 285/856 [35:35<1:10:44,  7.43s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 286/856 [35:43<1:10:36,  7.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 287/856 [35:50<1:10:28,  7.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 288/856 [35:58<1:10:19,  7.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 289/856 [36:05<1:10:14,  7.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 290/856 [36:13<1:10:06,  7.43s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 291/856 [36:20<1:10:02,  7.44s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 292/856 [36:27<1:09:55,  7.44s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 293/856 [36:35<1:09:49,  7.44s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 294/856 [36:42<1:09:42,  7.44s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 295/856 [36:50<1:09:36,  7.44s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 296/856 [36:57<1:09:31,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 297/856 [37:05<1:09:24,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 298/856 [37:12<1:09:18,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 299/856 [37:20<1:09:10,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 300/856 [37:27<1:09:04,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 301/856 [37:35<1:08:57,  7.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 302/856 [37:42<1:08:50,  7.46s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 303/856 [37:49<1:08:43,  7.46s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 304/856 [37:57<1:08:37,  7.46s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 305/856 [38:04<1:08:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 306/856 [38:12<1:08:24,  7.46s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 307/856 [38:19<1:08:19,  7.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 308/856 [38:27<1:08:11,  7.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 309/856 [38:34<1:08:04,  7.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 310/856 [38:42<1:07:57,  7.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 311/856 [38:49<1:07:49,  7.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 312/856 [38:57<1:07:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 313/856 [39:04<1:07:32,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 314/856 [39:12<1:07:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 315/856 [39:19<1:07:19,  7.47s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 316/856 [39:27<1:07:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 317/856 [39:34<1:07:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 318/856 [39:41<1:06:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 319/856 [39:49<1:06:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 320/856 [39:56<1:06:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 321/856 [40:04<1:06:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 322/856 [40:11<1:06:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 323/856 [40:19<1:06:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 324/856 [40:26<1:06:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 325/856 [40:34<1:06:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 326/856 [40:41<1:05:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 327/856 [40:49<1:05:48,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 328/856 [40:56<1:05:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 329/856 [41:04<1:05:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 330/856 [41:11<1:05:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 331/856 [41:18<1:05:14,  7.46s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 332/856 [41:26<1:05:05,  7.45s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 333/856 [41:33<1:04:56,  7.45s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 334/856 [41:41<1:04:48,  7.45s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 335/856 [41:48<1:04:37,  7.44s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 336/856 [41:56<1:04:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 337/856 [42:03<1:04:20,  7.44s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 338/856 [42:11<1:04:11,  7.44s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 339/856 [42:18<1:04:02,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 340/856 [42:25<1:03:54,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 341/856 [42:33<1:03:47,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 342/856 [42:40<1:03:38,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 343/856 [42:48<1:03:30,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 344/856 [42:55<1:03:24,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 345/856 [43:03<1:03:17,  7.43s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 346/856 [43:10<1:03:13,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 347/856 [43:17<1:03:08,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 348/856 [43:25<1:03:01,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 349/856 [43:32<1:02:52,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 350/856 [43:40<1:02:47,  7.45s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 351/856 [43:47<1:02:39,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 352/856 [43:55<1:02:31,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 353/856 [44:02<1:02:23,  7.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 354/856 [44:10<1:02:18,  7.45s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 355/856 [44:17<1:02:11,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 356/856 [44:24<1:02:05,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 357/856 [44:32<1:01:58,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 358/856 [44:39<1:01:50,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 359/856 [44:47<1:01:41,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 360/856 [44:54<1:01:37,  7.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 361/856 [45:02<1:01:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 362/856 [45:09<1:01:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 363/856 [45:17<1:01:18,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 364/856 [45:24<1:01:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 365/856 [45:32<1:01:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 366/856 [45:39<1:00:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 367/856 [45:46<1:00:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 368/856 [45:54<1:00:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 369/856 [46:01<1:00:32,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 370/856 [46:09<1:00:24,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 371/856 [46:16<1:00:17,  7.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 372/856 [46:24<1:00:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 373/856 [46:31<1:00:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 374/856 [46:39<59:52,  7.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 375/856 [46:46<59:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 376/856 [46:54<59:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 377/856 [47:01<59:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 378/856 [47:09<59:22,  7.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 379/856 [47:16<59:14,  7.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 380/856 [47:23<59:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 381/856 [47:31<59:02,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 382/856 [47:38<58:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 383/856 [47:46<58:48,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 384/856 [47:53<58:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 385/856 [48:01<58:30,  7.45s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 386/856 [48:08<58:24,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 387/856 [48:16<58:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 388/856 [48:23<58:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 389/856 [48:31<58:02,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 390/856 [48:38<57:54,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 391/856 [48:45<57:46,  7.45s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 392/856 [48:53<57:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 393/856 [49:00<57:32,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 394/856 [49:08<57:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 395/856 [49:15<57:20,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 396/856 [49:23<57:12,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 397/856 [49:30<57:05,  7.46s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 398/856 [49:38<57:05,  7.48s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 399/856 [49:45<56:54,  7.47s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 400/856 [49:53<56:44,  7.47s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 401/856 [50:00<56:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 402/856 [50:08<56:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 403/856 [50:15<56:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 404/856 [50:22<56:12,  7.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 405/856 [50:30<56:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 406/856 [50:37<55:58,  7.46s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 407/856 [50:45<55:52,  7.47s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 408/856 [50:52<55:45,  7.47s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 409/856 [51:00<55:38,  7.47s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 410/856 [51:07<55:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 411/856 [51:15<55:22,  7.47s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 412/856 [51:22<55:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 413/856 [51:30<55:02,  7.45s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 414/856 [51:37<54:53,  7.45s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 415/856 [51:45<54:43,  7.45s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 416/856 [51:52<54:32,  7.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 417/856 [51:59<54:24,  7.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 418/856 [52:07<54:14,  7.43s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 419/856 [52:14<54:07,  7.43s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 420/856 [52:22<54:02,  7.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 421/856 [52:29<53:56,  7.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 422/856 [52:37<53:47,  7.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 423/856 [52:44<53:37,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 424/856 [52:51<53:30,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 425/856 [52:59<53:21,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 426/856 [53:06<53:13,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 427/856 [53:14<53:06,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 428/856 [53:21<53:00,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 429/856 [53:29<52:55,  7.44s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 430/856 [53:36<52:46,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 431/856 [53:43<52:39,  7.43s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 432/856 [53:51<52:30,  7.43s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 433/856 [53:58<52:26,  7.44s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 434/856 [54:06<52:15,  7.43s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 435/856 [54:13<52:09,  7.43s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 436/856 [54:21<52:06,  7.44s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 437/856 [54:28<52:00,  7.45s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 438/856 [54:36<51:53,  7.45s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 439/856 [54:43<51:45,  7.45s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 440/856 [54:50<51:39,  7.45s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 441/856 [54:58<51:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 442/856 [55:05<51:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 443/856 [55:13<51:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 444/856 [55:20<51:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 445/856 [55:28<51:10,  7.47s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 446/856 [55:35<51:01,  7.47s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 447/856 [55:43<50:52,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 448/856 [55:50<50:44,  7.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 449/856 [55:58<50:37,  7.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 450/856 [56:05<50:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 451/856 [56:13<50:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 452/856 [56:20<50:16,  7.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 453/856 [56:28<50:09,  7.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 454/856 [56:35<50:01,  7.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 455/856 [56:42<49:52,  7.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 456/856 [56:50<49:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 457/856 [56:57<49:37,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 458/856 [57:05<49:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 459/856 [57:12<49:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 460/856 [57:20<49:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 461/856 [57:27<49:09,  7.47s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 462/856 [57:35<49:01,  7.47s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 463/856 [57:42<48:51,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 464/856 [57:50<48:43,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 465/856 [57:57<48:37,  7.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 466/856 [58:05<48:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 467/856 [58:12<48:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 468/856 [58:19<48:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 469/856 [58:27<48:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 470/856 [58:34<48:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 471/856 [58:42<47:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 472/856 [58:49<47:46,  7.47s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 473/856 [58:57<47:38,  7.46s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 474/856 [59:04<47:32,  7.47s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 475/856 [59:12<47:24,  7.47s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 476/856 [59:19<47:17,  7.47s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 477/856 [59:27<47:09,  7.47s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 478/856 [59:34<46:58,  7.46s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 479/856 [59:41<46:48,  7.45s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 480/856 [59:49<46:39,  7.44s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 481/856 [59:56<46:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 482/856 [1:00:04<46:23,  7.44s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 483/856 [1:00:11<46:13,  7.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 484/856 [1:00:19<46:06,  7.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 485/856 [1:00:26<45:58,  7.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 486/856 [1:00:34<45:51,  7.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 487/856 [1:00:41<45:42,  7.43s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 488/856 [1:00:48<45:34,  7.43s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 489/856 [1:00:56<45:28,  7.43s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 490/856 [1:01:03<45:20,  7.43s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 491/856 [1:01:11<45:14,  7.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 492/856 [1:01:18<45:06,  7.44s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 493/856 [1:01:26<44:57,  7.43s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 494/856 [1:01:33<44:50,  7.43s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 495/856 [1:01:40<44:43,  7.43s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 496/856 [1:01:48<44:38,  7.44s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 497/856 [1:01:55<44:32,  7.44s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 498/856 [1:02:03<44:25,  7.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 499/856 [1:02:10<44:17,  7.44s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 500/856 [1:02:18<44:09,  7.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9918, 'grad_norm': 0.138864642866298, 'learning_rate': 3.694091893909746e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 500/856 [1:02:18<44:09,  7.44s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:187: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 501/856 [1:02:33<57:11,  9.67s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 502/856 [1:02:40<53:05,  9.00s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 503/856 [1:02:48<50:43,  8.62s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 504/856 [1:02:55<48:30,  8.27s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 505/856 [1:03:03<46:53,  8.01s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 506/856 [1:03:10<45:44,  7.84s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 507/856 [1:03:17<44:53,  7.72s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 508/856 [1:03:25<44:17,  7.64s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 509/856 [1:03:32<43:47,  7.57s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 510/856 [1:03:40<43:27,  7.53s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 511/856 [1:03:47<43:09,  7.51s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 512/856 [1:03:55<42:54,  7.48s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 513/856 [1:04:02<42:43,  7.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 514/856 [1:04:09<42:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 515/856 [1:04:17<42:20,  7.45s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 516/856 [1:04:24<42:12,  7.45s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 517/856 [1:04:32<42:02,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 518/856 [1:04:39<41:55,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 519/856 [1:04:47<41:47,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 520/856 [1:04:54<41:40,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 521/856 [1:05:02<41:33,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 522/856 [1:05:09<41:26,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 523/856 [1:05:16<41:17,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 524/856 [1:05:24<41:10,  7.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 525/856 [1:05:31<41:04,  7.45s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 526/856 [1:05:39<40:56,  7.44s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 527/856 [1:05:46<40:50,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 528/856 [1:05:54<40:44,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 529/856 [1:06:01<40:35,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 530/856 [1:06:09<40:29,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 531/856 [1:06:16<40:22,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 532/856 [1:06:24<40:14,  7.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 533/856 [1:06:31<40:08,  7.46s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 534/856 [1:06:38<40:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 535/856 [1:06:46<39:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 536/856 [1:06:53<39:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 537/856 [1:07:01<39:40,  7.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 538/856 [1:07:08<39:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 539/856 [1:07:16<39:25,  7.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 540/856 [1:07:23<39:19,  7.47s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 541/856 [1:07:31<39:12,  7.47s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 542/856 [1:07:38<39:04,  7.47s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 543/856 [1:07:46<38:57,  7.47s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 544/856 [1:07:53<38:47,  7.46s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 545/856 [1:08:01<38:37,  7.45s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 546/856 [1:08:08<38:29,  7.45s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 547/856 [1:08:15<38:21,  7.45s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 548/856 [1:08:23<38:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 549/856 [1:08:30<38:07,  7.45s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 550/856 [1:08:38<37:57,  7.44s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 551/856 [1:08:45<37:49,  7.44s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 552/856 [1:08:53<37:41,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 553/856 [1:09:00<37:34,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 554/856 [1:09:07<37:25,  7.43s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 555/856 [1:09:15<37:17,  7.43s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 556/856 [1:09:22<37:10,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 557/856 [1:09:30<37:03,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 558/856 [1:09:37<36:55,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 559/856 [1:09:45<36:48,  7.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 560/856 [1:09:52<36:41,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 561/856 [1:10:00<36:33,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 562/856 [1:10:07<36:26,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 563/856 [1:10:14<36:19,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 564/856 [1:10:22<36:12,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 565/856 [1:10:29<36:05,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 566/856 [1:10:37<35:56,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 567/856 [1:10:44<35:49,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 568/856 [1:10:52<35:43,  7.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 569/856 [1:10:59<35:36,  7.45s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 570/856 [1:11:07<35:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 571/856 [1:11:14<35:23,  7.45s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 572/856 [1:11:21<35:18,  7.46s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 573/856 [1:11:29<35:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 574/856 [1:11:36<35:05,  7.47s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 575/856 [1:11:44<34:58,  7.47s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 576/856 [1:11:51<34:52,  7.47s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 577/856 [1:11:59<34:44,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 578/856 [1:12:06<34:37,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 579/856 [1:12:14<34:28,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 580/856 [1:12:21<34:21,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 581/856 [1:12:29<34:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 582/856 [1:12:36<34:05,  7.46s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 583/856 [1:12:44<33:58,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 584/856 [1:12:51<33:50,  7.46s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 585/856 [1:12:59<33:43,  7.47s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 586/856 [1:13:06<33:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 587/856 [1:13:13<33:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 588/856 [1:13:21<33:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 589/856 [1:13:28<33:12,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 590/856 [1:13:36<33:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 591/856 [1:13:43<32:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 592/856 [1:13:51<32:46,  7.45s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 593/856 [1:13:58<32:38,  7.45s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 594/856 [1:14:06<32:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 595/856 [1:14:13<32:22,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 596/856 [1:14:20<32:15,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 597/856 [1:14:28<32:08,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 598/856 [1:14:35<31:59,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 599/856 [1:14:43<31:51,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 600/856 [1:14:50<31:43,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 601/856 [1:14:58<31:36,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 602/856 [1:15:05<31:28,  7.44s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 603/856 [1:15:13<31:20,  7.43s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 604/856 [1:15:20<31:14,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 605/856 [1:15:27<31:07,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 606/856 [1:15:35<30:59,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 607/856 [1:15:42<30:52,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 608/856 [1:15:50<30:44,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 609/856 [1:15:57<30:37,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 610/856 [1:16:05<30:31,  7.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 611/856 [1:16:12<30:24,  7.45s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 612/856 [1:16:20<30:18,  7.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 613/856 [1:16:27<30:11,  7.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 614/856 [1:16:34<30:03,  7.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 615/856 [1:16:42<29:56,  7.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 616/856 [1:16:49<29:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 617/856 [1:16:57<29:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 618/856 [1:17:04<29:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 619/856 [1:17:12<29:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 620/856 [1:17:19<29:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 621/856 [1:17:27<29:13,  7.46s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 622/856 [1:17:34<29:10,  7.48s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 623/856 [1:17:42<29:02,  7.48s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 624/856 [1:17:49<28:54,  7.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 625/856 [1:17:57<28:45,  7.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 626/856 [1:18:04<28:37,  7.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 627/856 [1:18:12<28:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 628/856 [1:18:19<28:20,  7.46s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 629/856 [1:18:26<28:12,  7.45s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 630/856 [1:18:34<28:03,  7.45s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 631/856 [1:18:41<27:57,  7.45s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 632/856 [1:18:49<27:49,  7.45s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 633/856 [1:18:56<27:40,  7.45s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 634/856 [1:19:04<27:32,  7.44s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 635/856 [1:19:11<27:24,  7.44s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 636/856 [1:19:19<27:16,  7.44s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 637/856 [1:19:26<27:09,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 638/856 [1:19:33<27:01,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 639/856 [1:19:41<26:54,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 640/856 [1:19:48<26:46,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 641/856 [1:19:56<26:39,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 642/856 [1:20:03<26:32,  7.44s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 643/856 [1:20:11<26:25,  7.45s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 644/856 [1:20:18<26:18,  7.45s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 645/856 [1:20:26<26:11,  7.45s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 646/856 [1:20:33<26:04,  7.45s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 647/856 [1:20:40<25:57,  7.45s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 648/856 [1:20:48<25:49,  7.45s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 649/856 [1:20:55<25:42,  7.45s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 650/856 [1:21:03<25:35,  7.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 651/856 [1:21:10<25:28,  7.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 652/856 [1:21:18<25:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 653/856 [1:21:25<25:14,  7.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 654/856 [1:21:33<25:08,  7.47s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 655/856 [1:21:40<25:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 656/856 [1:21:48<24:52,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 657/856 [1:21:55<24:45,  7.47s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 658/856 [1:22:03<24:38,  7.47s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 659/856 [1:22:10<24:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 660/856 [1:22:17<24:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 661/856 [1:22:25<24:14,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 662/856 [1:22:32<24:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 663/856 [1:22:40<24:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 664/856 [1:22:47<23:53,  7.47s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 665/856 [1:22:55<23:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 666/856 [1:23:02<23:36,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 667/856 [1:23:10<23:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 668/856 [1:23:17<23:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 669/856 [1:23:25<23:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 670/856 [1:23:32<23:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 671/856 [1:23:40<23:00,  7.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 672/856 [1:23:47<22:51,  7.45s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 673/856 [1:23:54<22:43,  7.45s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 674/856 [1:24:02<22:36,  7.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 675/856 [1:24:09<22:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 676/856 [1:24:17<22:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 677/856 [1:24:24<22:16,  7.47s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 678/856 [1:24:32<22:09,  7.47s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 679/856 [1:24:39<22:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 680/856 [1:24:47<21:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 681/856 [1:24:54<21:46,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 682/856 [1:25:02<21:39,  7.47s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 683/856 [1:25:09<21:31,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 684/856 [1:25:17<21:23,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 685/856 [1:25:24<21:16,  7.47s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 686/856 [1:25:31<21:08,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 687/856 [1:25:39<21:01,  7.47s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 688/856 [1:25:46<20:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 689/856 [1:25:54<20:45,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 690/856 [1:26:01<20:38,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 691/856 [1:26:09<20:30,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 692/856 [1:26:16<20:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 693/856 [1:26:24<20:15,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 694/856 [1:26:31<20:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 695/856 [1:26:39<20:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 696/856 [1:26:46<19:54,  7.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 697/856 [1:26:54<19:46,  7.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 698/856 [1:27:01<19:39,  7.47s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 699/856 [1:27:08<19:32,  7.47s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 700/856 [1:27:16<19:24,  7.47s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 701/856 [1:27:23<19:16,  7.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 702/856 [1:27:31<19:08,  7.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 703/856 [1:27:38<19:01,  7.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 704/856 [1:27:46<18:53,  7.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 705/856 [1:27:53<18:45,  7.45s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 706/856 [1:28:01<18:37,  7.45s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 707/856 [1:28:08<18:29,  7.45s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 708/856 [1:28:16<18:21,  7.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 709/856 [1:28:23<18:14,  7.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 710/856 [1:28:30<18:05,  7.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 711/856 [1:28:38<17:57,  7.43s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 712/856 [1:28:45<17:50,  7.43s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 713/856 [1:28:53<17:43,  7.43s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 714/856 [1:29:00<17:35,  7.43s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 715/856 [1:29:08<17:28,  7.43s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 716/856 [1:29:15<17:20,  7.44s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 717/856 [1:29:22<17:13,  7.43s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 718/856 [1:29:30<17:06,  7.44s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 719/856 [1:29:37<16:59,  7.44s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 720/856 [1:29:45<16:52,  7.45s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 721/856 [1:29:52<16:45,  7.45s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 722/856 [1:30:00<16:38,  7.45s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 723/856 [1:30:07<16:30,  7.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 724/856 [1:30:15<16:23,  7.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 725/856 [1:30:22<16:16,  7.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 726/856 [1:30:30<16:09,  7.46s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 727/856 [1:30:37<16:01,  7.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 728/856 [1:30:44<15:53,  7.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 729/856 [1:30:52<15:46,  7.46s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 730/856 [1:30:59<15:39,  7.46s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 731/856 [1:31:07<15:32,  7.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 732/856 [1:31:14<15:25,  7.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 733/856 [1:31:22<15:17,  7.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 734/856 [1:31:29<15:10,  7.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 735/856 [1:31:37<15:02,  7.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 736/856 [1:31:44<14:55,  7.47s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 737/856 [1:31:52<14:56,  7.53s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 738/856 [1:31:59<14:46,  7.52s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 739/856 [1:32:07<14:37,  7.50s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 740/856 [1:32:14<14:28,  7.49s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 741/856 [1:32:22<14:22,  7.50s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 742/856 [1:32:29<14:13,  7.49s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 743/856 [1:32:37<14:05,  7.48s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 744/856 [1:32:44<13:57,  7.47s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 745/856 [1:32:52<13:49,  7.47s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 746/856 [1:32:59<13:41,  7.47s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 747/856 [1:33:07<13:34,  7.47s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 748/856 [1:33:14<13:26,  7.47s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 749/856 [1:33:21<13:19,  7.47s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 750/856 [1:33:29<13:11,  7.47s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 751/856 [1:33:36<13:03,  7.47s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 752/856 [1:33:44<12:55,  7.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 753/856 [1:33:51<12:48,  7.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 754/856 [1:33:59<12:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 755/856 [1:34:06<12:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 756/856 [1:34:14<12:26,  7.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 757/856 [1:34:21<12:18,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 758/856 [1:34:29<12:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 759/856 [1:34:36<12:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 760/856 [1:34:44<11:56,  7.47s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 761/856 [1:34:51<11:49,  7.47s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 762/856 [1:34:58<11:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 763/856 [1:35:06<11:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 764/856 [1:35:13<11:26,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 765/856 [1:35:21<11:18,  7.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 766/856 [1:35:28<11:11,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 767/856 [1:35:36<11:03,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 768/856 [1:35:43<10:56,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 769/856 [1:35:51<10:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 770/856 [1:35:58<10:41,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 771/856 [1:36:06<10:33,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 772/856 [1:36:13<10:26,  7.45s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 773/856 [1:36:21<10:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 774/856 [1:36:28<10:12,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 775/856 [1:36:35<10:04,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 776/856 [1:36:43<09:57,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 777/856 [1:36:50<09:49,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 778/856 [1:36:58<09:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 779/856 [1:37:05<09:34,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 780/856 [1:37:13<09:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 781/856 [1:37:20<09:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 782/856 [1:37:28<09:12,  7.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 783/856 [1:37:35<09:05,  7.47s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 784/856 [1:37:43<08:57,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 785/856 [1:37:50<08:49,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 786/856 [1:37:58<08:42,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 787/856 [1:38:05<08:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 788/856 [1:38:12<08:27,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 789/856 [1:38:20<08:19,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 790/856 [1:38:27<08:12,  7.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 791/856 [1:38:35<08:04,  7.46s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 792/856 [1:38:42<07:56,  7.45s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 793/856 [1:38:50<07:49,  7.45s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 794/856 [1:38:57<07:41,  7.45s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 795/856 [1:39:05<07:34,  7.44s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 796/856 [1:39:12<07:26,  7.44s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 797/856 [1:39:19<07:18,  7.44s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 798/856 [1:39:27<07:11,  7.44s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 799/856 [1:39:34<07:04,  7.44s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 800/856 [1:39:42<06:56,  7.44s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 801/856 [1:39:49<06:49,  7.44s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 802/856 [1:39:57<06:42,  7.44s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 803/856 [1:40:04<06:34,  7.45s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 804/856 [1:40:12<06:27,  7.45s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 805/856 [1:40:19<06:19,  7.45s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 806/856 [1:40:27<06:12,  7.45s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 807/856 [1:40:34<06:05,  7.45s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 808/856 [1:40:41<05:57,  7.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 809/856 [1:40:49<05:50,  7.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 810/856 [1:40:56<05:42,  7.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 811/856 [1:41:04<05:35,  7.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 812/856 [1:41:11<05:27,  7.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 813/856 [1:41:19<05:20,  7.44s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 814/856 [1:41:26<05:12,  7.44s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 815/856 [1:41:34<05:05,  7.44s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 816/856 [1:41:41<04:57,  7.44s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 817/856 [1:41:48<04:50,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 818/856 [1:41:56<04:42,  7.43s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 819/856 [1:42:03<04:35,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 820/856 [1:42:11<04:27,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 821/856 [1:42:18<04:20,  7.43s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 822/856 [1:42:26<04:12,  7.43s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 823/856 [1:42:33<04:05,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 824/856 [1:42:40<03:58,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 825/856 [1:42:48<03:50,  7.44s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 826/856 [1:42:55<03:43,  7.44s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 827/856 [1:43:03<03:35,  7.44s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 828/856 [1:43:10<03:28,  7.44s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 829/856 [1:43:18<03:21,  7.45s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 830/856 [1:43:25<03:13,  7.45s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 831/856 [1:43:33<03:06,  7.45s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 832/856 [1:43:40<02:58,  7.45s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 833/856 [1:43:48<02:51,  7.45s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 834/856 [1:43:55<02:43,  7.45s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 835/856 [1:44:02<02:36,  7.45s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 836/856 [1:44:10<02:29,  7.46s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 837/856 [1:44:17<02:21,  7.46s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 838/856 [1:44:25<02:14,  7.47s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 839/856 [1:44:32<02:06,  7.46s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 840/856 [1:44:40<01:59,  7.47s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 841/856 [1:44:47<01:52,  7.47s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 842/856 [1:44:55<01:44,  7.47s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 843/856 [1:45:02<01:37,  7.47s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 844/856 [1:45:10<01:29,  7.47s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 845/856 [1:45:17<01:22,  7.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 846/856 [1:45:25<01:14,  7.47s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 847/856 [1:45:32<01:07,  7.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 848/856 [1:45:39<00:59,  7.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 849/856 [1:45:47<00:52,  7.45s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 850/856 [1:45:54<00:44,  7.45s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 851/856 [1:46:02<00:37,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 852/856 [1:46:09<00:29,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 853/856 [1:46:17<00:22,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 854/856 [1:46:24<00:14,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 855/856 [1:46:32<00:07,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 856/856 [1:46:39<00:00,  7.44s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/342 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m1%|          | 2/342 [00:04<13:28,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m1%|          | 3/342 [00:05<09:57,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[34m1%|          | 4/342 [00:06<08:35,  1.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/342 [00:07<07:48,  1.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/342 [00:09<07:18,  1.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/342 [00:10<06:58,  1.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/342 [00:11<06:45,  1.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/342 [00:12<06:37,  1.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/342 [00:13<06:30,  1.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/342 [00:14<06:25,  1.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 12/342 [00:15<06:20,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/342 [00:17<06:17,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/342 [00:18<06:15,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/342 [00:19<06:12,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/342 [00:20<06:11,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/342 [00:21<06:09,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/342 [00:22<06:08,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 19/342 [00:23<06:07,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/342 [00:24<06:06,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/342 [00:26<06:06,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/342 [00:27<06:05,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/342 [00:28<06:05,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/342 [00:29<06:04,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/342 [00:30<06:03,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/342 [00:31<06:00,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/342 [00:32<05:59,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/342 [00:34<05:57,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/342 [00:35<05:55,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 30/342 [00:36<05:55,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/342 [00:37<05:54,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/342 [00:38<05:53,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 33/342 [00:39<05:52,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/342 [00:40<05:51,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 35/342 [00:42<05:49,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 36/342 [00:43<05:48,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 37/342 [00:44<05:48,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 38/342 [00:45<05:46,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 39/342 [00:46<05:45,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 40/342 [00:47<05:43,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/342 [00:48<05:41,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/342 [00:50<05:40,  1.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 43/342 [00:51<05:40,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/342 [00:52<05:39,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/342 [00:53<05:38,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/342 [00:54<05:37,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▎        | 47/342 [00:55<05:36,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 48/342 [00:56<05:35,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/342 [00:58<05:33,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 50/342 [00:59<05:32,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/342 [01:00<05:31,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 52/342 [01:01<05:29,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/342 [01:02<05:29,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 54/342 [01:03<05:28,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/342 [01:04<05:27,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▋        | 56/342 [01:06<05:25,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 57/342 [01:07<05:24,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/342 [01:08<05:23,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/342 [01:09<05:22,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 60/342 [01:10<05:21,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 61/342 [01:11<05:20,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/342 [01:12<05:19,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/342 [01:13<05:18,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 64/342 [01:15<05:15,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 65/342 [01:16<05:15,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/342 [01:17<05:13,  1.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 67/342 [01:18<05:12,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 68/342 [01:19<05:12,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 69/342 [01:20<05:11,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 70/342 [01:21<05:09,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 71/342 [01:23<05:08,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 72/342 [01:24<05:07,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 73/342 [01:25<05:06,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 74/342 [01:26<05:05,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 75/342 [01:27<05:05,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/342 [01:28<05:03,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 77/342 [01:29<05:02,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 78/342 [01:31<05:02,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/342 [01:32<05:01,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/342 [01:33<05:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 81/342 [01:34<04:59,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 82/342 [01:35<04:58,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 83/342 [01:36<04:56,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▍       | 84/342 [01:37<04:55,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▍       | 85/342 [01:39<04:54,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 86/342 [01:40<04:53,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 87/342 [01:41<04:52,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 88/342 [01:42<04:51,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 89/342 [01:43<04:49,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 90/342 [01:44<04:48,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 91/342 [01:45<04:47,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 92/342 [01:47<04:46,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 93/342 [01:48<04:45,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/342 [01:49<04:44,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 95/342 [01:50<04:43,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 96/342 [01:51<04:42,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/342 [01:52<04:41,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 98/342 [01:54<04:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 99/342 [01:55<04:38,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 100/342 [01:56<04:37,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 101/342 [01:57<04:36,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 102/342 [01:58<04:35,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 103/342 [01:59<04:34,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 104/342 [02:00<04:33,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 105/342 [02:02<04:32,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 106/342 [02:03<04:31,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 107/342 [02:04<04:30,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 108/342 [02:05<04:29,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 109/342 [02:06<04:26,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 110/342 [02:07<04:25,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/342 [02:08<04:24,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 112/342 [02:10<04:22,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 113/342 [02:11<04:22,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 114/342 [02:12<04:21,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▎      | 115/342 [02:13<04:20,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 116/342 [02:14<04:19,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 117/342 [02:15<04:18,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 118/342 [02:16<04:17,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 119/342 [02:18<04:16,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 120/342 [02:19<04:15,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 121/342 [02:20<04:13,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 122/342 [02:21<04:12,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 123/342 [02:22<04:11,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▋      | 124/342 [02:23<04:10,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 125/342 [02:25<04:09,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 126/342 [02:26<04:07,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 127/342 [02:27<04:06,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 128/342 [02:28<04:04,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 129/342 [02:29<04:03,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 130/342 [02:30<04:02,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 131/342 [02:31<04:01,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 132/342 [02:33<04:00,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 133/342 [02:34<03:58,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 134/342 [02:35<03:58,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 135/342 [02:36<03:56,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|███▉      | 136/342 [02:37<03:55,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 137/342 [02:38<03:54,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 138/342 [02:39<03:53,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 139/342 [02:41<03:52,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 140/342 [02:42<03:50,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 141/342 [02:43<03:49,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 142/342 [02:44<03:48,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 143/342 [02:45<03:47,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 144/342 [02:46<03:47,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 145/342 [02:47<03:46,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 146/342 [02:49<03:44,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 147/342 [02:50<03:43,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 148/342 [02:51<03:42,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▎     | 149/342 [02:52<03:41,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 150/342 [02:53<03:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 151/342 [02:54<03:38,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 152/342 [02:55<03:36,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 153/342 [02:57<03:35,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 154/342 [02:58<03:35,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 155/342 [02:59<03:34,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 156/342 [03:00<03:33,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 157/342 [03:01<03:32,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 158/342 [03:02<03:31,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 159/342 [03:03<03:29,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 160/342 [03:05<03:27,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 161/342 [03:06<03:26,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 162/342 [03:07<03:25,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 163/342 [03:08<03:24,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 164/342 [03:09<03:23,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 165/342 [03:10<03:22,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 166/342 [03:11<03:21,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 167/342 [03:13<03:20,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 168/342 [03:14<03:19,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 169/342 [03:15<03:18,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|████▉     | 170/342 [03:16<03:17,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 171/342 [03:17<03:16,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 172/342 [03:18<03:15,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 173/342 [03:20<03:14,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 174/342 [03:21<03:13,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 175/342 [03:22<03:11,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 176/342 [03:23<03:09,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 177/342 [03:24<03:08,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 178/342 [03:25<03:08,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 179/342 [03:26<03:06,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 180/342 [03:28<03:05,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 181/342 [03:29<03:04,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 182/342 [03:30<03:03,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 183/342 [03:31<03:02,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 184/342 [03:32<03:01,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 185/342 [03:33<03:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 186/342 [03:34<02:59,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 187/342 [03:36<02:57,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 188/342 [03:37<02:56,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 189/342 [03:38<02:55,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 190/342 [03:39<02:54,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 191/342 [03:40<02:53,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 192/342 [03:41<02:52,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 193/342 [03:42<02:50,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 194/342 [03:44<02:48,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 195/342 [03:45<02:47,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 196/342 [03:46<02:46,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 197/342 [03:47<02:45,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 198/342 [03:48<02:44,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 199/342 [03:49<02:43,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 200/342 [03:50<02:42,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 201/342 [03:52<02:41,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 202/342 [03:53<02:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 203/342 [03:54<02:39,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 204/342 [03:55<02:37,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 205/342 [03:56<02:36,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 206/342 [03:57<02:34,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 207/342 [03:58<02:34,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 208/342 [04:00<02:33,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 209/342 [04:01<02:32,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 210/342 [04:02<02:31,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 211/342 [04:03<02:29,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 212/342 [04:04<02:28,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 213/342 [04:05<02:28,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 214/342 [04:06<02:26,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 215/342 [04:08<02:25,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 216/342 [04:09<02:24,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 217/342 [04:10<02:23,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 218/342 [04:11<02:22,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 219/342 [04:12<02:21,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 220/342 [04:13<02:19,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 221/342 [04:14<02:18,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 222/342 [04:16<02:17,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 223/342 [04:17<02:16,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 224/342 [04:18<02:15,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 225/342 [04:19<02:14,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 226/342 [04:20<02:12,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 227/342 [04:21<02:11,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 228/342 [04:23<02:10,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 229/342 [04:24<02:09,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 230/342 [04:25<02:08,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 231/342 [04:26<02:07,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 232/342 [04:27<02:05,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 233/342 [04:28<02:04,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 234/342 [04:29<02:03,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 235/342 [04:31<02:02,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 236/342 [04:32<02:01,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 237/342 [04:33<02:00,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 238/342 [04:34<01:59,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 239/342 [04:35<01:57,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 240/342 [04:36<01:56,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 241/342 [04:37<01:55,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 242/342 [04:39<01:54,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 243/342 [04:40<01:53,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 244/342 [04:41<01:52,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 245/342 [04:42<01:51,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 246/342 [04:43<01:49,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 247/342 [04:44<01:48,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 248/342 [04:45<01:47,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 249/342 [04:47<01:46,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 250/342 [04:48<01:45,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 251/342 [04:49<01:44,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 252/342 [04:50<01:43,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 253/342 [04:51<01:41,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 254/342 [04:52<01:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 255/342 [04:53<01:39,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 256/342 [04:55<01:38,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 257/342 [04:56<01:37,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 258/342 [04:57<01:36,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 259/342 [04:58<01:35,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 260/342 [04:59<01:34,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 261/342 [05:00<01:32,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 262/342 [05:01<01:31,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 263/342 [05:03<01:30,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 264/342 [05:04<01:29,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 265/342 [05:05<01:28,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 266/342 [05:06<01:27,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 267/342 [05:07<01:25,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 268/342 [05:08<01:24,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 269/342 [05:09<01:23,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 270/342 [05:11<01:22,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 271/342 [05:12<01:21,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 272/342 [05:13<01:19,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 273/342 [05:14<01:18,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 274/342 [05:15<01:17,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 275/342 [05:16<01:16,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 276/342 [05:17<01:15,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 277/342 [05:19<01:14,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 278/342 [05:20<01:12,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 279/342 [05:21<01:11,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 280/342 [05:22<01:10,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 281/342 [05:23<01:09,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 282/342 [05:24<01:08,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 283/342 [05:25<01:07,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 284/342 [05:27<01:06,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 285/342 [05:28<01:05,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 286/342 [05:29<01:03,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 287/342 [05:30<01:02,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 288/342 [05:31<01:01,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 289/342 [05:32<01:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 290/342 [05:33<00:59,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 291/342 [05:35<00:58,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 292/342 [05:36<00:57,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 293/342 [05:37<00:56,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 294/342 [05:38<00:55,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 295/342 [05:39<00:53,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 296/342 [05:40<00:52,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 297/342 [05:42<00:51,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 298/342 [05:43<00:50,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 299/342 [05:44<00:49,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 300/342 [05:45<00:48,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 301/342 [05:46<00:47,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 302/342 [05:47<00:45,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 303/342 [05:48<00:44,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 304/342 [05:50<00:43,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 305/342 [05:51<00:42,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 306/342 [05:52<00:41,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 307/342 [05:53<00:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 308/342 [05:54<00:39,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 309/342 [05:55<00:37,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 310/342 [05:56<00:36,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 311/342 [05:58<00:35,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 312/342 [05:59<00:34,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 313/342 [06:00<00:33,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 314/342 [06:01<00:32,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 315/342 [06:02<00:30,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 316/342 [06:03<00:29,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 317/342 [06:04<00:28,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 318/342 [06:06<00:27,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 319/342 [06:07<00:26,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 320/342 [06:08<00:25,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 321/342 [06:09<00:24,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 322/342 [06:10<00:22,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 323/342 [06:11<00:21,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 324/342 [06:12<00:20,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 325/342 [06:14<00:19,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 326/342 [06:15<00:18,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 327/342 [06:16<00:17,  1.14s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 328/342 [06:17<00:16,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 329/342 [06:18<00:14,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 330/342 [06:19<00:13,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 331/342 [06:20<00:12,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 332/342 [06:22<00:11,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 333/342 [06:23<00:10,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 334/342 [06:24<00:09,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 335/342 [06:25<00:08,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 336/342 [06:26<00:06,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 337/342 [06:27<00:05,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 338/342 [06:29<00:04,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 339/342 [06:30<00:03,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 340/342 [06:31<00:02,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 341/342 [06:32<00:01,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 342/342 [06:33<00:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.9762113094329834, 'eval_runtime': 395.0149, 'eval_samples_per_second': 3.463, 'eval_steps_per_second': 0.866, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 856/856 [1:53:14<00:00,  7.44s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 342/342 [06:33<00:00,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 6794.4877, 'train_samples_per_second': 1.008, 'train_steps_per_second': 0.126, 'train_loss': 0.9837665023090684, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 856/856 [1:53:14<00:00,  7.44s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 856/856 [1:53:14<00:00,  7.94s/it]\u001b[0m\n",
      "\u001b[34m2024-04-14 19:03:00,587 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-14 19:03:00,587 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-14 19:03:00,588 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-14 19:04:10 Uploading - Uploading generated training model\n",
      "2024-04-14 19:04:10 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 7426\n",
      "Billable seconds: 7426\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 8 gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# next"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

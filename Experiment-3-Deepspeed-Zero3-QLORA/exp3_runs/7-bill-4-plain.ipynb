{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "In this section we'll import the requisite libraries and instantiate a number of objects and variables to configure our training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                        # SageMaker Python SDK\n",
    "from sagemaker.pytorch import PyTorch                   # PyTorch Estimator for running pytorch training jobs\n",
    "from sagemaker.debugger import TensorBoardOutputConfig  # Debugger TensorBoard config to log training metrics to TensorBoard\n",
    "import boto3                                            # AWS SDK for Python\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()   # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_key_prefix = \"7-bill\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl -O data/dialogsum.train.jsonl\n",
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.test.jsonl -O data/dialogsum.test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accelerate launch` command has two key parts, the `config.yml` file and the `train.py` script. The `config.yml` file is used to configure the distributed training job. The `train.py` script is the training script that will be launched by the launcher. In this example, we'll use the [ds_zero3.yml](src/train/ds_zero3.yaml) configuration file. The config file enables [DeepSpeed ZeRo Stage3](#https://www.deepspeed.ai/tutorials/zero/) and a number of other optimizations to enable training of large scale models. This file was generated by running `accelerate config --config_file ds_zero3.yml` and then following the on-screen prompts. \n",
    "The [train.py](src/train/train.py) makes use of a number of key libraries to enable training of large models with minimal code changes:\n",
    "- ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) - Configures the distributed training environment and adapts training objects (data loaders, models, optimizers) to the distributed environment\n",
    "- ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index) - Provides a number of pre-trained models and utilities for training and evaluating models\n",
    "- ğŸ¤— [PEFT](https://github.com/huggingface/peft) - Provides a number of methods for Parameter Efficient Finetuning(PEFT) of large language models. The [LoRA](https://arxiv.org/pdf/2106.09685.pdf) method will be used to finetune the model\n",
    "- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Provides a number of optimizations to enable training of large models. In this example, we'll use DeepSpeed ZeRO Stage3 to enable training of models with over 1B parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the tesnorboard output directly to S3\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://{bucket}/{s3_key_prefix}/tensorboard\"\n",
    ")\n",
    "\n",
    "image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker'\n",
    "estimator1 = PyTorch(\n",
    "    source_dir = \"src/train\",\n",
    "    entry_point=\"acc_launcher.py\",\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    framework_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    hyperparameters = {\n",
    "    \"training_script\": \"train_qlora.py\",\n",
    "    \"config_file\": \"qlora.yaml\",\n",
    "    \"seed\": 100,\n",
    "    \"model_name_or_path\": \"NousResearch/Llama-2-7b-hf\",\n",
    "    \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
    "    \"chat_template_format\": \"chatml\",\n",
    "    \"add_special_tokens\": False,\n",
    "    \"append_concat_token\": False,\n",
    "    \"splits\": \"train,test\",\n",
    "    \"max_seq_len\": 2048,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"bf16\": True,\n",
    "    \"packing\": True,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_ratio\": 0.0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"use_reentrant\": True,\n",
    "    \"dataset_text_field\": \"content\",\n",
    "    \"use_flash_attn\": False,\n",
    "    \"use_peft_lora\": False,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": \"all-linear\",\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_nested_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
    "    \"report_to\":\"none\",\n",
    "\n",
    "},\n",
    "\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-04-10-20-57-52-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-10 20:57:53 Starting - Starting the training job...\n",
      "2024-04-10 20:58:09 Pending - Training job waiting for capacity...\n",
      "2024-04-10 20:58:35 Pending - Preparing the instances for training........................\n",
      "2024-04-10 21:02:46 Downloading - Downloading input data...\n",
      "2024-04-10 21:03:21 Downloading - Downloading the training image.....................\n",
      "2024-04-10 21:06:37 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-10 21:07:50,090 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-10 21:07:50,187 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-10 21:07:50,195 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:07:50,196 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:07:51,471 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers to /tmp/pip-req-build-8h0v_tho\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-8h0v_tho\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers to commit d71f5b3ea8b4a3a530d0c868e87d57311b177c1c\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/accelerate (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-ipnlj_9p\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-ipnlj_9p\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/accelerate to commit e9b9c7d022098a53e0ec207b2803ef4ded2d40ea\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/peft (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/peft to /tmp/pip-req-build-d0mm3lop\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-d0mm3lop\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/peft to commit 31c884e93469dd1391bb54eb1468311c38bbccac\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/trl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/trl to /tmp/pip-req-build-p5as1ju9\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-p5as1ju9\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/trl to commit 57aebe9c36021e679662181468b04ff42432daf3\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/datatrove.git (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/datatrove.git to /tmp/pip-req-build-8vcp59pj\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/datatrove.git /tmp/pip-req-build-8vcp59pj\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/datatrove.git to commit 5015a4c7044235be9578b115d1865d2a23de98c5\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting unsloth[conda]@ git+https://github.com/unslothai/unsloth.git (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-gasy9pxh/unsloth_f759460fbd7a4091af067b4289b57a9b\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-gasy9pxh/unsloth_f759460fbd7a4091af067b4289b57a9b\u001b[0m\n",
      "\u001b[34mResolved https://github.com/unslothai/unsloth.git to commit 4606443b77f98a624896d4ca50710255d8436d86\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting PyGithub (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyGithub-2.3.0-py3-none-any.whl (354 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 354.4/354.4 kB 16.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 388.9/388.9 kB 31.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 84.1/84.1 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 510.5/510.5 kB 37.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 102.2/102.2 MB 14.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.6.1)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.2/2.2 MB 85.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.5/5.5 MB 80.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.8/1.8 MB 87.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (3.7.2)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 86.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.5/1.5 MB 90.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xformers (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 222.5/222.5 MB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting hf_transfer (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.4/4.4 MB 91.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 774.0/774.0 kB 71.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.6/3.6 MB 84.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers==4.40.0.dev0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 79.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl==0.8.2.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 102.0/102.0 kB 28.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.12.2 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 172.0/172.0 kB 39.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting humanize (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.9.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.8/126.8 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting loguru>=0.7.0 (from datatrove==0.0.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 62.5/62.5 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datatrove==0.0.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18.2/18.2 MB 63.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynacl>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.5.0)\u001b[0m\n",
      "\u001b[34mCollecting pyjwt[crypto]>=2.4.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub->-r requirements.txt (line 8)) (1.26.15)\u001b[0m\n",
      "\u001b[34mCollecting Deprecated (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 194.1/194.1 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 11)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 12)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 77.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (8.1.4)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 207.3/207.3 kB 34.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 267.1/267.1 kB 37.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 15)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 133.7/133.7 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.5/5.5 MB 97.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 105.4/105.4 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.6/6.6 MB 96.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 18)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (4.41.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 21)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 23)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting torch>=1.10.0 (from accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 755.5/755.5 MB 1.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.0.0 (from PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mUsing cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23.7/23.7 MB 52.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 823.6/823.6 kB 76.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14.1/14.1 MB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 731.7/731.7 MB 1.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 410.6/410.6 MB 3.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 121.6/121.6 MB 12.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.5/56.5 MB 25.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 124.2/124.2 MB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 196.0/196.0 MB 7.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 166.0/166.0 MB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 99.1/99.1 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting triton==2.2.0 (from torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 167.9/167.9 MB 9.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.1/21.1 MB 64.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting fsspec[http]>=2021.05.0 (from evaluate->-r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 170.9/170.9 kB 35.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 239.5/239.5 kB 46.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 124.3/124.3 kB 27.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 301.6/301.6 kB 53.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 62.7/62.7 kB 13.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub->-r requirements.txt (line 8)) (41.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4)) (13.4.2)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 16)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10 (from Deprecated->PyGithub->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 80.3/80.3 kB 21.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wheel>=0.42.0 (from unsloth[conda]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub->-r requirements.txt (line 8)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.0.dev0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.2.dev0->-r requirements.txt (line 4)) (0.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, accelerate, peft, trl, datatrove, unsloth\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8829432 sha256=f160b3075787a0157bf2a202c8decb0ea3adc3335096181e52dc133178c8f41a\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for accelerate (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for accelerate: filename=accelerate-0.30.0.dev0-py3-none-any.whl size=297653 sha256=3005cd20a068f4d48ce5d220351326f8ba308c2f27c08843dcf93118e92c6d07\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/f6/c7/9d/1b8a5ca8353d9307733bc719107acb67acdc95063bba749f26\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for peft (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for peft: filename=peft-0.10.1.dev0-py3-none-any.whl size=202303 sha256=c6109b13cc7d5173662a7cd16eefcf1f69d3bfe364b49e60751fa7e2d2fbba54\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for trl (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for trl: filename=trl-0.8.2.dev0-py3-none-any.whl size=240006 sha256=98257edf6d8a829e3fd18932f71428374f83db3552d1eb748eac675bf5b2a23e\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/6a/aa/56/d64d9ae3521350622f9325fdc3bccb4dd3d3ec1c1d8e917400\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for datatrove (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for datatrove: filename=datatrove-0.0.1-py3-none-any.whl size=16639225 sha256=f54fa956b6589ec0a3ef7a7497f4f5795384e4171b7f5fa082685ce1284c0af3\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/8a/74/96/5af76f3e0504a1ed76602ea64ffadeb493297b90d375394cd0\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for unsloth (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for unsloth: filename=unsloth-2024.4-py3-none-any.whl size=96655 sha256=27af94ed0b3e7f5d0bf95fc5fddd50d68d294b10f93c9fcb4fa8c014496f5682\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-phl91364/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers accelerate peft trl datatrove unsloth\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, appdirs, xxhash, wrapt, wheel, unsloth, typing-extensions, triton, tensorboard-data-server, smmap, shtab, setproctitle, sentry-sdk, safetensors, regex, pyjwt, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown, loguru, humanize, hf_transfer, grpcio, fsspec, frozenlist, docstring-parser, docker-pycreds, async-timeout, absl-py, yarl, tiktoken, tensorboard, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, huggingface-hub, gitdb, Deprecated, aiosignal, tyro, tokenizers, nvidia-cusolver-cu12, GitPython, datatrove, aiohttp, wandb, transformers, torch, PyGithub, xformers, datasets, bitsandbytes, accelerate, trl, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: wheel\u001b[0m\n",
      "\u001b[34mFound existing installation: wheel 0.40.0\u001b[0m\n",
      "\u001b[34mUninstalling wheel-0.40.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled wheel-0.40.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.24.4\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.24.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.24.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.6.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.6.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.6.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.20.3\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Deprecated-1.2.14 GitPython-3.1.43 PyGithub-2.3.0 absl-py-2.1.0 accelerate-0.30.0.dev0 aiohttp-3.9.3 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.43.0 datasets-2.18.0 datatrove-0.0.1 docker-pycreds-0.4.0 docstring-parser-0.16 evaluate-0.4.1 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 grpcio-1.62.1 hf_transfer-0.1.6 huggingface-hub-0.22.2 humanize-4.9.0 loguru-0.7.2 markdown-3.6 multidict-6.0.5 nltk-3.8.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 peft-0.10.1.dev0 pyarrow-hotfix-0.6 pyjwt-2.8.0 regex-2023.12.25 responses-0.18.0 safetensors-0.4.2 sentencepiece-0.2.0 sentry-sdk-1.45.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.2 transformers-4.40.0.dev0 triton-2.2.0 trl-0.8.2.dev0 typing-extensions-4.11.0 tyro-0.8.3 unsloth-2024.4 wandb-0.16.6 wheel-0.43.0 wrapt-1.16.0 xformers-0.0.25.post1 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,639 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,639 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,769 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,875 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,979 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:16,987 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_special_tokens\": false,\n",
      "        \"append_concat_token\": false,\n",
      "        \"bf16\": true,\n",
      "        \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "        \"bnb_4bit_quant_storage_dtype\": \"bfloat16\",\n",
      "        \"chat_template_format\": \"chatml\",\n",
      "        \"config_file\": \"qlora.yaml\",\n",
      "        \"dataset_name\": \"smangrul/ultrachat-10k-chatml\",\n",
      "        \"dataset_text_field\": \"content\",\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 8,\n",
      "        \"lora_target_modules\": \"all-linear\",\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_grad_norm\": 1.0,\n",
      "        \"max_seq_len\": 2048,\n",
      "        \"model_name_or_path\": \"NousResearch/Llama-2-7b-hf\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"llama-sft-qlora-dsz3\",\n",
      "        \"packing\": true,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"report_to\": \"none\",\n",
      "        \"seed\": 100,\n",
      "        \"splits\": \"train,test\",\n",
      "        \"training_script\": \"train_qlora.py\",\n",
      "        \"use_4bit_quantization\": false,\n",
      "        \"use_flash_attn\": false,\n",
      "        \"use_nested_quant\": true,\n",
      "        \"use_peft_lora\": false,\n",
      "        \"use_reentrant\": true,\n",
      "        \"warmup_ratio\": 0.0,\n",
      "        \"weight_decay\": 0.0001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-10-20-57-52-597\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-10-20-57-52-597/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"acc_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"acc_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-7b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":false,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":false,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=acc_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=acc_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-10-20-57-52-597/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_special_tokens\":false,\"append_concat_token\":false,\"bf16\":true,\"bnb_4bit_compute_dtype\":\"bfloat16\",\"bnb_4bit_quant_storage_dtype\":\"bfloat16\",\"chat_template_format\":\"chatml\",\"config_file\":\"qlora.yaml\",\"dataset_name\":\"smangrul/ultrachat-10k-chatml\",\"dataset_text_field\":\"content\",\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"learning_rate\":0.0001,\"logging_strategy\":\"steps\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":8,\"lora_target_modules\":\"all-linear\",\"lr_scheduler_type\":\"cosine\",\"max_grad_norm\":1.0,\"max_seq_len\":2048,\"model_name_or_path\":\"NousResearch/Llama-2-7b-hf\",\"num_train_epochs\":1,\"output_dir\":\"llama-sft-qlora-dsz3\",\"packing\":true,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"report_to\":\"none\",\"seed\":100,\"splits\":\"train,test\",\"training_script\":\"train_qlora.py\",\"use_4bit_quantization\":false,\"use_flash_attn\":false,\"use_nested_quant\":true,\"use_peft_lora\":false,\"use_reentrant\":true,\"warmup_ratio\":0.0,\"weight_decay\":0.0001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-10-20-57-52-597\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-10-20-57-52-597/source/sourcedir.tar.gz\",\"module_name\":\"acc_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"acc_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_special_tokens\",\"False\",\"--append_concat_token\",\"False\",\"--bf16\",\"True\",\"--bnb_4bit_compute_dtype\",\"bfloat16\",\"--bnb_4bit_quant_storage_dtype\",\"bfloat16\",\"--chat_template_format\",\"chatml\",\"--config_file\",\"qlora.yaml\",\"--dataset_name\",\"smangrul/ultrachat-10k-chatml\",\"--dataset_text_field\",\"content\",\"--evaluation_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0001\",\"--logging_strategy\",\"steps\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"8\",\"--lora_target_modules\",\"all-linear\",\"--lr_scheduler_type\",\"cosine\",\"--max_grad_norm\",\"1.0\",\"--max_seq_len\",\"2048\",\"--model_name_or_path\",\"NousResearch/Llama-2-7b-hf\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"llama-sft-qlora-dsz3\",\"--packing\",\"True\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--report_to\",\"none\",\"--seed\",\"100\",\"--splits\",\"train,test\",\"--training_script\",\"train_qlora.py\",\"--use_4bit_quantization\",\"False\",\"--use_flash_attn\",\"False\",\"--use_nested_quant\",\"True\",\"--use_peft_lora\",\"False\",\"--use_reentrant\",\"True\",\"--warmup_ratio\",\"0.0\",\"--weight_decay\",\"0.0001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_SPECIAL_TOKENS=false\u001b[0m\n",
      "\u001b[34mSM_HP_APPEND_CONCAT_TOKEN=false\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_COMPUTE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_QUANT_STORAGE_DTYPE=bfloat16\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE_FORMAT=chatml\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=qlora.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=smangrul/ultrachat-10k-chatml\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_TEXT_FIELD=content\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_TARGET_MODULES=all-linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=NousResearch/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=llama-sft-qlora-dsz3\u001b[0m\n",
      "\u001b[34mSM_HP_PACKING=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=none\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=100\u001b[0m\n",
      "\u001b[34mSM_HP_SPLITS=train,test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=train_qlora.py\u001b[0m\n",
      "\u001b[34mSM_HP_USE_4BIT_QUANTIZATION=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_NESTED_QUANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_PEFT_LORA=false\u001b[0m\n",
      "\u001b[34mSM_HP_USE_REENTRANT=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 acc_launcher.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --config_file qlora.yaml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-7b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --training_script train_qlora.py --use_4bit_quantization False --use_flash_attn False --use_nested_quant True --use_peft_lora False --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m2024-04-10 21:10:17,015 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcommand = accelerate launch --config_file qlora.yaml train_qlora.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-7b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --use_4bit_quantization False --use_flash_attn False --use_nested_quant True --use_peft_lora False --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:19,738] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:19,738] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:19,738] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:19,738] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.6.1+1ea3d4b)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mCollecting deepspeed\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 42.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 39.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 27.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.3/1.3 MB 28.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53.1/53.1 kB 1.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53.1/53.1 kB 1.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53.1/53.1 kB 842.8 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53.1/53.1 kB 560.4 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400399 sha256=e4a2c90e0ec7449b32a34a99845e9d7dfb47150ad06e77c7bf01ca0a660d87df\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400396 sha256=972f0208adbafa95d3a674c10695d4963a858a27b549f433bff116590bdc30fb\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400398 sha256=8ea550929af15a9771a8882b14b2654ec0ccad18ca9c119d8317d7128f3debe6\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400397 sha256=d8f132c85436d34aa2ff4ceef5a0bb2d14c492df262eab46cc3e2245b93b2e4c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mWARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mCan't uninstall 'deepspeed'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.3\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.2.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.3\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.2.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,393] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.3\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.2.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34mPackages installed/updated successfully.\u001b[0m\n",
      "\u001b[34munsloth 2024.4\u001b[0m\n",
      "\u001b[34mfsspec 2024.2.0\u001b[0m\n",
      "\u001b[34mregex 2023.12.25\u001b[0m\n",
      "\u001b[34mjsonschema-specifications 2023.6.1\u001b[0m\n",
      "\u001b[34mcertifi 2023.5.7\u001b[0m\n",
      "\u001b[34mxyzservices 2023.5.0\u001b[0m\n",
      "\u001b[34mpytz 2023.3\u001b[0m\n",
      "\u001b[34mtzdata 2023.3\u001b[0m\n",
      "\u001b[34msetuptools 65.6.3\u001b[0m\n",
      "\u001b[34mcryptography 41.0.2\u001b[0m\n",
      "\u001b[34mpyzmq 25.1.0\u001b[0m\n",
      "\u001b[34mgevent 23.7.0\u001b[0m\n",
      "\u001b[34mpyOpenSSL 23.2.0\u001b[0m\n",
      "\u001b[34mpip 23.1.2\u001b[0m\n",
      "\u001b[34mconda 23.1.0\u001b[0m\n",
      "\u001b[34mattrs 23.1.0\u001b[0m\n",
      "\u001b[34mpackaging 23.1\u001b[0m\n",
      "\u001b[34mcontextlib2 21.6.0\u001b[0m\n",
      "\u001b[34mlit 16.0.6\u001b[0m\n",
      "\u001b[34municodedata2 15.0.0\u001b[0m\n",
      "\u001b[34mrich 13.4.2\u001b[0m\n",
      "\u001b[34mPyQt5-sip 12.11.0\u001b[0m\n",
      "\u001b[34mnvidia-nvjitlink-cu12 12.4.127\u001b[0m\n",
      "\u001b[34mnvidia-nvtx-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-runtime-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-nvrtc-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cuda-cupti-cu12 12.1.105\u001b[0m\n",
      "\u001b[34mnvidia-cublas-cu12 12.1.3.1\u001b[0m\n",
      "\u001b[34mnvidia-cusparse-cu12 12.1.0.106\u001b[0m\n",
      "\u001b[34mpyarrow 12.0.1\u001b[0m\n",
      "\u001b[34mnvidia-cusolver-cu12 11.4.5.107\u001b[0m\n",
      "\u001b[34mnvidia-cufft-cu12 11.0.2.54\u001b[0m\n",
      "\u001b[34mnvidia-curand-cu12 10.3.2.106\u001b[0m\n",
      "\u001b[34mPillow 10.0.0\u001b[0m\n",
      "\u001b[34mpy-cpuinfo 9.0.0\u001b[0m\n",
      "\u001b[34mipython 8.14.0\u001b[0m\n",
      "\u001b[34mnvidia-cudnn-cu12 8.9.2.26\u001b[0m\n",
      "\u001b[34mjupyter-client 8.3.0\u001b[0m\n",
      "\u001b[34mtenacity 8.2.2\u001b[0m\n",
      "\u001b[34mthinc 8.1.10\u001b[0m\n",
      "\u001b[34mclick 8.1.4\u001b[0m\n",
      "\u001b[34mipykernel 6.24.0\u001b[0m\n",
      "\u001b[34mimportlib-metadata 6.8.0\u001b[0m\n",
      "\u001b[34msip 6.7.9\u001b[0m\n",
      "\u001b[34mtornado 6.3.2\u001b[0m\n",
      "\u001b[34mmultidict 6.0.5\u001b[0m\n",
      "\u001b[34mPyYAML 6.0\u001b[0m\n",
      "\u001b[34mzope.interface 6.0\u001b[0m\n",
      "\u001b[34mPyQt5 5.15.7\u001b[0m\n",
      "\u001b[34mplotly 5.15.0\u001b[0m\n",
      "\u001b[34mpsutil 5.9.5\u001b[0m\n",
      "\u001b[34mtraitlets 5.9.0\u001b[0m\n",
      "\u001b[34mjupyter-core 5.3.1\u001b[0m\n",
      "\u001b[34msmart-open 5.2.1\u001b[0m\n",
      "\u001b[34mdecorator 5.1.1\u001b[0m\n",
      "\u001b[34msmmap 5.0.1\u001b[0m\n",
      "\u001b[34mzope.event 5.0\u001b[0m\n",
      "\u001b[34mtqdm 4.65.0\u001b[0m\n",
      "\u001b[34mfonttools 4.41.0\u001b[0m\n",
      "\u001b[34mtransformers 4.40.0.dev0\u001b[0m\n",
      "\u001b[34mjsonschema 4.18.3\u001b[0m\n",
      "\u001b[34mtyping-extensions 4.11.0\u001b[0m\n",
      "\u001b[34mhumanize 4.9.0\u001b[0m\n",
      "\u001b[34mpexpect 4.8.0\u001b[0m\n",
      "\u001b[34mrsa 4.7.2\u001b[0m\n",
      "\u001b[34msagemaker-training 4.6.1\u001b[0m\n",
      "\u001b[34mopencv-python 4.6.0\u001b[0m\n",
      "\u001b[34mgitdb 4.0.11\u001b[0m\n",
      "\u001b[34masync-timeout 4.0.3\u001b[0m\n",
      "\u001b[34mbcrypt 4.0.1\u001b[0m\n",
      "\u001b[34mcmake 3.26.4\u001b[0m\n",
      "\u001b[34mprotobuf 3.20.3\u001b[0m\n",
      "\u001b[34mzipp 3.16.0\u001b[0m\n",
      "\u001b[34mfilelock 3.12.2\u001b[0m\n",
      "\u001b[34mply 3.11\u001b[0m\n",
      "\u001b[34maiohttp 3.9.3\u001b[0m\n",
      "\u001b[34mnltk 3.8.1\u001b[0m\n",
      "\u001b[34mplatformdirs 3.8.1\u001b[0m\n",
      "\u001b[34mmatplotlib 3.7.2\u001b[0m\n",
      "\u001b[34mh5py 3.7.0\u001b[0m\n",
      "\u001b[34mMarkdown 3.6\u001b[0m\n",
      "\u001b[34mspacy 3.6.0\u001b[0m\n",
      "\u001b[34mpyinstrument 3.4.2\u001b[0m\n",
      "\u001b[34mxxhash 3.4.1\u001b[0m\n",
      "\u001b[34midna 3.4\u001b[0m\n",
      "\u001b[34mlangcodes 3.3.0\u001b[0m\n",
      "\u001b[34mbokeh 3.2.0\u001b[0m\n",
      "\u001b[34mparamiko 3.2.0\u001b[0m\n",
      "\u001b[34mthreadpoolctl 3.2.0\u001b[0m\n",
      "\u001b[34mGitPython 3.1.43\u001b[0m\n",
      "\u001b[34mmpi4py 3.1.4\u001b[0m\n",
      "\u001b[34mJinja2 3.1.2\u001b[0m\n",
      "\u001b[34mcharset-normalizer 3.1.0\u001b[0m\n",
      "\u001b[34mhjson 3.1.0\u001b[0m\n",
      "\u001b[34mnetworkx 3.1\u001b[0m\n",
      "\u001b[34mprompt-toolkit 3.0.39\u001b[0m\n",
      "\u001b[34mspacy-legacy 3.0.12\u001b[0m\n",
      "\u001b[34mpyparsing 3.0.9\u001b[0m\n",
      "\u001b[34mpreshed 3.0.8\u001b[0m\n",
      "\u001b[34mmarkdown-it-py 3.0.0\u001b[0m\n",
      "\u001b[34msagemaker 2.172.0\u001b[0m\n",
      "\u001b[34mimageio 2.31.1\u001b[0m\n",
      "\u001b[34mrequests 2.31.0\u001b[0m\n",
      "\u001b[34mpycparser 2.21\u001b[0m\n",
      "\u001b[34mnvidia-nccl-cu12 2.19.3\u001b[0m\n",
      "\u001b[34mdatasets 2.18.0\u001b[0m\n",
      "\u001b[34mtensorboard 2.16.2\u001b[0m\n",
      "\u001b[34mPygments 2.15.1\u001b[0m\n",
      "\u001b[34mpybind11 2.10.4\u001b[0m\n",
      "\u001b[34mpybind11-global 2.10.4\u001b[0m\n",
      "\u001b[34mpython-dateutil 2.8.2\u001b[0m\n",
      "\u001b[34mPyJWT 2.8.0\u001b[0m\n",
      "\u001b[34msagemaker-pytorch-training 2.8.0\u001b[0m\n",
      "\u001b[34mfastai 2.7.12\u001b[0m\n",
      "\u001b[34msrsly 2.4.6\u001b[0m\n",
      "\u001b[34mjsonpointer 2.4\u001b[0m\n",
      "\u001b[34mWerkzeug 2.3.6\u001b[0m\n",
      "\u001b[34mPyGithub 2.3.0\u001b[0m\n",
      "\u001b[34mtorch 2.2.2\u001b[0m\n",
      "\u001b[34mcloudpickle 2.2.1\u001b[0m\n",
      "\u001b[34masttokens 2.2.1\u001b[0m\n",
      "\u001b[34mtriton 2.2.0\u001b[0m\n",
      "\u001b[34mMarkupSafe 2.1.3\u001b[0m\n",
      "\u001b[34mgmpy2 2.1.2\u001b[0m\n",
      "\u001b[34mabsl-py 2.1.0\u001b[0m\n",
      "\u001b[34mcatalogue 2.0.8\u001b[0m\n",
      "\u001b[34mcymem 2.0.7\u001b[0m\n",
      "\u001b[34mpandas 2.0.3\u001b[0m\n",
      "\u001b[34mconda-package-handling 2.0.2\u001b[0m\n",
      "\u001b[34mgreenlet 2.0.2\u001b[0m\n",
      "\u001b[34mtorchaudio 2.0.1\u001b[0m\n",
      "\u001b[34mtomli 2.0.1\u001b[0m\n",
      "\u001b[34mgrpcio 1.62.1\u001b[0m\n",
      "\u001b[34msentry-sdk 1.45.0\u001b[0m\n",
      "\u001b[34mjsonpatch 1.33\u001b[0m\n",
      "\u001b[34mbotocore 1.31.2\u001b[0m\n",
      "\u001b[34mawscli 1.29.2\u001b[0m\n",
      "\u001b[34mboto3 1.28.2\u001b[0m\n",
      "\u001b[34murllib3 1.26.15\u001b[0m\n",
      "\u001b[34mnumpy 1.26.4\u001b[0m\n",
      "\u001b[34mwrapt 1.16.0\u001b[0m\n",
      "\u001b[34msix 1.16.0\u001b[0m\n",
      "\u001b[34mcffi 1.15.1\u001b[0m\n",
      "\u001b[34msmdistributed-modelparallel 1.15.0\u001b[0m\n",
      "\u001b[34msympy 1.12\u001b[0m\n",
      "\u001b[34mninja 1.11.1\u001b[0m\n",
      "\u001b[34mscipy 1.11.1\u001b[0m\n",
      "\u001b[34mpydantic 1.10.11\u001b[0m\n",
      "\u001b[34myarl 1.9.4\u001b[0m\n",
      "\u001b[34msmdistributed-dataparallel 1.8.0\u001b[0m\n",
      "\u001b[34mppft 1.7.6.6\u001b[0m\n",
      "\u001b[34mPySocks 1.7.1\u001b[0m\n",
      "\u001b[34mshtab 1.7.1\u001b[0m\n",
      "\u001b[34mtblib 1.7.0\u001b[0m\n",
      "\u001b[34mpooch 1.7.0\u001b[0m\n",
      "\u001b[34mdebugpy 1.6.7\u001b[0m\n",
      "\u001b[34mbackports.functools-lru-cache 1.6.5\u001b[0m\n",
      "\u001b[34mwebsocket-client 1.6.1\u001b[0m\n",
      "\u001b[34mfastcore 1.5.29\u001b[0m\n",
      "\u001b[34mnest-asyncio 1.5.6\u001b[0m\n",
      "\u001b[34mcached-property 1.5.2\u001b[0m\n",
      "\u001b[34mshellingham 1.5.1\u001b[0m\n",
      "\u001b[34mPyNaCl 1.5.0\u001b[0m\n",
      "\u001b[34mlibmambapy 1.4.9\u001b[0m\n",
      "\u001b[34mmamba 1.4.9\u001b[0m\n",
      "\u001b[34mappdirs 1.4.4\u001b[0m\n",
      "\u001b[34mkiwisolver 1.4.4\u001b[0m\n",
      "\u001b[34mpyfunctional 1.4.3\u001b[0m\n",
      "\u001b[34mfrozenlist 1.4.1\u001b[0m\n",
      "\u001b[34margparse 1.4.0\u001b[0m\n",
      "\u001b[34mretrying 1.3.4\u001b[0m\n",
      "\u001b[34msetproctitle 1.3.3\u001b[0m\n",
      "\u001b[34maiosignal 1.3.1\u001b[0m\n",
      "\u001b[34mjoblib 1.3.0\u001b[0m\n",
      "\u001b[34mscikit-learn 1.3.0\u001b[0m\n",
      "\u001b[34mmpmath 1.3.0\u001b[0m\n",
      "\u001b[34mDeprecated 1.2.14\u001b[0m\n",
      "\u001b[34minotify-simple 1.2.1\u001b[0m\n",
      "\u001b[34mexecuting 1.2.0\u001b[0m\n",
      "\u001b[34mmunkres 1.1.4\u001b[0m\n",
      "\u001b[34mwasabi 1.1.2\u001b[0m\n",
      "\u001b[34mdgl 1.1.1+cu118\u001b[0m\n",
      "\u001b[34mcontourpy 1.1.0\u001b[0m\n",
      "\u001b[34msmdebug 1.0.34\u001b[0m\n",
      "\u001b[34mmurmurhash 1.0.9\u001b[0m\n",
      "\u001b[34mspacy-loggers 1.0.4\u001b[0m\n",
      "\u001b[34mfastprogress 1.0.3\u001b[0m\n",
      "\u001b[34msmdebug-rulesconfig 1.0.1\u001b[0m\n",
      "\u001b[34mjmespath 1.0.1\u001b[0m\n",
      "\u001b[34mpluggy 1.0.0\u001b[0m\n",
      "\u001b[34mmultiprocess 0.70.14\u001b[0m\n",
      "\u001b[34mnumba 0.57.1\u001b[0m\n",
      "\u001b[34mwheel 0.43.0\u001b[0m\n",
      "\u001b[34mbitsandbytes 0.43.0\u001b[0m\n",
      "\u001b[34mshap 0.42.0\u001b[0m\n",
      "\u001b[34mllvmlite 0.40.1\u001b[0m\n",
      "\u001b[34maccelerate 0.30.0.dev0\u001b[0m\n",
      "\u001b[34mCython 0.29.36\u001b[0m\n",
      "\u001b[34mreferencing 0.29.1\u001b[0m\n",
      "\u001b[34mhorovod 0.26.1\u001b[0m\n",
      "\u001b[34mhuggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mzstandard 0.19.0\u001b[0m\n",
      "\u001b[34mjedi 0.18.2\u001b[0m\n",
      "\u001b[34mresponses 0.18.0\u001b[0m\n",
      "\u001b[34mruamel.yaml 0.17.21\u001b[0m\n",
      "\u001b[34mwandb 0.16.6\u001b[0m\n",
      "\u001b[34mdocstring-parser 0.16\u001b[0m\n",
      "\u001b[34mtokenizers 0.15.2\u001b[0m\n",
      "\u001b[34mdocutils 0.15.2\u001b[0m\n",
      "\u001b[34mtorchtext 0.15.1\u001b[0m\n",
      "\u001b[34mtorchvision 0.15.1\u001b[0m\n",
      "\u001b[34mstatsmodels 0.14.0\u001b[0m\n",
      "\u001b[34mseaborn 0.12.2\u001b[0m\n",
      "\u001b[34mtoolz 0.12.0\u001b[0m\n",
      "\u001b[34mcycler 0.11.0\u001b[0m\n",
      "\u001b[34mpathy 0.10.2\u001b[0m\n",
      "\u001b[34mtoml 0.10.2\u001b[0m\n",
      "\u001b[34mpeft 0.10.1.dev0\u001b[0m\n",
      "\u001b[34mtabulate 0.9.0\u001b[0m\n",
      "\u001b[34mtyper 0.9.0\u001b[0m\n",
      "\u001b[34mrpds-py 0.8.10\u001b[0m\n",
      "\u001b[34mtyro 0.8.3\u001b[0m\n",
      "\u001b[34mparso 0.8.3\u001b[0m\n",
      "\u001b[34mtrl 0.8.2.dev0\u001b[0m\n",
      "\u001b[34mblis 0.7.9\u001b[0m\n",
      "\u001b[34mschema 0.7.5\u001b[0m\n",
      "\u001b[34mpickleshare 0.7.5\u001b[0m\n",
      "\u001b[34mtensorboard-data-server 0.7.2\u001b[0m\n",
      "\u001b[34mloguru 0.7.2\u001b[0m\n",
      "\u001b[34mbrotlipy 0.7.0\u001b[0m\n",
      "\u001b[34mconda-package-streaming 0.7.0\u001b[0m\n",
      "\u001b[34mptyprocess 0.7.0\u001b[0m\n",
      "\u001b[34mpycosat 0.6.4\u001b[0m\n",
      "\u001b[34mstack-data 0.6.2\u001b[0m\n",
      "\u001b[34mdeepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34meinops 0.6.1\u001b[0m\n",
      "\u001b[34ms3transfer 0.6.1\u001b[0m\n",
      "\u001b[34mpyarrow-hotfix 0.6\u001b[0m\n",
      "\u001b[34mtiktoken 0.6.0\u001b[0m\n",
      "\u001b[34mtorchdata 0.6.0\u001b[0m\n",
      "\u001b[34mpatsy 0.5.3\u001b[0m\n",
      "\u001b[34msmclarify 0.5\u001b[0m\n",
      "\u001b[34mpyasn1 0.4.8\u001b[0m\n",
      "\u001b[34mcolorama 0.4.4\u001b[0m\n",
      "\u001b[34msafetensors 0.4.2\u001b[0m\n",
      "\u001b[34ms3fs 0.4.2\u001b[0m\n",
      "\u001b[34mevaluate 0.4.1\u001b[0m\n",
      "\u001b[34mdocker-pycreds 0.4.0\u001b[0m\n",
      "\u001b[34msmppy 0.3.319\u001b[0m\n",
      "\u001b[34mdill 0.3.6\u001b[0m\n",
      "\u001b[34mpox 0.3.2\u001b[0m\n",
      "\u001b[34mpathos 0.3.0\u001b[0m\n",
      "\u001b[34mflash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mruamel.yaml.clib 0.2.7\u001b[0m\n",
      "\u001b[34mwcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mpyinstrument-cext 0.2.4\u001b[0m\n",
      "\u001b[34mvisdom 0.2.4\u001b[0m\n",
      "\u001b[34mpure-eval 0.2.2\u001b[0m\n",
      "\u001b[34msentencepiece 0.2.0\u001b[0m\n",
      "\u001b[34mgoogle-pasta 0.2.0\u001b[0m\n",
      "\u001b[34mbackcall 0.2.0\u001b[0m\n",
      "\u001b[34msagemaker-experiments 0.1.45\u001b[0m\n",
      "\u001b[34mhf-transfer 0.1.6\u001b[0m\n",
      "\u001b[34mmatplotlib-inline 0.1.6\u001b[0m\n",
      "\u001b[34mcomm 0.1.3\u001b[0m\n",
      "\u001b[34mconda-content-trust 0.1.3\u001b[0m\n",
      "\u001b[34mapex 0.1\u001b[0m\n",
      "\u001b[34mconfection 0.1.0\u001b[0m\n",
      "\u001b[34mmdurl 0.1.0\u001b[0m\n",
      "\u001b[34mxformers 0.0.25.post1\u001b[0m\n",
      "\u001b[34mslicer 0.0.7\u001b[0m\n",
      "\u001b[34mfastdownload 0.0.7\u001b[0m\n",
      "\u001b[34mtorchnet 0.0.4\u001b[0m\n",
      "\u001b[34mdatatrove 0.0.1\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,486] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,518] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,641] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,723] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,723] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,815] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:10:41,841] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:44<03:44, 224.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:44<03:44, 224.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:44<03:44, 224.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:44<03:44, 224.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 139.97s/it]#015Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 152.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 139.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 152.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 139.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 152.65s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 139.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:05<00:00, 152.72s/it]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:15:49,535] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  4.02s/it]#015Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.93s/it]#015Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.93s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.45s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr10_21-10-41_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=2,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr10_21-10-41_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=3,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr10_21-10-41_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=False,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.0001,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=llama-sft-qlora-dsz3/runs/Apr10_21-10-41_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=cosine,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moptim_target_modules=None,\u001b[0m\n",
      "\u001b[34moutput_dir=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=2,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=llama-sft-qlora-dsz3,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=100,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0001,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/524 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 900kB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/35.2M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|â–ˆâ–ˆâ–‰       | 10.5M/35.2M [00:00<00:00, 79.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31.5M/35.2M [00:00<00:00, 142MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.2M/35.2M [00:00<00:00, 139MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/7.08M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.08M/7.08M [00:00<00:00, 69.4MB/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 47492.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 50996.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 57283.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8210.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8248.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8103.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9696.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8091.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9934.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9629.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10050.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10533.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9620.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10019.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10832.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10230.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10001.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10205.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10999.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10416.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10176.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10721.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10249.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10392.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10374.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10209.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11182.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11080.21 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11061.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 10962.41 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10193.02 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 9845.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 9745.28 examples/s]\u001b[0m\n",
      "\u001b[34mSize of the train set: 10000. Size of the validation set: 2000\u001b[0m\n",
      "\u001b[34mA sample of train dataset: {'content': \"<|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\\n<|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\\n<|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\\n<|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\\n<|im_start|>user\\nCan you provide me with a link to the documentation for my theme?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\\n<|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\\n<|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\\n\"}\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  2.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 363 examples [00:00, 926.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 693 examples [00:00, 1463.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:01, 914.28 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1376 examples [00:01, 1352.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1755 examples [00:01, 1791.45 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2184 examples [00:01, 1174.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2554 examples [00:02, 1502.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2833 examples [00:02, 1183.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3185 examples [00:02, 1281.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3535 examples [00:03, 1114.49 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3898 examples [00:03, 1425.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4183 examples [00:03, 1503.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4416 examples [00:03, 1064.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4778 examples [00:03, 1402.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5000 examples [00:03, 1452.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5295 examples [00:04, 1068.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5658 examples [00:04, 1416.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5940 examples [00:04, 1587.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6178 examples [00:04, 1179.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6452 examples [00:05, 1415.02 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6822 examples [00:05, 1819.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6848 examples [00:05, 1300.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  3.58 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 368 examples [00:00, 1241.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 693 examples [00:00, 1745.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 1161.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1368 examples [00:01, 1585.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1368 examples [00:01, 1329.53 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mLlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32008, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32008, bias=False)\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/428 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:16:32,043] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 1/428 [00:07<50:25,  7.09s/it]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 182, in <module>\u001b[0m\n",
      "\u001b[34mmain(model_args, data_args, training_args)\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 158, in main\n",
      "    trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 360, in train\u001b[0m\n",
      "\u001b[34moutput = super().train(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1849, in train\u001b[0m\n",
      "\u001b[34mreturn inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 182, in <module>\u001b[0m\n",
      "\u001b[34mmain(model_args, data_args, training_args)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_qlora.py\", line 158, in main\u001b[0m\n",
      "\u001b[34mtr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3137, in training_step\n",
      "    trainer.train(resume_from_checkpoint=checkpoint)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 360, in train\u001b[0m\n",
      "\u001b[34moutput = super().train(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1849, in train\u001b[0m\n",
      "\u001b[34mreturn inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\u001b[0m\n",
      "\u001b[34mself.accelerator.backward(loss)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2007, in backward\u001b[0m\n",
      "\u001b[34mself.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\u001b[0m\n",
      "\u001b[34mtr_loss_step = self.training_step(model, inputs)\n",
      "      File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3137, in training_step\u001b[0m\n",
      "\u001b[34mself.engine.step()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 182, in <module>\u001b[0m\n",
      "\u001b[34mmain(model_args, data_args, training_args)\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 158, in main\u001b[0m\n",
      "\u001b[34mtrainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 360, in train\u001b[0m\n",
      "\u001b[34mself._take_model_step(lr_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\u001b[0m\n",
      "\u001b[34moutput = super().train(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1849, in train\u001b[0m\n",
      "\u001b[34mself.accelerator.backward(loss)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2007, in backward\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\u001b[0m\n",
      "\u001b[34mret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 2050, in step\u001b[0m\n",
      "\u001b[34mreturn inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\u001b[0m\n",
      "\u001b[34mself.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\u001b[0m\n",
      "\u001b[34mself.engine.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\u001b[0m\n",
      "\u001b[34mself._optimizer_step(sub_group_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 947, in _optimizer_step\u001b[0m\n",
      "\u001b[34mtr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3137, in training_step\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\u001b[0m\n",
      "\u001b[34mreturn wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\u001b[0m\n",
      "\u001b[34mself._take_model_step(lr_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\u001b[0m\n",
      "\u001b[34mout = func(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 187, in step\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 182, in <module>\u001b[0m\n",
      "\u001b[34madamw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 339, in adamw\u001b[0m\n",
      "\u001b[34mfunc(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 608, in _multi_tensor_adamw\u001b[0m\n",
      "\u001b[34mmain(model_args, data_args, training_args)\n",
      "  File \"/opt/ml/code/train_qlora.py\", line 158, in main\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\u001b[0m\n",
      "\u001b[34mtrainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 360, in train\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 2050, in step\n",
      "    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 1 has a total capacity of 39.39 GiB of which 3.17 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "    self.accelerator.backward(loss)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2007, in backward\u001b[0m\n",
      "\u001b[34moutput = super().train(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1849, in train\u001b[0m\n",
      "\u001b[34mself._optimizer_step(sub_group_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 947, in _optimizer_step\u001b[0m\n",
      "\u001b[34mself.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\u001b[0m\n",
      "\u001b[34mself.engine.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\u001b[0m\n",
      "\u001b[34mreturn wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\u001b[0m\n",
      "\u001b[34mout = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\u001b[0m\n",
      "\u001b[34mret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 187, in step\u001b[0m\n",
      "\u001b[34madamw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 339, in adamw\u001b[0m\n",
      "\u001b[34mself._take_model_step(lr_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\u001b[0m\n",
      "\u001b[34mfunc(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 608, in _multi_tensor_adamw\u001b[0m\n",
      "\u001b[34mtr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3137, in training_step\u001b[0m\n",
      "\u001b[34mexp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 3 has a total capacity of 39.39 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\u001b[0m\n",
      "\u001b[34mret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 2050, in step\u001b[0m\n",
      "\u001b[34mself.accelerator.backward(loss)    \u001b[0m\n",
      "\u001b[34mself._optimizer_step(sub_group_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2007, in backward\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 947, in _optimizer_step\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\u001b[0m\n",
      "\u001b[34mreturn wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\u001b[0m\n",
      "\u001b[34mout = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\u001b[0m\n",
      "\u001b[34mself.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\n",
      "    ret = func(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 187, in step\u001b[0m\n",
      "\u001b[34mself.engine.step()adamw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 339, in adamw\u001b[0m\n",
      "\u001b[34mfunc(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 608, in _multi_tensor_adamw\u001b[0m\n",
      "\u001b[34mexp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 2 has a total capacity of 39.39 GiB of which 3.17 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "\u001b[34mself._take_model_step(lr_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\u001b[0m\n",
      "\u001b[34mret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 2050, in step\u001b[0m\n",
      "\u001b[34mself._optimizer_step(sub_group_id)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 947, in _optimizer_step\u001b[0m\n",
      "\u001b[34mself.optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\u001b[0m\n",
      "\u001b[34mreturn wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\u001b[0m\n",
      "\u001b[34mout = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\u001b[0m\n",
      "\u001b[34mret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 187, in step\u001b[0m\n",
      "\u001b[34madamw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 339, in adamw\u001b[0m\n",
      "\u001b[34mfunc(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 608, in _multi_tensor_adamw\u001b[0m\n",
      "\u001b[34mexp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 0 has a total capacity of 39.39 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "\u001b[34m0%|          | 1/428 [00:17<2:07:33, 17.92s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-10 21:16:45,114] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 337) of binary: /opt/conda/bin/python3.10\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/accelerate\", line 8, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\u001b[0m\n",
      "\u001b[34margs.func(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1060, in launch_command\u001b[0m\n",
      "\u001b[34mdeepspeed_launcher(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 764, in deepspeed_launcher\u001b[0m\n",
      "\u001b[34mdistrib_run.run(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors\u001b[0m\n",
      "\u001b[34m.ChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mtrain_qlora.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\n",
      "  time      : 2024-04-10_21:16:45\n",
      "  host      : algo-1\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 338)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\n",
      "  time      : 2024-04-10_21:16:45\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 339)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\n",
      "  time      : 2024-04-10_21:16:45\n",
      "  host      : algo-1\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 340)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2024-04-10_21:16:45\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 337)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m2024-04-10 21:16:45,573 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:16:45,573 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-10 21:16:45,574 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-04-10 21:16:45,574 sagemaker-training-toolkit ERROR    modes:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      " warnings.warn(\n",
      " /opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      " /opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      " /opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      " Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.92s/it]\n",
      " Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.66s/it]\n",
      " Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.45s/it]\n",
      " Downloading readme:   0%|          | 0.00/524 [00:00<?, ?B/s]\n",
      " Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 900kB/s]\n",
      " Downloading data:   0%|          | 0.00/35.2M [00:00<?, ?B/s]\n",
      " Downloading data:  30%|â–ˆâ–ˆâ–‰       | 10.5M/35.2M [00:00<00:00, 79.8MB/s]\n",
      " Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31.5M/35.2M [00:00<00:00, 142MB/s]\n",
      " Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.2M/35.2M [00:00<00:00, 139MB/s]\n",
      " Downloading data:   0%|          | 0.00/7.08M [00:00<?, ?B/s]\n",
      " Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.08M/7.08M [00:00<00:00, 69.4MB/s]\n",
      " Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]\n",
      " Generating train split:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 47492.01 examples/s]\n",
      " Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 50996.00 examples/s]\n",
      " Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]\n",
      " Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 57283.19 examples/s]\n",
      " Map:   0%|          | 0/10000 [00:00<?, ? examples/s]\n",
      " Map:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8210.99 examples/s]\n",
      " Map:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8248.64 examples/s]\n",
      " Map:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8103.49 examples/s]\n",
      " Map:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9696.13 examples/s]\n",
      " Map:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 8091.44 examples/s]\n",
      " Map:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9934.68 examples/s]\n",
      " Map:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9629.64 examples/s]\n",
      " Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10050.83 examples/s]\n",
      " Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10533.70 examples/s]\n",
      " Map:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 9620.21 examples/s]\n",
      " Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10019.77 examples/s]\n",
      " Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10832.25 examples/s]\n",
      " Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10230.19 examples/s]\n",
      " Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 10001.01 examples/s]\n",
      " Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10205.70 examples/s]\n",
      " Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10999.13 examples/s]\n",
      " Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10416.57 examples/s]\n",
      " Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 10176.83 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10721.02 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10249.05 examples/s]\n",
      " Map:   0%|          | 0/2000 [00:00<?, ? examples/s]\n",
      " Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10392.14 examples/s]\n",
      " Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 10374.84 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10209.59 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11182.83 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11080.21 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11061.86 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 10962.41 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 10193.02 examples/s]\n",
      " Generating train split: 0 examples [00:00, ? examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 9845.95 examples/s]\n",
      " Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 9745.28 examples/s]\n",
      " Generating train split: 1 examples [00:00,  2.41 examples/s]\n",
      " Generating train split: 363 examples [00:00, 926.89 examples/s]\n",
      " Generating train split: 693 examples [00:00, 1463.69 examples/s]\n",
      " Generating train split: 1000 examples [00:01, 914.28 examples/s]\n",
      " Generating train split: 1376 examples [00:01, 1352.70 examples/s]\n",
      " Generating train split: 1755 examples [00:01, 1791.45 examples/s]\n",
      " Generating train split: 2184 examples [00:01, 1174.87 examples/s]\n",
      " Generating train split: 2554 examples [00:02, 1502.78 examples/s]\n",
      " Generating train split: 2833 examples [00:02, 1183.01 examples/s]\n",
      " Generating train split: 3185 examples [00:02, 1281.42 examples/s]\n",
      " Generating train split: 3535 examples [00:03, 1114.49 examples/s]\n",
      " Generating train split: 3898 examples [00:03, 1425.03 examples/s]\n",
      " Generating train split: 4183 examples [00:03, 1503.83 examples/s]\n",
      " Generating train split: 4416 examples [00:03, 1064.68 examples/s]\n",
      " Generating train split: 4778 examples [00:03, 1402.00 examples/s]\n",
      " Generating train split: 5000 examples [00:03, 1452.68 examples/s]\n",
      " Generating train split: 5295 examples [00:04, 1068.61 examples/s]\n",
      " Generating train split: 5658 examples [00:04, 1416.21 examples/s]\n",
      " Generating train split: 5940 examples [00:04, 1587.36 examples/s]\n",
      " Generating train split: 6178 examples [00:04, 1179.90 examples/s]\n",
      " Generating train split: 6452 examples [00:05, 1415.02 examples/s]\n",
      " Generating train split: 6822 examples [00:05, 1819.68 examples/s]\n",
      " Generating train split: 6848 examples [00:05, 1300.44 examples/s]\n",
      " Generating train split: 1 examples [00:00,  3.58 examples/s]\n",
      " Generating train split: 368 examples [00:00, 1241.85 examples/s]\n",
      " Generating train split: 693 examples [00:00, 1745.21 examples/s]\n",
      " Generating train split: 1000 examples [00:00, 1161.91 examples/s]\n",
      " Generating train split: 1368 examples [00:01, 1585.33 examples/s]\n",
      " Generating train split: 1368 examples [00:01, 1329.53 examples/s]\n",
      " /opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      " 0%|          | 0/428 [00:00<?, ?it/s]\n",
      " 0%|          | 1/428 [00:07<50:25,  7.09s/it]\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/ml/code/train_qlora.py\", line 182, in <module>\n",
      " main(model_args, data_args, training_args)\n",
      " File \"/opt/ml/code/train_qlora.py\", line 158, in main\n",
      " trainer.train(resume_from_checkpoint=checkpoint)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 360, in train\n",
      " output = super().train(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1849, in train\n",
      " return inner_training_loop(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\n",
      " tr_loss_step = self.training_step(model, inputs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3137, in training_step\n",
      " self.accelerator.backward(loss)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2007, in backward\n",
      " self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\n",
      " self.engine.step()\n",
      " File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\n",
      " self._take_model_step(lr_kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\n",
      " self.optimizer.step()\n",
      " File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      " ret_val = func(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 2050, in step\n",
      " self._optimizer_step(sub_group_id)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 947, in _optimizer_step\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      " return wrapped(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      " out = func(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      " ret = func(self, *args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 187, in step\n",
      " adamw(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 339, in adamw\n",
      " func(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py\", line 608, in _multi_tensor_adamw\n",
      " exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 1 has a total capacity of 39.39 GiB of which 3.17 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 3 has a total capacity of 39.39 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      " self.engine.step()adamw(\n",
      " \n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 2 has a total capacity of 39.39 GiB of which 3.17 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB. GPU 0 has a total capacity of 39.39 GiB of which 3.45 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      " 0%|          | 1/428 [00:17<2:07:33, 17.92s/it]\n",
      " [2024-04-10 21:16:45,114] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 337) of binary: /opt/conda/bin/python3.10\n",
      " File \"/opt/conda/bin/accelerate\", line 8, in <module>\n",
      " sys.exit(main())\n",
      " File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      " args.func(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1060, in launch_command\n",
      " deepspeed_launcher(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 764, in deepspeed_launcher\n",
      " distrib_run.run(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors\n",
      " .ChildFailedError\n",
      " ============================================================\n",
      " train_qlora.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " [1]\n",
      " time      : 2024-04-10_21:16:45\n",
      " host      : algo-1\n",
      " rank      : 1 (local_rank: 1)\n",
      " exitcode  : 1 (pid: 338)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      " [2]\n",
      " rank      : 2 (local_rank: 2)\n",
      " exitcode  : 1 (pid: 339)\n",
      " [3]\n",
      " rank      : 3 (local_rank: 3)\n",
      " exitcode  : 1 (pid: 340)\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " rank      : 0 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 337)\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.10 acc_launcher.py --add_special_tokens False --append_concat_token False --bf16 True --bnb_4bit_compute_dtype bfloat16 --bnb_4bit_quant_storage_dtype bfloat16 --chat_template_format chatml --config_file qlora.yaml --dataset_name smangrul/ultrachat-10k-chatml --dataset_text_field content --evaluation_strategy epoch --gradient_accumulation_steps 2 --gradient_checkpointing True --learning_rate 0.0001 --logging_strategy steps --lora_alpha 16 --lora_dropout 0.1 --lora_r 8 --lora_target_modules all-linear --lr_scheduler_type cosine --max_grad_norm 1.0 --max_seq_len 2048 --model_name_or_path NousResearch/Llama-2-7b-hf --num_train_epochs 1 --output_dir llama-sft-qlora-dsz3 --packing True --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --report_to none --seed 100 --splits train,test --training_script train_qlora.py --use_4bit_quantization False --use_flash_attn False --use_nested_quant True --use_peft_lora False --use_reentrant True --warmup_ratio 0.0 --weight_decay 0.0001\"\u001b[0m\n",
      "\u001b[34m2024-04-10 21:16:45,574 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-04-10 21:17:31 Uploading - Uploading generated training model\n",
      "2024-04-10 21:17:31 Failed - Instances not retained as a result of warmpool resource limits being exceeded\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2024-04-10-20-57-52-597: Failed. Reason: AlgorithmError: modes:\nExitCode 1\nErrorMessage \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n warnings.warn(\n /opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n /opt/conda/lib/python3.10/site-packages/transformers/generation/conf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1341\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2680\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5766\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5747\u001b[0m \n\u001b[1;32m   5748\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5764\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5766\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:7995\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7992\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   7994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 7995\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7996\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   7997\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:8048\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8044\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8045\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8046\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8047\u001b[0m     )\n\u001b[0;32m-> 8048\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8049\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8050\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8051\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8052\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2024-04-10-20-57-52-597: Failed. Reason: AlgorithmError: modes:\nExitCode 1\nErrorMessage \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n warnings.warn(\n /opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n /opt/conda/lib/python3.10/site-packages/transformers/generation/conf"
     ]
    }
   ],
   "source": [
    "estimator1.fit(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "cur_dir = os.getcwd().replace(os.environ[\"HOME\"],\"\")\n",
    "HTML(f'''1. Paste the following command into the Studio Terminal <code style=\"background-color:gray;\">tensorboard --logdir {tensorboard_output_config.s3_output_path}</code><br>\n",
    "2. Click <a href='/jupyter/default/proxy/6006/'>here</a> to open TensorBoard''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to wait for the job to finish before we can deploy the model \n",
    "estimator.latest_training_job.wait(logs=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job has completed, we can deploy the model to a SageMaker Endpoint.\n",
    "We will use a [Deep learning container for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) for deployment which is optimized for serving large models in excess of 100B parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need a few additional imports for model deployment\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the docker image that will be used for inference\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a model deploymnent packages which will be used to deploy our model to a SageMaker Endpoint. The model deployment package is a tarball that contains the model artifacts, [inference code](src/inference/model.py), and any [additional dependencies](src/inference/requirements.txt) required to run the inference code. We'll go through the following steps to create the model deployment package:\n",
    "1. Download the trained model artifact from S3 to the local filesystem\n",
    "2. Cretae a `serving.properties` file that will configure our hosting environment\n",
    "3. Combine the trained model, the inference code, and the `serving.properties` file into a tarball with the following structure:\n",
    "```\n",
    "|-- model.py         # inference code\n",
    "|-- requirements.txt    # additional dependencies\n",
    "|-- serving.properties  # configuration file\n",
    "|-- <model_id>\\         # model artifacts\n",
    "    |-- config.json\n",
    "    |-- pytorch_model.bin\n",
    "    |-- special_tokens_map.json\n",
    "    |-- tokenizer_config.json\n",
    "    |-- tokenizer.json\n",
    "    |-- vocab.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model was trained using Low Rank Adaptation (LoRA), and as a result the model artifact is small (~10Mb) allowing us to repackage it along with our inference code. At deployment time, the base model will be downloaded from Hugging Face Hub and the LoRA weights will be applied to the base model. For deployment of larger models with LoRA weights, it is recommended to store the based model weights in your own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model artifacts into the inference code directory \n",
    "with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "    contents = tar.getnames()\n",
    "    model_id = os.path.dirname(contents[-1]) # model id is the name of the folder containing the model files as generated by the training job\n",
    "    tar.extractall(\"src/inference/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the serving.properties file\n",
    "# We'll use the python engine for inference and specify the model_id for the base model we want to use\n",
    "with open(\"src/inference/serving.properties\", \"w\") as f:\n",
    "    f.write(\n",
    "f\"\"\"engine=Python\n",
    "option.model_id={model_id}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything needed to create the model package. We'll combine the contents of the `src/inference` directory with the model artifact and create a tarball. We'll then upload the tarball to S3 and use the S3 URI to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd src/\n",
    "tar czvf model.tar.gz inference/\n",
    "mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, f\"{s3_key_prefix}/model\")\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {hf_s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(image_uri, model_data, role, endpoint_name, instance_type, sagemaker_session):\n",
    "    \"\"\"Helper function to create the SageMaker Endpoint resources and return a predictor\"\"\"\n",
    "    model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "    model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name)\n",
    "\n",
    "    # our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=serializers.JSONSerializer(),        # will convert python dict to json\n",
    "        deserializer=deserializers.JSONDeserializer(),  # will convert json to python dict\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique endpoint name\n",
    "hf_endpoint_name = sagemaker.utils.name_from_base(\"t5-summarization\")\n",
    "print(f\"Our endpoint will be called {hf_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment will take 5 to 10 minutes\n",
    "hf_predictor = deploy_model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=hf_s3_code_artifact,\n",
    "    role=role,\n",
    "    endpoint_name=hf_endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the endpoint deployed, we can generate summaries on dialogues from the test dataset. We'll randomly select an examples and generate summaries. You can also provide your own dialogue to generate summaries just be sure to use the same format as the examples in the train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dialogue_idx = randint(0, test_data.shape[0])\n",
    "random_dialogue = test_data[\"dialogue\"][random_dialogue_idx]\n",
    "\n",
    "output = hf_predictor.predict({\"inputs\": [random_dialogue], \"parameters\":{\"max_length\": 100}})\n",
    "output_summary = output[\"outputs\"][0][\"summary_text\"]\n",
    "\n",
    "print(\"#####DIALOGUE######\\n\", random_dialogue)\n",
    "print(\"\\n#####GENERATED SUMMARY######\\n\", output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint when finished experimenting\n",
    "hf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "In this section we'll import the requisite libraries and instantiate a number of objects and variables to configure our training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                        # SageMaker Python SDK\n",
    "from sagemaker.pytorch import PyTorch                   # PyTorch Estimator for running pytorch training jobs\n",
    "from sagemaker.debugger import TensorBoardOutputConfig  # Debugger TensorBoard config to log training metrics to TensorBoard\n",
    "import boto3                                            # AWS SDK for Python\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()   # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_key_prefix = \"flan-t5-finetune-for-dialogue\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case and Data Exploration\n",
    "For this lab we'll utilize the [DialogSum Dataset](https://github.com/cylnlp/dialogsum) which is comprised of over 13K dialogues along with human provided summaries. Our goal is to finetune a model that given a dialogue will automatically generate a summary that captures all of the salient points of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-03 20:13:30--  https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12068822 (12M) [text/plain]\n",
      "Saving to: ‘data/dialogsum.train.jsonl’\n",
      "\n",
      "data/dialogsum.trai 100%[===================>]  11.51M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-04-03 20:13:30 (101 MB/s) - ‘data/dialogsum.train.jsonl’ saved [12068822/12068822]\n",
      "\n",
      "--2024-04-03 20:13:31--  https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.test.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 639707 (625K) [text/plain]\n",
      "Saving to: ‘data/dialogsum.test.jsonl’\n",
      "\n",
      "data/dialogsum.test 100%[===================>] 624.71K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2024-04-03 20:13:31 (109 MB/s) - ‘data/dialogsum.test.jsonl’ saved [639707/639707]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.train.jsonl -O data/dialogsum.train.jsonl\n",
    "!wget https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.test.jsonl -O data/dialogsum.test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (12460, 4)\n",
      "Test data shape:  (500, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load the train and test data into a pandas dataframe\n",
    "train_data = pd.read_json(\"data/dialogsum.train.jsonl\", lines=True)\n",
    "test_data = pd.read_json(\"data/dialogsum.test.jsonl\", lines=True)\n",
    "print (\"Train data shape: \", train_data.shape)\n",
    "print (\"Test data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
       "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
       "      <td>get a check-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
       "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
       "      <td>vaccines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
       "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
       "      <td>find keys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
       "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
       "      <td>have a girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
       "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fname                                           dialogue  \\\n",
       "0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
       "1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
       "2  train_2  #Person1#: Excuse me, did you see a set of key...   \n",
       "3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n",
       "4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
       "\n",
       "                                             summary              topic  \n",
       "0  Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n",
       "1  Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n",
       "2  #Person1#'s looking for a set of keys and asks...          find keys  \n",
       "3  #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n",
       "4  Malik invites Nikki to dance. Nikki agrees if ...              dance  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####DIALOGUE###### \n",
      " #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "\n",
      "#####SUMMARY###### \n",
      " Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "print(\"#####DIALOGUE###### \\n\", train_data[\"dialogue\"][0])\n",
    "print(\"\\n#####SUMMARY###### \\n\", train_data[\"summary\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data to S3\n",
    "# We will only use the training data which we will split into train and validation sets inside the training script\n",
    "# We will use the test data to evaluate the model after we deploy it\n",
    "s3_data_path = sess.upload_data(\"data/dialogsum.train.jsonl\", bucket=bucket, key_prefix=f\"{s3_key_prefix}/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accelerate launch` command has two key parts, the `config.yml` file and the `train.py` script. The `config.yml` file is used to configure the distributed training job. The `train.py` script is the training script that will be launched by the launcher. In this example, we'll use the [ds_zero3.yml](src/train/ds_zero3.yaml) configuration file. The config file enables [DeepSpeed ZeRo Stage3](#https://www.deepspeed.ai/tutorials/zero/) and a number of other optimizations to enable training of large scale models. This file was generated by running `accelerate config --config_file ds_zero3.yml` and then following the on-screen prompts. \n",
    "The [train.py](src/train/train.py) makes use of a number of key libraries to enable training of large models with minimal code changes:\n",
    "- 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) - Configures the distributed training environment and adapts training objects (data loaders, models, optimizers) to the distributed environment\n",
    "- 🤗 [Transformers](https://huggingface.co/docs/transformers/index) - Provides a number of pre-trained models and utilities for training and evaluating models\n",
    "- 🤗 [PEFT](https://github.com/huggingface/peft) - Provides a number of methods for Parameter Efficient Finetuning(PEFT) of large language models. The [LoRA](https://arxiv.org/pdf/2106.09685.pdf) method will be used to finetune the model\n",
    "- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Provides a number of optimizations to enable training of large models. In this example, we'll use DeepSpeed ZeRO Stage3 to enable training of models with over 1B parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# configure the tesnorboard output directly to S3\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://{bucket}/{s3_key_prefix}/tensorboard\"\n",
    ")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir = \"src/train\",\n",
    "    entry_point=\"acc_launcher.py\",\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    framework_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    disable_profiler=True,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    hyperparameters={\"training_script\": \"train.py\",\n",
    "                     \"config_file\": \"ds_zero3.yaml\",\n",
    "                     \"lr\": 3e-3,\n",
    "                     \"batch_size\": 2,\n",
    "                     \"subsample\": 50, # percent of data to use\n",
    "                     \"num_epochs\": 2,\n",
    "                     \"pretrained_model_name_or_path\": \"google/flan-t5-xl\"\n",
    "                     \n",
    "    },\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-04-03-18-32-31-963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2024-04-03 18:32:32 Starting - Starting the training job...\n",
      "2024-04-03 18:32:36 Pending - Training job waiting for capacity...\n",
      "2024-04-03 18:33:10 Pending - Preparing the instances for training........................\n",
      "2024-04-03 18:37:26 Downloading - Downloading input data......\n",
      "2024-04-03 18:38:06 Downloading - Downloading the training image............\n",
      "2024-04-03 18:40:22 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:15,832 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:15,929 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:15,939 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:15,941 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:17,074 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.33.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers==4.33.2 from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.9/119.9 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.22.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for accelerate==0.22.0 from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.9.3 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.9.3.tar.gz (807 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 807.5/807.5 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.10.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets==2.10.1 from https://files.pythonhosted.org/packages/fe/17/5825fdf034ff1a315becdbb9b6fe5a2bd9d8e724464535f18809593bf9c2/datasets-2.10.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.10.1-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/b4/5d/758c00ba637bc850f35fff7fad442c470ac3d606fe586d881b0bed7ef5a5/peft-0.10.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.33.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/05/9e/80c20f1151432a6025690c9c2037053039b028a7b236fa81d7e7ac9dec60/regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tokenizers!=0.11.3,<0.14,>=0.11.1 from https://files.pythonhosted.org/packages/d6/27/07a337087dd507170a1b20fed3bbf8da81401185a7130a6e74e440c52040/tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers==4.33.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/aa/9a/723fc6eed972b28bbb24241b246005093b3c27340bc8f7b7606d75a92834/safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.33.2->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.22.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.22.0->-r requirements.txt (line 2)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 3)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 4)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for dill<0.3.7,>=0.3.0 from https://files.pythonhosted.org/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 4)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 4)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 4)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/bf/63/bff4168e4be6da032225fe3f2492b4627bf9416a62e58b7e9dc98c6280b2/aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for responses<0.19 from https://files.pythonhosted.org/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 5)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py (from rouge-score->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/50/54/827c21cbe1187f2e39c041eeeb545f3bf327b19fadb5b97b8395f3883b25/grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/fc/b3/0c0c994fe49cd661084f8d5dc06562af53818cc0abefaca35bdc894577c3/Markdown-3.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 9)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 9)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 9)) (2.3.6)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/39/a9/1f8d42c8103bcb1da6bb719f1bc018594b5acc8eae56b3fec4720ebee225/multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/69/ea/d7e961ea9b1b818a43b155ee512117be6ab9ab67c1e94967b2e64126e8e4/yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 9)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.33.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.33.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.33.2->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.33.2->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 9)) (2.1.3)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting multiprocess (from datasets==2.10.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/da/d9/f7f9379981e39b8c2511c9e0326d212accacb82f12fbfdc1aa2ce2a7b2b6/multiprocess-0.70.16-py39-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/6a/f4/fbeb03ef7abdda54db4a6a75c971b88ab73d724ff09e3275cc1e99f1c946/multiprocess-0.70.14-py39-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.14-py39-none-any.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 4)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 4)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 9)) (3.16.2)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 49.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.2/251.2 kB 36.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 59.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 73.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 19.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 32.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 59.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.6-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 77.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 50.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.4/773.4 kB 61.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 80.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 74.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.9/132.9 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 39.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.8/123.8 kB 26.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.3/304.3 kB 43.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.3-py3-none-any.whl size=843297 sha256=4181b6bda7ca6fb481b94cef9b0c98b02d78a6983584def019e07d9ed3f207d8\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/d3/d9/43/7997c899a97c61ce042413cc7b58d2ac9a71e92590778a6cf4\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3abf8925e1e61a50475b17a02c4709cacc77c94e6753e9b3c394a81de593a8cd\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, tensorboard-data-server, safetensors, regex, multidict, grpcio, frozenlist, dill, async-timeout, absl-py, yarl, responses, nltk, multiprocess, markdown, huggingface-hub, deepspeed, aiosignal, accelerate, transformers, tensorboard, rouge-score, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.7\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.7:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.7\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.15\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.15:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.15\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+4c3ff1a:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.21.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.21.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.21.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 accelerate-0.22.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.10.1 deepspeed-0.9.3 dill-0.3.6 evaluate-0.4.1 frozenlist-1.4.1 grpcio-1.62.1 huggingface-hub-0.22.2 markdown-3.6 multidict-6.0.5 multiprocess-0.70.14 nltk-3.8.1 peft-0.10.0 regex-2023.12.25 responses-0.18.0 rouge-score-0.1.2 safetensors-0.4.2 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,419 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,420 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,552 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,666 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,780 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,793 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 2,\n",
      "        \"config_file\": \"ds_zero3.yaml\",\n",
      "        \"lr\": 0.003,\n",
      "        \"num_epochs\": 2,\n",
      "        \"pretrained_model_name_or_path\": \"google/flan-t5-xl\",\n",
      "        \"subsample\": 50,\n",
      "        \"training_script\": \"train.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-03-18-32-31-963\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-03-18-32-31-963/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"acc_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"acc_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":2,\"config_file\":\"ds_zero3.yaml\",\"lr\":0.003,\"num_epochs\":2,\"pretrained_model_name_or_path\":\"google/flan-t5-xl\",\"subsample\":50,\"training_script\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=acc_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=acc_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-03-18-32-31-963/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":2,\"config_file\":\"ds_zero3.yaml\",\"lr\":0.003,\"num_epochs\":2,\"pretrained_model_name_or_path\":\"google/flan-t5-xl\",\"subsample\":50,\"training_script\":\"train.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-03-18-32-31-963\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/pytorch-training-2024-04-03-18-32-31-963/source/sourcedir.tar.gz\",\"module_name\":\"acc_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"acc_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"2\",\"--config_file\",\"ds_zero3.yaml\",\"--lr\",\"0.003\",\"--num_epochs\",\"2\",\"--pretrained_model_name_or_path\",\"google/flan-t5-xl\",\"--subsample\",\"50\",\"--training_script\",\"train.py\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=ds_zero3.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.003\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_PRETRAINED_MODEL_NAME_OR_PATH=google/flan-t5-xl\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=50\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=train.py\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 acc_launcher.py --batch_size 2 --config_file ds_zero3.yaml --lr 0.003 --num_epochs 2 --pretrained_model_name_or_path google/flan-t5-xl --subsample 50 --training_script train.py\u001b[0m\n",
      "\u001b[34m2024-04-03 18:41:44,820 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcommand = accelerate launch --config_file ds_zero3.yaml train.py --batch_size 2 --lr 0.003 --num_epochs 2 --pretrained_model_name_or_path google/flan-t5-xl --subsample 50\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:55,918] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:55,918] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,182] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,182] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,204] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,204] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,250] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,250] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,261] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,261] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,280] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,281] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,281] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,332] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:41:56,332] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date![nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 8019.70it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1402.31it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34m[nltk_data]   Package punkt is already up-to-date!\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/root/.cache/huggingface/datasets/json/default-2befc5695ba2c761/0.0.0)\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:01, 4059.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:01, 3236.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:01, 3855.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:01, 2866.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:02, 2435.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:02, 2119.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3874.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3628.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3182.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3140.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3319.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 3009.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:03, 1425.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:00<00:00, 3746.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 1000/6230 [00:00<00:04, 1188.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:00<00:00, 3343.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:00<00:00, 3329.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:00<00:00, 3259.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:00<00:01, 2955.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:01, 2186.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:01<00:01, 2942.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:01<00:02, 2058.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 3469.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 3378.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 3375.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 3268.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 3134.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3730.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:01<00:01, 2436.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3842.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 2970.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 3000/6230 [00:01<00:01, 2475.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3514.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3333.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3298.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3403.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 2895.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3800.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3893.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:01<00:00, 2776.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3469.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3500.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3811.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:01<00:00, 3374.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3214.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:01<00:00, 3396.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:02<00:00, 3819.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:02<00:00, 3989.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 15372.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 7084.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 6316.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/6230 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 7843.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 6973.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 6917.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 5000/6230 [00:00<00:00, 12148.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 9676.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 10085.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 9066.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 10591.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 11111.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 8997.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 5807.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 2000/6230 [00:00<00:00, 6776.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 12637.22 examples/s]#015Map:  96%|█████████▋| 6000/6230 [00:00<00:00, 11361.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 11872.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 10346.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 9570.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 4000/6230 [00:00<00:00, 8616.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 9196.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 6000/6230 [00:00<00:00, 9242.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/5607 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1311.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1355.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1444.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1394.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:02, 1562.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1310.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1458.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 1000/5607 [00:00<00:03, 1423.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1392.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1374.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1440.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1377.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1502.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1385.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1525.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 2000/5607 [00:01<00:02, 1533.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1381.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1444.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1399.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1415.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:01<00:01, 1585.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1330.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:01, 1494.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 3000/5607 [00:02<00:02, 1225.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1386.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1479.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1446.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1392.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1520.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1369.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1356.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 4000/5607 [00:02<00:01, 1448.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1436.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1447.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1460.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1525.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1436.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1514.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1430.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 5000/5607 [00:03<00:00, 1411.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1475.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1496.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1438.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1480.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1416.41 examples/s]#015Running tokenizer on dataset: 100%|██████████| 5607/5607 [00:04<00:00, 1414.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:04<00:00, 1410.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 5607/5607 [00:03<00:00, 1463.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/623 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1713.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1667.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1878.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1837.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1701.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1733.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1662.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 623/623 [00:00<00:00, 1492.72 examples/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:33<00:33, 33.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 16.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:38<00:00, 19.46s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:48,523] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 2.85B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,723] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,725] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,725] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,771] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,771] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,771] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,939] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,940] [INFO] [utils.py:786:see_memory_usage] MA 1.36 GB         Max_MA 1.85 GB         CA 3.64 GB         Max_CA 4 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,940] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.25 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,945] [INFO] [stage3.py:113:__init__] Reduce bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:42:51,945] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.037949562072754 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.819292306900024 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.019589900970459 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.019359827041626 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.019540786743164 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.020015478134155 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.019512176513672 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.020024299621582 seconds\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,490] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,491] [INFO] [utils.py:786:see_memory_usage] MA 1.36 GB         Max_MA 1.36 GB         CA 3.64 GB         Max_CA 4 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,491] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.3 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 251904 in 124 params\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,662] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,663] [INFO] [utils.py:786:see_memory_usage] MA 1.36 GB         Max_MA 1.36 GB         CA 3.64 GB         Max_CA 4 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,663] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.31 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,792] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,792] [INFO] [utils.py:786:see_memory_usage] MA 1.36 GB         Max_MA 1.36 GB         CA 3.64 GB         Max_CA 4 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:07,793] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.31 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,576] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,577] [INFO] [utils.py:786:see_memory_usage] MA 1.33 GB         Max_MA 1.36 GB         CA 1.33 GB         Max_CA 4 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,578] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 47.46 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,739] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,740] [INFO] [utils.py:786:see_memory_usage] MA 1.33 GB         Max_MA 1.33 GB         CA 1.33 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,740] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.3 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,876] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,876] [INFO] [utils.py:786:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.66 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:09,877] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.3 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,019] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,020] [INFO] [utils.py:786:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.66 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,020] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.3 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,168] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,168] [INFO] [utils.py:786:see_memory_usage] MA 5.31 GB         Max_MA 9.29 GB         CA 9.3 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,169] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.3 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,169] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003864765167236328 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00044846534729003906 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00039577484130859375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003962516784667969 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]#015  0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00039577484130859375 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004584789276123047 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00041961669921875 seconds\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,602] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [utils.py:786:see_memory_usage] MA 8.5 GB         Max_MA 8.99 GB         CA 13.15 GB         Max_CA 13 GB\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 42.26 GB, percent = 3.8%\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,605] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,605] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,605] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,605] [INFO] [config.py:964:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,605] [INFO] [config.py:964:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabcc09db80>\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 4\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,606] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10,607] [INFO] [config.py:950:print_user_config]   json = {\n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00034809112548828125 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.627 algo-1:398 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.627 algo-1:402 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.627 algo-1:403 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.628 algo-1:400 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.628 algo-1:405 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.628 algo-1:401 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.628 algo-1:399 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.628 algo-1:404 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:403 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:402 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:400 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:399 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:398 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:403 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:402 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.677 algo-1:405 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:400 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:399 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:398 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:401 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:403 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:403 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:402 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:402 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:405 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:400 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:400 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:404 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:399 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.678 algo-1:398 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:401 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:399 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:398 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:405 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:405 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:401 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:401 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.679 algo-1:404 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.680 algo-1:404 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:10.680 algo-1:404 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:05<30:43,  5.27s/it]#015  0%|          | 1/351 [00:05<30:41,  5.26s/it]#015  0%|          | 1/351 [00:05<30:45,  5.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:05<30:22,  5.21s/it]#015  0%|          | 1/351 [00:05<30:44,  5.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:05<30:45,  5.27s/it]#015  0%|          | 1/351 [00:05<30:44,  5.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:05<29:45,  5.10s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:09<25:37,  4.40s/it]#015  1%|          | 2/351 [00:09<25:38,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:08<25:13,  4.34s/it]#015  1%|          | 2/351 [00:09<25:37,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:09<25:29,  4.38s/it]#015  1%|          | 2/351 [00:09<25:38,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:09<25:38,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:09<25:38,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:10<17:10,  2.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:10<17:23,  3.00s/it]#015  1%|          | 3/351 [00:10<17:23,  3.00s/it]#015  1%|          | 3/351 [00:10<17:23,  3.00s/it]#015  1%|          | 3/351 [00:10<17:24,  3.00s/it]#015  1%|          | 3/351 [00:10<17:24,  3.00s/it]#015  1%|          | 3/351 [00:10<17:19,  2.99s/it]#015  1%|          | 3/351 [00:10<17:24,  3.00s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:22,986] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:12<15:23,  2.66s/it]#015  1%|          | 4/351 [00:12<15:23,  2.66s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:12<15:23,  2.66s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:12<15:23,  2.66s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:12<15:20,  2.65s/it]#015  1%|          | 4/351 [00:12<15:23,  2.66s/it]#015  1%|          | 4/351 [00:12<15:23,  2.66s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:12<15:16,  2.64s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:14<13:39,  2.37s/it]#015  1%|▏         | 5/351 [00:14<13:39,  2.37s/it]#015  1%|▏         | 5/351 [00:14<13:34,  2.35s/it]#015  1%|▏         | 5/351 [00:14<13:39,  2.37s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:14<13:39,  2.37s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:14<13:39,  2.37s/it]#015  1%|▏         | 5/351 [00:14<13:39,  2.37s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:14<13:37,  2.36s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:16<13:18,  2.31s/it]#015  2%|▏         | 6/351 [00:16<13:21,  2.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:16<13:21,  2.32s/it]#015  2%|▏         | 6/351 [00:16<13:21,  2.32s/it]#015  2%|▏         | 6/351 [00:16<13:21,  2.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:16<13:21,  2.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:16<13:21,  2.32s/it]#015  2%|▏         | 6/351 [00:16<13:20,  2.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:19<14:43,  2.57s/it]#015  2%|▏         | 7/351 [00:19<14:46,  2.58s/it]#015  2%|▏         | 7/351 [00:19<14:46,  2.58s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:19<14:46,  2.58s/it]#015  2%|▏         | 7/351 [00:19<14:46,  2.58s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:19<14:46,  2.58s/it]#015  2%|▏         | 7/351 [00:19<14:45,  2.57s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:19<14:46,  2.58s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:33,711] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:28,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:28,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.89s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:23<16:29,  2.89s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:24<14:16,  2.50s/it]#015  3%|▎         | 9/351 [00:24<14:16,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:24<14:15,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:24<14:16,  2.50s/it]#015  3%|▎         | 9/351 [00:24<14:16,  2.50s/it]#015  3%|▎         | 9/351 [00:24<14:16,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:24<14:16,  2.50s/it]#015  3%|▎         | 9/351 [00:24<14:16,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:26<12:12,  2.15s/it]#015  3%|▎         | 10/351 [00:26<12:12,  2.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:26<12:12,  2.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:26<12:11,  2.15s/it]#015  3%|▎         | 10/351 [00:26<12:12,  2.15s/it]#015  3%|▎         | 10/351 [00:26<12:12,  2.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:26<12:12,  2.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:26<12:12,  2.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]#015  3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]#015  3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<11:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:41,265] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:30<12:46,  2.26s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:30<12:46,  2.26s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:30<12:46,  2.26s/it]#015  3%|▎         | 12/351 [00:30<12:46,  2.26s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:30<12:46,  2.26s/it]#015  3%|▎         | 12/351 [00:30<12:46,  2.26s/it]#015  3%|▎         | 12/351 [00:30<12:46,  2.26s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:30<12:46,  2.26s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<11:27,  2.03s/it]#015  4%|▎         | 13/351 [00:32<11:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<11:26,  2.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<11:27,  2.03s/it]#015  4%|▎         | 13/351 [00:32<11:27,  2.03s/it]#015  4%|▎         | 13/351 [00:32<11:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<11:27,  2.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<11:26,  2.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:33<10:16,  1.83s/it]#015  4%|▍         | 14/351 [00:33<10:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:33<10:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:33<10:16,  1.83s/it]#015  4%|▍         | 14/351 [00:33<10:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:33<10:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:33<10:16,  1.83s/it]#015  4%|▍         | 14/351 [00:33<10:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:34<09:28,  1.69s/it]#015  4%|▍         | 15/351 [00:35<09:28,  1.69s/it]#015  4%|▍         | 15/351 [00:35<09:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:35<09:28,  1.69s/it]#015  4%|▍         | 15/351 [00:35<09:28,  1.69s/it]#015  4%|▍         | 15/351 [00:35<09:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:35<09:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:35<09:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:47,800] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:37<10:27,  1.87s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:38<09:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:38<09:45,  1.75s/it]#015  5%|▍         | 17/351 [00:38<09:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:38<09:45,  1.75s/it]#015  5%|▍         | 17/351 [00:38<09:45,  1.75s/it]#015  5%|▍         | 17/351 [00:38<09:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:38<09:45,  1.75s/it]#015  5%|▍         | 17/351 [00:38<09:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:40<09:21,  1.69s/it]#015  5%|▌         | 18/351 [00:40<09:21,  1.69s/it]#015  5%|▌         | 18/351 [00:40<09:21,  1.69s/it]#015  5%|▌         | 18/351 [00:40<09:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:40<09:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:40<09:21,  1.69s/it]#015  5%|▌         | 18/351 [00:40<09:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:40<09:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:41<08:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:41<08:45,  1.58s/it]#015  5%|▌         | 19/351 [00:41<08:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:41<08:45,  1.58s/it]#015  5%|▌         | 19/351 [00:41<08:45,  1.58s/it]#015  5%|▌         | 19/351 [00:41<08:45,  1.58s/it]#015  5%|▌         | 19/351 [00:41<08:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:41<08:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:43:54,282] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:43<09:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:45<09:02,  1.64s/it]#015  6%|▌         | 21/351 [00:45<09:02,  1.64s/it]#015  6%|▌         | 21/351 [00:45<09:02,  1.64s/it]#015  6%|▌         | 21/351 [00:45<09:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:45<09:02,  1.64s/it]#015  6%|▌         | 21/351 [00:45<09:02,  1.64s/it]#015  6%|▌         | 21/351 [00:45<09:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:45<09:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/351 [00:46<08:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/351 [00:46<08:33,  1.56s/it]#015  6%|▋         | 22/351 [00:46<08:33,  1.56s/it]#015  6%|▋         | 22/351 [00:46<08:33,  1.56s/it]#015  6%|▋         | 22/351 [00:46<08:33,  1.56s/it]#015  6%|▋         | 22/351 [00:46<08:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/351 [00:46<08:33,  1.56s/it]#015  6%|▋         | 22/351 [00:46<08:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:47<08:12,  1.50s/it]#015  7%|▋         | 23/351 [00:47<08:12,  1.50s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:47<08:12,  1.50s/it]#015  7%|▋         | 23/351 [00:47<08:12,  1.50s/it]#015  7%|▋         | 23/351 [00:47<08:12,  1.50s/it]#015  7%|▋         | 23/351 [00:47<08:12,  1.50s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:47<08:12,  1.50s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:47<08:12,  1.50s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:49<08:24,  1.54s/it]#015  7%|▋         | 24/351 [00:49<08:24,  1.54s/it]#015  7%|▋         | 24/351 [00:49<08:24,  1.54s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:49<08:24,  1.54s/it]#015  7%|▋         | 24/351 [00:49<08:24,  1.54s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:49<08:24,  1.54s/it]#015  7%|▋         | 24/351 [00:49<08:24,  1.54s/it]#015  7%|▋         | 24/351 [00:49<08:24,  1.54s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:51<09:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:51<09:00,  1.66s/it]#015  7%|▋         | 25/351 [00:51<09:00,  1.66s/it]#015  7%|▋         | 25/351 [00:51<09:00,  1.66s/it]#015  7%|▋         | 25/351 [00:51<09:00,  1.66s/it]#015  7%|▋         | 25/351 [00:51<09:00,  1.66s/it]#015  7%|▋         | 25/351 [00:51<09:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:51<09:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [00:53<09:18,  1.72s/it]#015  7%|▋         | 26/351 [00:53<09:18,  1.72s/it]#015  7%|▋         | 26/351 [00:53<09:18,  1.72s/it]#015  7%|▋         | 26/351 [00:53<09:18,  1.72s/it]#015  7%|▋         | 26/351 [00:53<09:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [00:53<09:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [00:53<09:18,  1.72s/it]#015  7%|▋         | 26/351 [00:53<09:18,  1.72s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]#015  8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]#015  8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [00:54<08:49,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:44:07,168] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [00:56<09:12,  1.71s/it]#015  8%|▊         | 28/351 [00:56<09:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [00:56<09:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [00:56<09:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [00:56<09:12,  1.71s/it]#015  8%|▊         | 28/351 [00:56<09:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [00:56<09:12,  1.71s/it]#015  8%|▊         | 28/351 [00:56<09:12,  1.71s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:57<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]#015  8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [00:58<08:38,  1.61s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [00:59<08:10,  1.53s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [00:59<08:10,  1.53s/it]#015  9%|▊         | 30/351 [00:59<08:10,  1.53s/it]#015  9%|▊         | 30/351 [00:59<08:10,  1.53s/it]#015  9%|▊         | 30/351 [00:59<08:10,  1.53s/it]#015  9%|▊         | 30/351 [00:59<08:10,  1.53s/it]#015  9%|▊         | 30/351 [00:59<08:10,  1.53s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [00:59<08:10,  1.53s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:01<08:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:01<08:13,  1.54s/it]#015  9%|▉         | 31/351 [01:01<08:13,  1.54s/it]#015  9%|▉         | 31/351 [01:01<08:13,  1.54s/it]#015  9%|▉         | 31/351 [01:01<08:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:00<08:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:01<08:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:00<08:13,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:02<07:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:02<07:59,  1.50s/it]#015  9%|▉         | 32/351 [01:02<07:59,  1.50s/it]#015  9%|▉         | 32/351 [01:02<07:59,  1.50s/it]#015  9%|▉         | 32/351 [01:02<07:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:02<07:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:02<07:59,  1.50s/it]#015  9%|▉         | 32/351 [01:02<07:59,  1.50s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:03<07:42,  1.45s/it]#015  9%|▉         | 33/351 [01:03<07:42,  1.45s/it]#015  9%|▉         | 33/351 [01:03<07:41,  1.45s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:03<07:42,  1.45s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:03<07:42,  1.45s/it]#015  9%|▉         | 33/351 [01:03<07:42,  1.45s/it]#015  9%|▉         | 33/351 [01:03<07:42,  1.45s/it]#015  9%|▉         | 33/351 [01:03<07:42,  1.45s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:05<07:29,  1.42s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:05<07:29,  1.42s/it]#015 10%|▉         | 34/351 [01:05<07:29,  1.42s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:05<07:29,  1.42s/it]#015 10%|▉         | 34/351 [01:04<07:29,  1.42s/it]#015 10%|▉         | 34/351 [01:05<07:29,  1.42s/it]#015 10%|▉         | 34/351 [01:05<07:29,  1.42s/it]#015 10%|▉         | 34/351 [01:05<07:29,  1.42s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:06<07:20,  1.39s/it]#015 10%|▉         | 35/351 [01:06<07:20,  1.39s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:06<07:20,  1.39s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:06<07:20,  1.39s/it]#015 10%|▉         | 35/351 [01:06<07:20,  1.39s/it]#015 10%|▉         | 35/351 [01:06<07:20,  1.39s/it]#015 10%|▉         | 35/351 [01:06<07:20,  1.39s/it]#015 10%|▉         | 35/351 [01:06<07:20,  1.39s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:44:18,704] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:08<07:59,  1.52s/it]#015 10%|█         | 36/351 [01:08<07:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:08<07:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:08<07:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:08<07:59,  1.52s/it]#015 10%|█         | 36/351 [01:08<07:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:08<07:59,  1.52s/it]#015 10%|█         | 36/351 [01:08<07:59,  1.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]#015 11%|█         | 37/351 [01:09<07:52,  1.51s/it]#015 11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:09<07:52,  1.51s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:10<07:37,  1.46s/it]#015 11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:11<07:37,  1.46s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]#015 11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]#015 11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:12<07:24,  1.42s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]#015 11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]#015 11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]#015 11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:13<07:31,  1.45s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]#015 12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:15<07:22,  1.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]#015 12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]#015 12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]#015 12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]#015 12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:16<07:32,  1.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]#015 12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]#015 12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]#015 12%|█▏        | 43/351 [01:18<07:18,  1.42s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]#015 13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]#015 13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]#015 13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]#015 13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]#015 13%|█▎        | 44/351 [01:20<08:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]#015 13%|█▎        | 45/351 [01:21<08:29,  1.67s/it]#015 13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:21<08:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]#015 13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]#015 13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]#015 13%|█▎        | 45/351 [01:22<08:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]#015 13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]#015 13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]#015 13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]#015 13%|█▎        | 46/351 [01:23<08:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]#015 13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]#015 13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]#015 13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]#015 13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:24<07:39,  1.51s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]#015 14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]#015 14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]#015 14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:26<07:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]#015 14%|█▍        | 49/351 [01:27<07:52,  1.56s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]#015 14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]#015 14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]#015 14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]#015 14%|█▍        | 50/351 [01:29<07:45,  1.55s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]#015 15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]#015 15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]#015 15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]#015 15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]#015 15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:30<07:27,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:44:42,787] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]#015 15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]#015 15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:32<07:31,  1.51s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]#015 15%|█▌        | 53/351 [01:34<08:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [01:35<07:43,  1.56s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]#015 15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]#015 15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]#015 15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]#015 15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [01:35<07:44,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]#015 16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]#015 16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [01:37<07:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.86s/it]#015 16%|█▌        | 56/351 [01:39<09:10,  1.86s/it]#015 16%|█▌        | 56/351 [01:39<09:10,  1.86s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [01:39<09:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]#015 16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]#015 16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]#015 16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [01:41<08:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]#015 17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]#015 17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]#015 17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]#015 17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [01:42<07:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]#015 17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]#015 17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]#015 17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]#015 17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]#015 17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [01:43<07:28,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:44:55,951] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]#015 17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]#015 17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [01:45<07:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]#015 17%|█▋        | 61/351 [01:46<07:31,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]#015 17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]#015 17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]#015 17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]#015 17%|█▋        | 61/351 [01:47<07:31,  1.56s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [01:46<07:31,  1.56s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]#015 18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]#015 18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]#015 18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]#015 18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]#015 18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [01:48<07:13,  1.50s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]#015 18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]#015 18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]#015 18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]#015 18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]#015 18%|█▊        | 63/351 [01:50<08:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]#015 18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:45:03,125] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [01:52<08:43,  1.82s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [01:53<08:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]#015 19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]#015 19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]#015 19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [01:54<08:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]#015 19%|█▉        | 66/351 [01:55<07:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [01:56<07:30,  1.59s/it]#015 19%|█▉        | 67/351 [01:56<07:30,  1.59s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]#015 19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]#015 19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]#015 19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]#015 19%|█▉        | 67/351 [01:57<07:30,  1.59s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]#015 19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]#015 19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [01:58<07:32,  1.60s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [01:59<07:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]#015 20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]#015 20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]#015 20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]#015 20%|█▉        | 69/351 [02:00<07:21,  1.57s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]#015 20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]#015 20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]#015 20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]#015 20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]#015 20%|█▉        | 70/351 [02:01<07:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:02<06:45,  1.45s/it]#015 20%|██        | 71/351 [02:02<06:45,  1.45s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:02<06:45,  1.45s/it]#015 20%|██        | 71/351 [02:02<06:45,  1.45s/it]#015 20%|██        | 71/351 [02:02<06:45,  1.45s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:02<06:45,  1.45s/it]#015 20%|██        | 71/351 [02:02<06:45,  1.45s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:02<06:45,  1.45s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:49,  1.47s/it]#015 21%|██        | 72/351 [02:04<06:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:50,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:50,  1.47s/it]#015 21%|██        | 72/351 [02:04<06:50,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:04<06:50,  1.47s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:05<06:41,  1.44s/it]#015 21%|██        | 73/351 [02:05<06:41,  1.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:07<07:11,  1.56s/it]#015 21%|██        | 74/351 [02:07<07:11,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:07<07:11,  1.56s/it]#015 21%|██        | 74/351 [02:07<07:11,  1.56s/it]#015 21%|██        | 74/351 [02:07<07:11,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:07<07:11,  1.56s/it]#015 21%|██        | 74/351 [02:07<07:11,  1.56s/it]#015 21%|██        | 74/351 [02:07<07:11,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:08<07:10,  1.56s/it]#015 21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]#015 21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]#015 21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]#015 21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:09<07:10,  1.56s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:45:22,272] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]#015 22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]#015 22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:11<08:46,  1.91s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]#015 22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]#015 22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]#015 22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]#015 22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]#015 22%|██▏       | 77/351 [02:13<08:39,  1.89s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]#015 22%|██▏       | 78/351 [02:15<09:08,  2.01s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:17<09:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]#015 23%|██▎       | 79/351 [02:17<09:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]#015 23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]#015 23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]#015 23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:18<09:10,  2.03s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:45:31,600] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]#015 23%|██▎       | 80/351 [02:20<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:21<10:39,  2.36s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]#015 23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]#015 23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]#015 23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]#015 23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:23<10:41,  2.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]#015 23%|██▎       | 82/351 [02:25<10:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]#015 24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]#015 24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:28<10:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:45:41,916] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]#015 24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [02:31<11:34,  2.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]#015 24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]#015 24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [02:33<10:23,  2.34s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]#015 25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]#015 25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]#015 25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]#015 25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]#015 25%|██▍       | 86/351 [02:34<09:17,  2.10s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]#015 25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]#015 25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]#015 25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]#015 25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [02:36<09:19,  2.12s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:45:50,828] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.52s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.53s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.53s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [02:40<11:04,  2.53s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]#015 25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]#015 25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]#015 25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]#015 25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]#015 25%|██▌       | 89/351 [02:41<09:41,  2.22s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]#015 26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]#015 26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]#015 26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]#015 26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [02:43<08:31,  1.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]#015 26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]#015 26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [02:44<07:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]#015 26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:00,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:01,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [02:46<08:01,  1.86s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]#015 26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]#015 26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]#015 26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [02:48<07:43,  1.80s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]#015 27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]#015 27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]#015 27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [02:49<07:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]#015 27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]#015 27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]#015 27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]#015 27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [02:51<07:39,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:04,687] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [02:54<08:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]#015 28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]#015 28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]#015 28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]#015 28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [02:55<07:44,  1.83s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]#015 28%|██▊       | 98/351 [02:56<07:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]#015 28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]#015 28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [02:56<07:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [02:57<07:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]#015 28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]#015 28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]#015 28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [02:58<06:57,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:10,839] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]#015 28%|██▊       | 100/351 [03:00<07:05,  1.70s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]#015 29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]#015 29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]#015 29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:02<07:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:01<07:07,  1.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]#015 29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]#015 29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]#015 29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]#015 29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]#015 29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:03<06:54,  1.66s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:04<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]#015 29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]#015 29%|██▉       | 103/351 [03:04<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:05<06:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]#015 30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:06<06:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]#015 30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]#015 30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]#015 30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:08<06:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:09<06:33,  1.61s/it]#015 30%|███       | 106/351 [03:09<06:33,  1.61s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:09<06:33,  1.61s/it]#015 30%|███       | 106/351 [03:09<06:33,  1.61s/it]#015 30%|███       | 106/351 [03:09<06:33,  1.61s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:09<06:33,  1.61s/it]#015 30%|███       | 106/351 [03:09<06:33,  1.61s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:09<06:33,  1.61s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]#015 30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:11<06:46,  1.66s/it]#015 30%|███       | 107/351 [03:11<06:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:23,844] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]#015 31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:13<06:43,  1.66s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:14<06:22,  1.58s/it]#015 31%|███       | 109/351 [03:14<06:22,  1.58s/it]#015 31%|███       | 109/351 [03:14<06:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:14<06:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:14<06:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:14<06:22,  1.58s/it]#015 31%|███       | 109/351 [03:14<06:22,  1.58s/it]#015 31%|███       | 109/351 [03:14<06:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]#015 31%|███▏      | 110/351 [03:15<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]#015 31%|███▏      | 110/351 [03:16<06:05,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]#015 32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:17<05:52,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]#015 32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:18<06:02,  1.52s/it]#015 32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]#015 32%|███▏      | 112/351 [03:19<06:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]#015 32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]#015 32%|███▏      | 113/351 [03:20<05:49,  1.47s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]#015 32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]#015 32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]#015 32%|███▏      | 114/351 [03:22<06:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]#015 33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]#015 33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]#015 33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]#015 33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:24<06:54,  1.76s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:37,660] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]#015 33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:27<07:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]#015 33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]#015 33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]#015 33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]#015 33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [03:29<08:08,  2.09s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [03:31<08:03,  2.08s/it]#015 34%|███▎      | 118/351 [03:31<08:03,  2.07s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [03:31<08:03,  2.07s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [03:31<08:03,  2.07s/it]#015 34%|███▎      | 118/351 [03:31<08:03,  2.07s/it]#015 34%|███▎      | 118/351 [03:31<08:03,  2.07s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [03:31<08:03,  2.08s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [03:31<08:03,  2.08s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]#015 34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]#015 34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]#015 34%|███▍      | 119/351 [03:33<07:29,  1.94s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:46,743] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]#015 34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]#015 34%|███▍      | 120/351 [03:36<08:49,  2.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]#015 34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]#015 34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [03:37<07:49,  2.04s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [03:38<06:58,  1.83s/it]#015 35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]#015 35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]#015 35%|███▍      | 122/351 [03:39<06:58,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]#015 35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [03:40<06:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:46:53,036] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]#015 35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [03:42<06:55,  1.83s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]#015 36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]#015 36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [03:43<06:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]#015 36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]#015 36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [03:45<06:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [03:47<06:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [03:46<06:05,  1.63s/it]#015 36%|███▌      | 127/351 [03:47<06:05,  1.63s/it]#015 36%|███▌      | 127/351 [03:47<06:05,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [03:47<06:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [03:47<06:05,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [03:47<06:05,  1.63s/it]#015 36%|███▌      | 127/351 [03:47<06:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]#015 36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [03:48<06:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]#015 37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [03:50<05:55,  1.60s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]#015 37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]#015 37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]#015 37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]#015 37%|███▋      | 130/351 [03:51<05:36,  1.52s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]#015 37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [03:53<05:50,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:05,485] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:54<05:48,  1.59s/it]#015 38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [03:54<05:48,  1.59s/it]#015 38%|███▊      | 132/351 [03:55<05:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]#015 38%|███▊      | 133/351 [03:56<06:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]#015 38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]#015 38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]#015 38%|███▊      | 133/351 [03:57<06:14,  1.72s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]#015 38%|███▊      | 134/351 [03:58<06:06,  1.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:00<06:00,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<05:59,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<05:59,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<05:59,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<05:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:00<06:00,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<06:00,  1.67s/it]#015 38%|███▊      | 135/351 [04:00<05:59,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:13,015] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]#015 39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]#015 39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:02<06:37,  1.85s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]#015 39%|███▉      | 137/351 [04:03<06:18,  1.77s/it]#015 39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]#015 39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]#015 39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]#015 39%|███▉      | 137/351 [04:04<06:18,  1.77s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]#015 39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]#015 39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]#015 39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]#015 39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]#015 39%|███▉      | 138/351 [04:05<05:48,  1.64s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]#015 40%|███▉      | 139/351 [04:06<05:37,  1.59s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]#015 40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]#015 40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]#015 40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]#015 40%|███▉      | 140/351 [04:08<05:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]#015 40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]#015 40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:10<05:45,  1.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:11<05:24,  1.55s/it]#015 40%|████      | 142/351 [04:11<05:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:11<05:24,  1.55s/it]#015 40%|████      | 142/351 [04:11<05:24,  1.55s/it]#015 40%|████      | 142/351 [04:11<05:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:11<05:24,  1.55s/it]#015 40%|████      | 142/351 [04:11<05:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:11<05:24,  1.55s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:12<05:08,  1.49s/it]#015 41%|████      | 143/351 [04:12<05:08,  1.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:12<05:08,  1.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:12<05:08,  1.49s/it]#015 41%|████      | 143/351 [04:12<05:08,  1.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:12<05:08,  1.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:12<05:08,  1.49s/it]#015 41%|████      | 143/351 [04:12<05:08,  1.49s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:15<06:09,  1.79s/it]#015 41%|████      | 144/351 [04:15<06:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:15<06:09,  1.79s/it]#015 41%|████      | 144/351 [04:15<06:09,  1.79s/it]#015 41%|████      | 144/351 [04:15<06:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:15<06:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:15<06:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:25,909] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:15<06:09,  1.79s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:16<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]#015 41%|████▏     | 145/351 [04:17<05:59,  1.74s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]#015 42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:18<05:33,  1.63s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]#015 42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]#015 42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]#015 42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]#015 42%|████▏     | 147/351 [04:19<05:16,  1.55s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:21<05:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]#015 42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]#015 42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]#015 42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:22<05:07,  1.52s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]#015 43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]#015 43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]#015 43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]#015 43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]#015 43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:24<05:12,  1.55s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]#015 43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]#015 43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]#015 43%|████▎     | 151/351 [04:26<05:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:25<05:12,  1.56s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:38,388] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]#015 43%|████▎     | 152/351 [04:27<05:28,  1.65s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]#015 44%|████▎     | 153/351 [04:29<05:10,  1.57s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]#015 44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]#015 44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]#015 44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]#015 44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]#015 44%|████▍     | 154/351 [04:30<04:55,  1.50s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]#015 44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]#015 44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]#015 44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]#015 44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]#015 44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 155/351 [04:31<04:43,  1.45s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:44,384] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]#015 44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [04:33<05:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]#015 45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]#015 45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [04:35<04:55,  1.53s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]#015 45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]#015 45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]#015 45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [04:37<05:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]#015 45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]#015 45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]#015 45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [04:38<05:15,  1.64s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [04:40<05:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]#015 46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [04:41<05:00,  1.58s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]#015 46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]#015 46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]#015 46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]#015 46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [04:43<04:45,  1.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]#015 46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]#015 46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]#015 46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]#015 46%|████▋     | 163/351 [04:45<05:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [04:44<05:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:47:57,764] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [04:47<05:32,  1.78s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]#015 47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]#015 47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [04:48<05:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]#015 47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]#015 47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]#015 47%|████▋     | 166/351 [04:49<04:49,  1.56s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]#015 47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]#015 47%|████▋     | 166/351 [04:49<04:49,  1.56s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [04:50<04:49,  1.56s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [04:51<04:34,  1.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [04:51<04:34,  1.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [04:51<04:35,  1.49s/it]#015 48%|████▊     | 167/351 [04:51<04:34,  1.49s/it]#015 48%|████▊     | 167/351 [04:51<04:34,  1.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [04:51<04:35,  1.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [04:51<04:34,  1.49s/it]#015 48%|████▊     | 167/351 [04:51<04:35,  1.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]#015 48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]#015 48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [04:52<04:34,  1.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]#015 48%|████▊     | 169/351 [04:54<05:09,  1.70s/it]#015 48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [04:54<05:09,  1.70s/it]#015 48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]#015 48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]#015 48%|████▊     | 169/351 [04:55<05:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]#015 48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]#015 48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]#015 48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]#015 48%|████▊     | 170/351 [04:56<05:14,  1.74s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]#015 49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]#015 49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [04:59<05:42,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:12,512] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:01<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]#015 49%|████▉     | 172/351 [05:02<06:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]#015 49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]#015 49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]#015 49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:03<06:13,  2.10s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]#015 50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]#015 50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]#015 50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:05<05:37,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]#015 50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]#015 50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]#015 50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:06<05:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:19,536] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:09<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:08<05:34,  1.91s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]#015 50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]#015 50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:10<05:29,  1.89s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:12<05:06,  1.77s/it]#015 51%|█████     | 178/351 [05:12<05:06,  1.77s/it]#015 51%|█████     | 178/351 [05:12<05:06,  1.77s/it]#015 51%|█████     | 178/351 [05:12<05:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:12<05:06,  1.77s/it]#015 51%|█████     | 178/351 [05:12<05:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:12<05:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:12<05:06,  1.77s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:13<04:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:13<04:43,  1.65s/it]#015 51%|█████     | 179/351 [05:13<04:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:13<04:43,  1.65s/it]#015 51%|█████     | 179/351 [05:13<04:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:13<04:43,  1.65s/it]#015 51%|█████     | 179/351 [05:13<04:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:13<04:43,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:26,208] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:15<04:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]#015 52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]#015 52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]#015 52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]#015 52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:17<04:48,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]#015 52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]#015 52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]#015 52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]#015 52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:18<04:29,  1.60s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]#015 52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]#015 52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]#015 52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]#015 52%|█████▏    | 183/351 [05:20<04:24,  1.57s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:21<04:33,  1.64s/it]#015 52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]#015 52%|█████▏    | 184/351 [05:22<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:21<04:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]#015 53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]#015 53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]#015 53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]#015 53%|█████▎    | 185/351 [05:23<04:20,  1.57s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]#015 53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]#015 53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]#015 53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]#015 53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:24<04:08,  1.50s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]#015 53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [05:25<03:59,  1.46s/it]#015 53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]#015 53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]#015 53%|█████▎    | 187/351 [05:26<03:59,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:38,362] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]#015 54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [05:27<04:13,  1.56s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]#015 54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]#015 54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]#015 54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]#015 54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]#015 54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [05:29<04:03,  1.50s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]#015 54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]#015 54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]#015 54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [05:31<04:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]#015 54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]#015 54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [05:32<04:20,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:45,929] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]#015 55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]#015 55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]#015 55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]#015 55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [05:35<05:06,  1.93s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]#015 55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]#015 55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]#015 55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]#015 55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [05:36<04:41,  1.78s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]#015 55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]#015 55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [05:38<04:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]#015 56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]#015 56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]#015 56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]#015 56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [05:39<04:11,  1.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]#015 56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]#015 56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]#015 56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]#015 56%|█████▌    | 196/351 [05:42<04:49,  1.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]#015 56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]#015 56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]#015 56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]#015 56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [05:43<04:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]#015 56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]#015 56%|█████▋    | 198/351 [05:44<04:07,  1.62s/it]#015 56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]#015 56%|█████▋    | 198/351 [05:45<04:07,  1.62s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]#015 57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]#015 57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]#015 57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]#015 57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [05:46<03:55,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:48:58,740] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]#015 57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [05:48<04:06,  1.63s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]#015 57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]#015 57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [05:49<03:52,  1.55s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:50<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]#015 58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [05:51<03:46,  1.52s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]#015 58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]#015 58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]#015 58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [05:52<03:36,  1.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]#015 58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]#015 58%|█████▊    | 204/351 [05:53<03:33,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [05:53<03:32,  1.45s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]#015 58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]#015 58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]#015 58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [05:55<03:26,  1.42s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]#015 59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]#015 59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]#015 59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]#015 59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]#015 59%|█████▊    | 206/351 [05:56<03:21,  1.39s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [05:57<03:31,  1.47s/it]#015 59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]#015 59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]#015 59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [05:58<03:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:49:10,820] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]#015 59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]#015 59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:00<04:01,  1.69s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]#015 60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]#015 60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]#015 60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]#015 60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]#015 60%|█████▉    | 209/351 [06:01<03:55,  1.66s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]#015 60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]#015 60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]#015 60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:03<03:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:04<03:28,  1.49s/it]#015 60%|██████    | 211/351 [06:04<03:28,  1.49s/it]#015 60%|██████    | 211/351 [06:04<03:28,  1.49s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:04<03:28,  1.49s/it]#015 60%|██████    | 211/351 [06:04<03:28,  1.49s/it]#015 60%|██████    | 211/351 [06:04<03:28,  1.49s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:04<03:28,  1.49s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:04<03:28,  1.49s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:06<03:23,  1.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:06<03:23,  1.47s/it]#015 60%|██████    | 212/351 [06:06<03:23,  1.47s/it]#015 60%|██████    | 212/351 [06:05<03:23,  1.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:06<03:23,  1.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:06<03:23,  1.47s/it]#015 60%|██████    | 212/351 [06:05<03:23,  1.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:06<03:23,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:07<03:25,  1.49s/it]#015 61%|██████    | 213/351 [06:07<03:25,  1.49s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:07<03:25,  1.49s/it]#015 61%|██████    | 213/351 [06:07<03:25,  1.49s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:07<03:25,  1.49s/it]#015 61%|██████    | 213/351 [06:07<03:25,  1.49s/it]#015 61%|██████    | 213/351 [06:07<03:25,  1.49s/it]#015 61%|██████    | 213/351 [06:07<03:25,  1.49s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:08<03:17,  1.44s/it]#015 61%|██████    | 214/351 [06:08<03:17,  1.44s/it]#015 61%|██████    | 214/351 [06:08<03:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:08<03:17,  1.44s/it]#015 61%|██████    | 214/351 [06:08<03:17,  1.44s/it]#015 61%|██████    | 214/351 [06:08<03:17,  1.44s/it]#015 61%|██████    | 214/351 [06:08<03:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:08<03:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]#015 61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]#015 61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]#015 61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]#015 61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]#015 61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:11<03:55,  1.73s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]#015 62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]#015 62%|██████▏   | 216/351 [06:12<03:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]#015 62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]#015 62%|██████▏   | 216/351 [06:13<03:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]#015 62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]#015 62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]#015 62%|██████▏   | 217/351 [06:14<03:41,  1.65s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]#015 62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]#015 62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]#015 62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]#015 62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]#015 62%|██████▏   | 218/351 [06:15<03:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:17<03:16,  1.49s/it]#015 62%|██████▏   | 219/351 [06:17<03:16,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]#015 62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]#015 62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:17<03:17,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:49:30,176] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]#015 63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [06:19<03:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]#015 63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]#015 63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]#015 63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]#015 63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [06:21<03:59,  1.84s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]#015 63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]#015 63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]#015 63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]#015 63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [06:23<04:12,  1.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]#015 64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]#015 64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]#015 64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]#015 64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [06:25<04:13,  1.98s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:49:38,612] [WARNING] [stage3.py:1832:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]#015 64%|██████▍   | 224/351 [06:28<04:20,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]#015 64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]#015 64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]#015 64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [06:30<04:18,  2.05s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]#015 64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]#015 64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]#015 64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]#015 64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]#015 64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [06:32<04:33,  2.18s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]#015 65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]#015 65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]#015 65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]#015 65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]#015 65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [06:35<04:45,  2.30s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]#015 65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]#015 65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:49:49,433] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [06:38<05:35,  2.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]#015 65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]#015 65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]#015 65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]#015 65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [06:40<04:53,  2.40s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]#015 66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]#015 66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]#015 66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]#015 66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [06:41<04:12,  2.09s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]#015 66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [06:43<03:43,  1.86s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:49:56,672] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]#015 66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]#015 66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [06:46<04:18,  2.17s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]#015 66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]#015 66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]#015 66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]#015 66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [06:47<03:52,  1.97s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]#015 67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]#015 67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]#015 67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]#015 67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [06:49<03:49,  1.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]#015 67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]#015 67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]#015 67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]#015 67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [06:51<03:42,  1.92s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:04,597] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:53<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [06:54<04:06,  2.14s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]#015 68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]#015 68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]#015 68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]#015 68%|██████▊   | 237/351 [06:55<03:39,  1.93s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]#015 68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]#015 68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]#015 68%|██████▊   | 238/351 [06:56<03:17,  1.75s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]#015 68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]#015 68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]#015 68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]#015 68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [06:58<03:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:11,338] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]#015 68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:00<03:31,  1.90s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]#015 69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]#015 69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]#015 69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:02<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]#015 69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]#015 69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:03<02:56,  1.62s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]#015 69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]#015 69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:04<02:46,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:16,984] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]#015 70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]#015 70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:06<02:46,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]#015 70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]#015 70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]#015 70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]#015 70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:07<02:44,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:08<02:44,  1.55s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]#015 70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]#015 70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:09<02:36,  1.49s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:11<02:57,  1.71s/it]#015 70%|███████   | 247/351 [07:11<02:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:11<02:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:11<02:57,  1.71s/it]#015 70%|███████   | 247/351 [07:11<02:57,  1.71s/it]#015 70%|███████   | 247/351 [07:11<02:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:11<02:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:11<02:57,  1.71s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:13<03:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:13<03:08,  1.83s/it]#015 71%|███████   | 248/351 [07:13<03:08,  1.83s/it]#015 71%|███████   | 248/351 [07:13<03:08,  1.83s/it]#015 71%|███████   | 248/351 [07:13<03:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:13<03:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:13<03:08,  1.83s/it]#015 71%|███████   | 248/351 [07:13<03:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:15<02:53,  1.70s/it]#015 71%|███████   | 249/351 [07:15<02:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:14<02:53,  1.70s/it]#015 71%|███████   | 249/351 [07:15<02:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:15<02:53,  1.70s/it]#015 71%|███████   | 249/351 [07:15<02:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:15<02:53,  1.70s/it]#015 71%|███████   | 249/351 [07:15<02:53,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:16<02:40,  1.59s/it]#015 71%|███████   | 250/351 [07:16<02:40,  1.59s/it]#015 71%|███████   | 250/351 [07:16<02:40,  1.59s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:16<02:40,  1.59s/it]#015 71%|███████   | 250/351 [07:16<02:40,  1.59s/it]#015 71%|███████   | 250/351 [07:16<02:40,  1.59s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:16<02:40,  1.59s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:16<02:40,  1.59s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]#015 72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]#015 72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]#015 72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:18<02:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:17<02:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:17<02:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]#015 72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]#015 72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]#015 72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]#015 72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:19<02:34,  1.56s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]#015 72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]#015 72%|███████▏  | 253/351 [07:20<02:30,  1.54s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]#015 72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]#015 72%|███████▏  | 253/351 [07:21<02:30,  1.54s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:20<02:30,  1.54s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]#015 72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]#015 72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]#015 72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]#015 72%|███████▏  | 254/351 [07:22<02:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]#015 73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:23<02:17,  1.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]#015 73%|███████▎  | 256/351 [07:24<02:15,  1.43s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]#015 73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]#015 73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]#015 73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]#015 73%|███████▎  | 256/351 [07:25<02:15,  1.43s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]#015 73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]#015 73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]#015 73%|███████▎  | 257/351 [07:26<02:11,  1.40s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]#015 74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]#015 74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [07:28<02:19,  1.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]#015 74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:29<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]#015 74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [07:30<02:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:42,164] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]#015 74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]#015 74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [07:31<02:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]#015 74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]#015 74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [07:33<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [07:33<02:37,  1.75s/it]#015 74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [07:33<02:37,  1.76s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]#015 75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]#015 75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]#015 75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [07:35<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]#015 75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]#015 75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]#015 75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]#015 75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]#015 75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [07:37<02:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:49,968] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]#015 75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [07:39<02:44,  1.90s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]#015 75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]#015 75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]#015 75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [07:41<02:53,  2.02s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]#015 76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]#015 76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [07:43<02:48,  1.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]#015 76%|███████▌  | 267/351 [07:44<02:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]#015 76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]#015 76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]#015 76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [07:45<02:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:50:57,416] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]#015 76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]#015 76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [07:46<02:31,  1.82s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]#015 77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]#015 77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [07:48<02:18,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]#015 77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]#015 77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [07:49<02:13,  1.65s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]#015 77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]#015 77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]#015 77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]#015 77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [07:51<02:04,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:51:04,085] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]#015 77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]#015 77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [07:53<02:22,  1.81s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]#015 78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]#015 78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]#015 78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]#015 78%|███████▊  | 273/351 [07:55<02:18,  1.78s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]#015 78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]#015 78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]#015 78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [07:56<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]#015 78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]#015 78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]#015 78%|███████▊  | 275/351 [07:57<01:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [07:58<01:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]#015 79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [07:59<01:59,  1.60s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]#015 79%|███████▉  | 277/351 [08:01<02:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]#015 79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]#015 79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]#015 79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:02<01:55,  1.59s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]#015 79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]#015 79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]#015 79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]#015 79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:04<01:48,  1.51s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:51:16,275] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]#015 80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]#015 80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]#015 80%|███████▉  | 280/351 [08:05<01:47,  1.52s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]#015 80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:07<01:43,  1.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:08<01:39,  1.44s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:08<01:39,  1.44s/it]#015 80%|████████  | 282/351 [08:08<01:39,  1.44s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:08<01:39,  1.44s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:08<01:39,  1.44s/it]#015 80%|████████  | 282/351 [08:08<01:39,  1.44s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:08<01:39,  1.44s/it]#015 80%|████████  | 282/351 [08:08<01:39,  1.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:10<01:39,  1.47s/it]#015 81%|████████  | 283/351 [08:10<01:39,  1.47s/it]#015 81%|████████  | 283/351 [08:09<01:39,  1.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:10<01:39,  1.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:10<01:39,  1.47s/it]#015 81%|████████  | 283/351 [08:10<01:39,  1.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:10<01:39,  1.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:10<01:39,  1.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:11<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:11<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:13<01:37,  1.48s/it]#015 81%|████████  | 285/351 [08:13<01:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:12<01:37,  1.48s/it]#015 81%|████████  | 285/351 [08:13<01:37,  1.48s/it]#015 81%|████████  | 285/351 [08:13<01:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:13<01:37,  1.48s/it]#015 81%|████████  | 285/351 [08:13<01:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:13<01:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]#015 81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]#015 81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]#015 81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]#015 81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:14<01:33,  1.44s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]#015 82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]#015 82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]#015 82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]#015 82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:15<01:30,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]#015 82%|████████▏ | 288/351 [08:17<01:28,  1.41s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]#015 82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:18<01:25,  1.39s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]#015 83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:20<01:30,  1.49s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:22<01:37,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:51:34,749] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:24<01:44,  1.76s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]#015 83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]#015 83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [08:26<01:48,  1.88s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]#015 84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]#015 84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [08:28<01:50,  1.93s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]#015 84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]#015 84%|████████▍ | 295/351 [08:29<01:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]#015 84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]#015 84%|████████▍ | 295/351 [08:30<01:43,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:51:42,756] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [08:32<01:46,  1.94s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:33<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]#015 85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:34<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [08:33<01:41,  1.87s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]#015 85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]#015 85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]#015 85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [08:35<01:32,  1.75s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]#015 85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [08:36<01:24,  1.63s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:51:49,532] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:38<01:32,  1.81s/it]#015 85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [08:39<01:32,  1.81s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]#015 86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [08:40<01:25,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:41<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [08:42<01:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]#015 86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]#015 86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [08:43<01:15,  1.57s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]#015 87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]#015 87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]#015 87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]#015 87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [08:45<01:24,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]#015 87%|████████▋ | 305/351 [08:47<01:29,  1.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]#015 87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]#015 87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [08:48<01:29,  1.95s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]#015 87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]#015 87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [08:49<01:26,  1.93s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]#015 87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]#015 87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]#015 87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [08:51<01:21,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:52:04,884] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]#015 88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [08:54<01:31,  2.14s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]#015 88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]#015 88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]#015 88%|████████▊ | 309/351 [08:56<01:34,  2.25s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]#015 88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]#015 88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [08:59<01:32,  2.26s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]#015 89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:01<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:00<01:25,  2.15s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:52:13,833] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]#015 89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:03<01:25,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]#015 89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]#015 89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]#015 89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]#015 89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:05<01:25,  2.24s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]#015 89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:07<01:19,  2.15s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]#015 90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:09<01:13,  2.04s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:52:22,952] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]#015 90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:12<01:21,  2.33s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]#015 90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:13<01:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]#015 91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]#015 91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]#015 91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:15<01:01,  1.85s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]#015 91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:16<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:52:29,364] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]#015 91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:18<00:57,  1.87s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]#015 91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:20<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]#015 92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]#015 92%|█████████▏| 322/351 [09:21<00:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.65s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:21<00:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]#015 92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]#015 92%|█████████▏| 323/351 [09:23<00:47,  1.71s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]#015 92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]#015 92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]#015 92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:26<00:52,  1.96s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]#015 93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:27<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:28<00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]#015 93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]#015 93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]#015 93%|█████████▎| 326/351 [09:29<00:42,  1.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]#015 93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]#015 93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]#015 93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]#015 93%|█████████▎| 327/351 [09:30<00:39,  1.65s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]#015 93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]#015 93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [09:32<00:37,  1.64s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]#015 94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]#015 94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]#015 94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]#015 94%|█████████▎| 329/351 [09:33<00:35,  1.60s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]#015 94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [09:34<00:31,  1.52s/it]#015 94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]#015 94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]#015 94%|█████████▍| 330/351 [09:35<00:31,  1.52s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]#015 94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]#015 94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]#015 94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]#015 94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [09:36<00:29,  1.46s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [09:37<00:29,  1.53s/it]#015 95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]#015 95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]#015 95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [09:38<00:29,  1.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]#015 95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]#015 95%|█████████▍| 333/351 [09:39<00:30,  1.68s/it]#015 95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]#015 95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]#015 95%|█████████▍| 333/351 [09:40<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]#015 95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]#015 95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [09:42<00:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]#015 95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [09:44<00:29,  1.83s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:52:56,725] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]#015 96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [09:46<00:28,  1.91s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]#015 96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]#015 96%|█████████▌| 337/351 [09:47<00:24,  1.75s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [09:48<00:21,  1.63s/it]#015 96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]#015 96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]#015 96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]#015 96%|█████████▋| 338/351 [09:49<00:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [09:48<00:21,  1.63s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]#015 97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]#015 97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [09:50<00:18,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:51<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]#015 97%|█████████▋| 340/351 [09:52<00:17,  1.59s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]#015 97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]#015 97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]#015 97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [09:53<00:15,  1.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [09:54<00:14,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]#015 97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]#015 97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]#015 97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]#015 97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [09:55<00:14,  1.58s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]#015 98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]#015 98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]#015 98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [09:56<00:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:53:09,139] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]#015 98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [09:58<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [09:59<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]#015 98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]#015 98%|█████████▊| 345/351 [10:00<00:09,  1.62s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.68s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.68s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]#015 99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:01<00:08,  1.69s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]#015 99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]#015 99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]#015 99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:04<00:07,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:53:17,207] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]#015 99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:06<00:06,  2.05s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]#015 99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]#015 99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:09<00:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]#015100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]#015100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]#015100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]#015100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:11<00:02,  2.25s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  2.08s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  2.08s/it]#015100%|██████████| 351/351 [10:13<00:00,  2.08s/it]#015100%|██████████| 351/351 [10:13<00:00,  2.08s/it]#015100%|██████████| 351/351 [10:13<00:00,  2.08s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  2.08s/it]#015100%|██████████| 351/351 [10:13<00:00,  2.08s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  2.08s/it]#015100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 351/351 [10:13<00:00,  1.75s/it]#015100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:13<00:00,  1.75s/it]\u001b[0m\n",
      "\u001b[34mepoch=0: train_epoch_perplexity=tensor(15.6578, device='cuda:0') train_epoch_loss=tensor(2.7510, device='cuda:0')\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]#015  0%|          | 0/5 [00:00<?, ?it/s]#015  0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 2, but got module 0\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:09<00:36,  9.04s/it]#015 20%|██        | 1/5 [00:09<00:36,  9.04s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:09<00:36,  9.04s/it]#015 20%|██        | 1/5 [00:09<00:36,  9.04s/it]#015 20%|██        | 1/5 [00:09<00:36,  9.04s/it]#015 20%|██        | 1/5 [00:09<00:36,  9.04s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:09<00:36,  9.04s/it]#015 20%|██        | 1/5 [00:09<00:36,  9.04s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:15<00:23,  7.73s/it]#015 40%|████      | 2/5 [00:15<00:23,  7.73s/it]#015 40%|████      | 2/5 [00:15<00:23,  7.73s/it]#015 40%|████      | 2/5 [00:15<00:23,  7.73s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:15<00:23,  7.73s/it]#015 40%|████      | 2/5 [00:15<00:23,  7.73s/it]#015 40%|████      | 2/5 [00:15<00:23,  7.73s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:15<00:23,  7.73s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:22<00:14,  7.30s/it]#015 60%|██████    | 3/5 [00:22<00:14,  7.30s/it]#015 60%|██████    | 3/5 [00:22<00:14,  7.30s/it]#015 60%|██████    | 3/5 [00:22<00:14,  7.30s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:22<00:14,  7.30s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:22<00:14,  7.30s/it]#015 60%|██████    | 3/5 [00:22<00:14,  7.30s/it]#015 60%|██████    | 3/5 [00:22<00:14,  7.30s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:29<00:07,  7.17s/it]#015 80%|████████  | 4/5 [00:29<00:07,  7.17s/it]#015 80%|████████  | 4/5 [00:29<00:07,  7.17s/it]#015 80%|████████  | 4/5 [00:29<00:07,  7.17s/it]#015 80%|████████  | 4/5 [00:29<00:07,  7.17s/it]#015 80%|████████  | 4/5 [00:29<00:07,  7.17s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:29<00:07,  7.17s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:29<00:07,  7.17s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:36<00:00,  7.03s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.60MB/s]\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34m{'rouge1': 21.4615, 'rouge2': 3.1996, 'rougeL': 17.6372, 'rougeLsum': 18.6432, 'gen_len': 21.0}\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]#015  0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/351 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:03,636] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:08,  2.25s/it]#015  0%|          | 1/351 [00:02<13:08,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:08,  2.25s/it]#015  0%|          | 1/351 [00:02<13:08,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:08,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:08,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:08,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 1/351 [00:02<13:09,  2.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:04<12:17,  2.11s/it]#015  1%|          | 2/351 [00:04<12:17,  2.11s/it]#015  1%|          | 2/351 [00:04<12:17,  2.11s/it]#015  1%|          | 2/351 [00:04<12:17,  2.11s/it]#015  1%|          | 2/351 [00:04<12:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:04<12:17,  2.11s/it]#015  1%|          | 2/351 [00:04<12:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/351 [00:04<12:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:06<11:57,  2.06s/it]#015  1%|          | 3/351 [00:06<11:57,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:06<11:57,  2.06s/it]#015  1%|          | 3/351 [00:06<11:57,  2.06s/it]#015  1%|          | 3/351 [00:06<11:57,  2.06s/it]#015  1%|          | 3/351 [00:06<11:57,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:06<11:57,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/351 [00:06<11:57,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:08<11:54,  2.06s/it]#015  1%|          | 4/351 [00:08<11:54,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:08<11:54,  2.06s/it]#015  1%|          | 4/351 [00:08<11:54,  2.06s/it]#015  1%|          | 4/351 [00:08<11:54,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:08<11:54,  2.06s/it]#015  1%|          | 4/351 [00:08<11:54,  2.06s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/351 [00:08<11:54,  2.06s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:13,008] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]#015  1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/351 [00:11<14:27,  2.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:13<14:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:13<14:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:13<14:00,  2.43s/it]#015  2%|▏         | 6/351 [00:13<14:00,  2.43s/it]#015  2%|▏         | 6/351 [00:13<14:00,  2.43s/it]#015  2%|▏         | 6/351 [00:13<14:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/351 [00:13<14:00,  2.43s/it]#015  2%|▏         | 6/351 [00:13<14:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:16<14:24,  2.51s/it]#015  2%|▏         | 7/351 [00:16<14:24,  2.51s/it]#015  2%|▏         | 7/351 [00:16<14:24,  2.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:16<14:24,  2.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:16<14:24,  2.51s/it]#015  2%|▏         | 7/351 [00:16<14:24,  2.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/351 [00:16<14:24,  2.51s/it]#015  2%|▏         | 7/351 [00:16<14:24,  2.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:19<14:14,  2.49s/it]#015  2%|▏         | 8/351 [00:19<14:14,  2.49s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:19<14:14,  2.49s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:19<14:14,  2.49s/it]#015  2%|▏         | 8/351 [00:19<14:14,  2.49s/it]#015  2%|▏         | 8/351 [00:19<14:14,  2.49s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/351 [00:19<14:14,  2.49s/it]#015  2%|▏         | 8/351 [00:19<14:14,  2.49s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:24,420] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]#015  3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]#015  3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/351 [00:23<16:53,  2.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:25<15:39,  2.75s/it]#015  3%|▎         | 10/351 [00:25<15:39,  2.75s/it]#015  3%|▎         | 10/351 [00:25<15:39,  2.75s/it]#015  3%|▎         | 10/351 [00:25<15:39,  2.75s/it]#015  3%|▎         | 10/351 [00:25<15:39,  2.75s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:25<15:39,  2.75s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/351 [00:25<15:39,  2.75s/it]#015  3%|▎         | 10/351 [00:25<15:39,  2.75s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<14:08,  2.50s/it]#015  3%|▎         | 11/351 [00:27<14:08,  2.50s/it]#015  3%|▎         | 11/351 [00:27<14:08,  2.50s/it]#015  3%|▎         | 11/351 [00:27<14:08,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<14:08,  2.50s/it]#015  3%|▎         | 11/351 [00:27<14:08,  2.50s/it]#015  3%|▎         | 11/351 [00:27<14:08,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/351 [00:27<14:08,  2.50s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:29<13:17,  2.35s/it]#015  3%|▎         | 12/351 [00:29<13:17,  2.35s/it]#015  3%|▎         | 12/351 [00:29<13:17,  2.35s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:29<13:17,  2.35s/it]#015  3%|▎         | 12/351 [00:29<13:17,  2.35s/it]#015  3%|▎         | 12/351 [00:29<13:17,  2.35s/it]#015  3%|▎         | 12/351 [00:29<13:17,  2.35s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/351 [00:29<13:17,  2.35s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:34,004] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]#015  4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]#015  4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 13/351 [00:32<14:58,  2.66s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:35<15:15,  2.72s/it]#015  4%|▍         | 14/351 [00:35<15:15,  2.72s/it]#015  4%|▍         | 14/351 [00:35<15:15,  2.72s/it]#015  4%|▍         | 14/351 [00:35<15:15,  2.72s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:35<15:15,  2.72s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/351 [00:35<15:15,  2.72s/it]#015  4%|▍         | 14/351 [00:35<15:15,  2.72s/it]#015  4%|▍         | 14/351 [00:35<15:15,  2.72s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:37<14:42,  2.63s/it]#015  4%|▍         | 15/351 [00:37<14:42,  2.63s/it]#015  4%|▍         | 15/351 [00:37<14:42,  2.63s/it]#015  4%|▍         | 15/351 [00:37<14:42,  2.63s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:37<14:42,  2.63s/it]#015  4%|▍         | 15/351 [00:37<14:42,  2.63s/it]#015  4%|▍         | 15/351 [00:37<14:42,  2.63s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/351 [00:37<14:42,  2.63s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:40<13:57,  2.50s/it]#015  5%|▍         | 16/351 [00:40<13:57,  2.50s/it]#015  5%|▍         | 16/351 [00:40<13:57,  2.50s/it]#015  5%|▍         | 16/351 [00:40<13:57,  2.50s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:40<13:57,  2.50s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/351 [00:40<13:57,  2.50s/it]#015  5%|▍         | 16/351 [00:40<13:57,  2.50s/it]#015  5%|▍         | 16/351 [00:40<13:57,  2.50s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:44,908] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:27,  2.78s/it]#015  5%|▍         | 17/351 [00:43<15:27,  2.78s/it]#015  5%|▍         | 17/351 [00:43<15:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 17/351 [00:43<15:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:45<13:49,  2.49s/it]#015  5%|▌         | 18/351 [00:45<13:49,  2.49s/it]#015  5%|▌         | 18/351 [00:45<13:49,  2.49s/it]#015  5%|▌         | 18/351 [00:45<13:49,  2.49s/it]#015  5%|▌         | 18/351 [00:45<13:49,  2.49s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:45<13:49,  2.49s/it]#015  5%|▌         | 18/351 [00:45<13:49,  2.49s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/351 [00:45<13:49,  2.49s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:47<12:27,  2.25s/it]#015  5%|▌         | 19/351 [00:47<12:27,  2.25s/it]#015  5%|▌         | 19/351 [00:47<12:27,  2.25s/it]#015  5%|▌         | 19/351 [00:47<12:27,  2.25s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:47<12:27,  2.25s/it]#015  5%|▌         | 19/351 [00:47<12:27,  2.25s/it]#015  5%|▌         | 19/351 [00:47<12:27,  2.25s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 19/351 [00:47<12:27,  2.25s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:49<12:21,  2.24s/it]#015  6%|▌         | 20/351 [00:49<12:21,  2.24s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:49<12:21,  2.24s/it]#015  6%|▌         | 20/351 [00:49<12:21,  2.24s/it]#015  6%|▌         | 20/351 [00:49<12:21,  2.24s/it]#015  6%|▌         | 20/351 [00:49<12:21,  2.24s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/351 [00:49<12:21,  2.24s/it]#015  6%|▌         | 20/351 [00:49<12:21,  2.24s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:54:53,606] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]#015  6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]#015  6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/351 [00:52<13:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]#015  6%|▋         | 22/351 [00:54<12:22,  2.26s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:55<11:25,  2.09s/it]#015  7%|▋         | 23/351 [00:55<11:25,  2.09s/it]#015  7%|▋         | 23/351 [00:55<11:25,  2.09s/it]#015  7%|▋         | 23/351 [00:55<11:25,  2.09s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:55<11:25,  2.09s/it]#015  7%|▋         | 23/351 [00:55<11:25,  2.09s/it]#015  7%|▋         | 23/351 [00:55<11:25,  2.09s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/351 [00:55<11:25,  2.09s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:57<10:22,  1.90s/it]#015  7%|▋         | 24/351 [00:57<10:22,  1.90s/it]#015  7%|▋         | 24/351 [00:57<10:22,  1.90s/it]#015  7%|▋         | 24/351 [00:57<10:22,  1.90s/it]#015  7%|▋         | 24/351 [00:57<10:22,  1.90s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:57<10:22,  1.90s/it]#015  7%|▋         | 24/351 [00:57<10:22,  1.90s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/351 [00:57<10:22,  1.90s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]#015  7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]#015  7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/351 [00:59<11:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [01:01<10:24,  1.92s/it]#015  7%|▋         | 26/351 [01:01<10:24,  1.92s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [01:01<10:24,  1.92s/it]#015  7%|▋         | 26/351 [01:01<10:24,  1.92s/it]#015  7%|▋         | 26/351 [01:01<10:24,  1.92s/it]#015  7%|▋         | 26/351 [01:01<10:24,  1.92s/it]#015  7%|▋         | 26/351 [01:01<10:24,  1.92s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 26/351 [01:01<10:24,  1.92s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [01:03<10:43,  1.99s/it]#015  8%|▊         | 27/351 [01:03<10:43,  1.99s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [01:03<10:43,  1.99s/it]#015  8%|▊         | 27/351 [01:03<10:43,  1.99s/it]#015  8%|▊         | 27/351 [01:03<10:43,  1.99s/it]#015  8%|▊         | 27/351 [01:03<10:43,  1.99s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/351 [01:03<10:43,  1.99s/it]#015  8%|▊         | 27/351 [01:03<10:43,  1.99s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [01:05<10:33,  1.96s/it]#015  8%|▊         | 28/351 [01:05<10:33,  1.96s/it]#015  8%|▊         | 28/351 [01:05<10:33,  1.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [01:05<10:33,  1.96s/it]#015  8%|▊         | 28/351 [01:05<10:33,  1.96s/it]#015  8%|▊         | 28/351 [01:05<10:33,  1.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/351 [01:05<10:33,  1.96s/it]#015  8%|▊         | 28/351 [01:05<10:33,  1.96s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:08,800] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 29/351 [01:07<10:53,  2.03s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [01:08<09:55,  1.85s/it]#015  9%|▊         | 30/351 [01:08<09:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [01:08<09:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [01:08<09:55,  1.85s/it]#015  9%|▊         | 30/351 [01:08<09:55,  1.85s/it]#015  9%|▊         | 30/351 [01:08<09:55,  1.85s/it]#015  9%|▊         | 30/351 [01:08<09:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 30/351 [01:08<09:55,  1.85s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:10<09:15,  1.74s/it]#015  9%|▉         | 31/351 [01:10<09:15,  1.74s/it]#015  9%|▉         | 31/351 [01:10<09:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:10<09:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:10<09:15,  1.74s/it]#015  9%|▉         | 31/351 [01:10<09:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:10<09:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/351 [01:10<09:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:12<09:46,  1.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:12<09:46,  1.84s/it]#015  9%|▉         | 32/351 [01:12<09:46,  1.84s/it]#015  9%|▉         | 32/351 [01:12<09:46,  1.84s/it]#015  9%|▉         | 32/351 [01:12<09:46,  1.84s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/351 [01:12<09:46,  1.84s/it]#015  9%|▉         | 32/351 [01:12<09:46,  1.84s/it]#015  9%|▉         | 32/351 [01:12<09:46,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:17,048] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]#015  9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 33/351 [01:15<12:01,  2.27s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:18<12:30,  2.37s/it]#015 10%|▉         | 34/351 [01:18<12:30,  2.37s/it]#015 10%|▉         | 34/351 [01:18<12:30,  2.37s/it]#015 10%|▉         | 34/351 [01:18<12:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:18<12:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 34/351 [01:18<12:30,  2.37s/it]#015 10%|▉         | 34/351 [01:18<12:30,  2.37s/it]#015 10%|▉         | 34/351 [01:18<12:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:20<12:41,  2.41s/it]#015 10%|▉         | 35/351 [01:20<12:41,  2.41s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:20<12:41,  2.41s/it]#015 10%|▉         | 35/351 [01:20<12:41,  2.41s/it]#015 10%|▉         | 35/351 [01:20<12:41,  2.41s/it]#015 10%|▉         | 35/351 [01:20<12:41,  2.41s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:20<12:41,  2.41s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 35/351 [01:20<12:41,  2.41s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:23<12:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:23<12:41,  2.42s/it]#015 10%|█         | 36/351 [01:23<12:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:23<12:41,  2.42s/it]#015 10%|█         | 36/351 [01:23<12:41,  2.42s/it]#015 10%|█         | 36/351 [01:23<12:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:23<12:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 36/351 [01:23<12:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:27,948] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:26<14:07,  2.70s/it]#015 11%|█         | 37/351 [01:26<14:07,  2.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:26<14:07,  2.70s/it]#015 11%|█         | 37/351 [01:26<14:07,  2.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:26<14:07,  2.70s/it]#015 11%|█         | 37/351 [01:26<14:07,  2.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:26<14:07,  2.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/351 [01:26<14:07,  2.70s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:29<13:41,  2.63s/it]#015 11%|█         | 38/351 [01:29<13:41,  2.63s/it]#015 11%|█         | 38/351 [01:29<13:41,  2.63s/it]#015 11%|█         | 38/351 [01:29<13:41,  2.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:29<13:41,  2.63s/it]#015 11%|█         | 38/351 [01:29<13:41,  2.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:29<13:41,  2.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 38/351 [01:29<13:41,  2.63s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:31<13:06,  2.52s/it]#015 11%|█         | 39/351 [01:31<13:06,  2.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:31<13:06,  2.52s/it]#015 11%|█         | 39/351 [01:31<13:06,  2.52s/it]#015 11%|█         | 39/351 [01:31<13:06,  2.52s/it]#015 11%|█         | 39/351 [01:31<13:06,  2.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:31<13:06,  2.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 39/351 [01:31<13:06,  2.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]#015 11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]#015 11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]#015 11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]#015 11%|█▏        | 40/351 [01:33<12:56,  2.50s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:38,664] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]#015 12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]#015 12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/351 [01:37<14:31,  2.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]#015 12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]#015 12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]#015 12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]#015 12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/351 [01:38<12:30,  2.43s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]#015 12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]#015 12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]#015 12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]#015 12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]#015 12%|█▏        | 43/351 [01:40<11:11,  2.18s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]#015 13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]#015 13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]#015 13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]#015 13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]#015 13%|█▎        | 44/351 [01:42<10:40,  2.09s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:46,578] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]#015 13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]#015 13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]#015 13%|█▎        | 45/351 [01:45<11:53,  2.33s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]#015 13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]#015 13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]#015 13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 46/351 [01:46<10:32,  2.07s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]#015 13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]#015 13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]#015 13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]#015 13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 47/351 [01:48<10:04,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]#015 14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]#015 14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]#015 14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]#015 14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 48/351 [01:50<09:38,  1.91s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:55:53,732] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]#015 14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 49/351 [01:52<10:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]#015 14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]#015 14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]#015 14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]#015 14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]#015 14%|█▍        | 50/351 [01:53<09:08,  1.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]#015 15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]#015 15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 51/351 [01:55<09:15,  1.85s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:57<09:35,  1.92s/it]#015 15%|█▍        | 52/351 [01:57<09:35,  1.92s/it]#015 15%|█▍        | 52/351 [01:57<09:35,  1.92s/it]#015 15%|█▍        | 52/351 [01:57<09:35,  1.93s/it]#015 15%|█▍        | 52/351 [01:57<09:35,  1.93s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:57<09:35,  1.92s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:57<09:35,  1.92s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 52/351 [01:57<09:35,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:01,207] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 53/351 [01:59<09:42,  1.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]#015 15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]#015 15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]#015 15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]#015 15%|█▌        | 54/351 [02:02<10:05,  2.04s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]#015 16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]#015 16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]#015 16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]#015 16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 55/351 [02:03<09:51,  2.00s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]#015 16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]#015 16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]#015 16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 56/351 [02:05<08:52,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:09,111] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]#015 16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]#015 16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 57/351 [02:07<09:44,  1.99s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]#015 17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]#015 17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]#015 17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]#015 17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/351 [02:09<09:06,  1.86s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]#015 17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]#015 17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]#015 17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]#015 17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 59/351 [02:10<08:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]#015 17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]#015 17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]#015 17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]#015 17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]#015 17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 60/351 [02:12<08:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]#015 17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]#015 17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]#015 17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]#015 17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 61/351 [02:13<07:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]#015 18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]#015 18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]#015 18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]#015 18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/351 [02:15<07:22,  1.53s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]#015 18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]#015 18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]#015 18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]#015 18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 63/351 [02:16<07:06,  1.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]#015 18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]#015 18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]#015 18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]#015 18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 64/351 [02:18<07:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:22,868] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]#015 19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]#015 19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]#015 19%|█▊        | 65/351 [02:21<09:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]#015 19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]#015 19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]#015 19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]#015 19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 66/351 [02:23<09:57,  2.10s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]#015 19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]#015 19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]#015 19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]#015 19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 67/351 [02:26<10:21,  2.19s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]#015 19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]#015 19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]#015 19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 68/351 [02:28<10:31,  2.23s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]#015 20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]#015 20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:32,983] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]#015 20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 69/351 [02:31<11:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]#015 20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]#015 20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]#015 20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]#015 20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]#015 20%|█▉        | 70/351 [02:33<11:25,  2.44s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:35<10:10,  2.18s/it]#015 20%|██        | 71/351 [02:35<10:10,  2.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:35<10:10,  2.18s/it]#015 20%|██        | 71/351 [02:35<10:10,  2.18s/it]#015 20%|██        | 71/351 [02:35<10:10,  2.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:35<10:10,  2.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:35<10:10,  2.18s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 71/351 [02:35<10:10,  2.18s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:37<10:04,  2.17s/it]#015 21%|██        | 72/351 [02:37<10:04,  2.17s/it]#015 21%|██        | 72/351 [02:37<10:04,  2.17s/it]#015 21%|██        | 72/351 [02:37<10:04,  2.17s/it]#015 21%|██        | 72/351 [02:37<10:04,  2.17s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:37<10:04,  2.17s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 72/351 [02:37<10:04,  2.17s/it]#015 21%|██        | 72/351 [02:37<10:04,  2.17s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:42,313] [WARNING] [stage3.py:1832:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]#015 21%|██        | 73/351 [02:40<11:43,  2.53s/it]#015 21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 73/351 [02:40<11:43,  2.53s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:42<10:20,  2.24s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:42<10:20,  2.24s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:42<10:20,  2.24s/it]#015 21%|██        | 74/351 [02:42<10:20,  2.24s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:42<10:20,  2.24s/it]#015 21%|██        | 74/351 [02:42<10:20,  2.24s/it]#015 21%|██        | 74/351 [02:42<10:20,  2.24s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 74/351 [02:42<10:20,  2.24s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]#015 21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]#015 21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]#015 21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]#015 21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 75/351 [02:43<09:14,  2.01s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]#015 22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]#015 22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]#015 22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]#015 22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]#015 22%|██▏       | 76/351 [02:45<08:55,  1.95s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:50,468] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]#015 22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]#015 22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 77/351 [02:49<10:46,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]#015 22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]#015 22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]#015 22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]#015 22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]#015 22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 78/351 [02:50<09:30,  2.09s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]#015 23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]#015 23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]#015 23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]#015 23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]#015 23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/351 [02:52<08:57,  1.98s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]#015 23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]#015 23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]#015 23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 80/351 [02:53<08:35,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:56:57,648] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]#015 23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 81/351 [02:56<09:04,  2.02s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]#015 23%|██▎       | 82/351 [02:57<08:14,  1.84s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]#015 24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]#015 24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]#015 24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 83/351 [02:59<08:30,  1.90s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]#015 24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]#015 24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]#015 24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]#015 24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]#015 24%|██▍       | 84/351 [03:01<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:57:05,363] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:52,  2.00s/it]#015 24%|██▍       | 85/351 [03:03<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:52,  2.00s/it]#015 24%|██▍       | 85/351 [03:03<08:53,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:53,  2.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 85/351 [03:03<08:53,  2.00s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]#015 25%|██▍       | 86/351 [03:06<09:26,  2.14s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]#015 25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]#015 25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]#015 25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]#015 25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 87/351 [03:08<09:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]#015 25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]#015 25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]#015 25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 88/351 [03:09<08:17,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:57:13,426] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]#015 25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]#015 25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]#015 25%|██▌       | 89/351 [03:12<08:38,  1.98s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]#015 26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]#015 26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]#015 26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]#015 26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]#015 26%|██▌       | 90/351 [03:13<08:19,  1.91s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]#015 26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]#015 26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]#015 26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]#015 26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]#015 26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 91/351 [03:15<07:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]#015 26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]#015 26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]#015 26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]#015 26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 92/351 [03:17<07:39,  1.77s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]#015 26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]#015 26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]#015 26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]#015 26%|██▋       | 93/351 [03:19<08:22,  1.95s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]#015 27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]#015 27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]#015 27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]#015 27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 94/351 [03:20<07:46,  1.82s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]#015 27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]#015 27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]#015 27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]#015 27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]#015 27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 95/351 [03:22<07:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]#015 27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]#015 27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 96/351 [03:24<07:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:57:27,276] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]#015 28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]#015 28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]#015 28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]#015 28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 97/351 [03:25<07:18,  1.73s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]#015 28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]#015 28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]#015 28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]#015 28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 98/351 [03:27<06:59,  1.66s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]#015 28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]#015 28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]#015 28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 99/351 [03:28<06:33,  1.56s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]#015 28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]#015 28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]#015 28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 100/351 [03:30<06:15,  1.50s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]#015 29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]#015 29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]#015 29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]#015 29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 101/351 [03:31<06:07,  1.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]#015 29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]#015 29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]#015 29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]#015 29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 102/351 [03:33<06:51,  1.65s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]#015 29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]#015 29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]#015 29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]#015 29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 103/351 [03:35<07:16,  1.76s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]#015 30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]#015 30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]#015 30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]#015 30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]#015 30%|██▉       | 104/351 [03:37<07:25,  1.80s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:57:41,674] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]#015 30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]#015 30%|██▉       | 105/351 [03:40<08:38,  2.11s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:41<07:46,  1.90s/it]#015 30%|███       | 106/351 [03:41<07:46,  1.90s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:41<07:46,  1.90s/it]#015 30%|███       | 106/351 [03:41<07:46,  1.90s/it]#015 30%|███       | 106/351 [03:41<07:46,  1.90s/it]#015 30%|███       | 106/351 [03:41<07:46,  1.90s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:41<07:46,  1.90s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 106/351 [03:41<07:46,  1.90s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:43<07:03,  1.73s/it]#015 30%|███       | 107/351 [03:43<07:03,  1.73s/it]#015 30%|███       | 107/351 [03:43<07:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:43<07:03,  1.73s/it]#015 30%|███       | 107/351 [03:43<07:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:43<07:03,  1.73s/it]#015 30%|███       | 107/351 [03:43<07:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 107/351 [03:43<07:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]#015 31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 108/351 [03:44<06:46,  1.67s/it]#015 31%|███       | 108/351 [03:44<06:46,  1.67s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:46<07:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:57:48,212] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:46<07:26,  1.84s/it]#015 31%|███       | 109/351 [03:46<07:26,  1.84s/it]#015 31%|███       | 109/351 [03:46<07:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:46<07:26,  1.84s/it]#015 31%|███       | 109/351 [03:46<07:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:46<07:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 109/351 [03:46<07:26,  1.84s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]#015 31%|███▏      | 110/351 [03:48<06:51,  1.71s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]#015 32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]#015 32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]#015 32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 111/351 [03:50<07:06,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]#015 32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]#015 32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]#015 32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 112/351 [03:51<07:05,  1.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]#015 32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]#015 32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]#015 32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 113/351 [03:53<07:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]#015 32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]#015 32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]#015 32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 114/351 [03:55<06:50,  1.73s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]#015 33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]#015 33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]#015 33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]#015 33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 115/351 [03:56<06:33,  1.67s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]#015 33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]#015 33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]#015 33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]#015 33%|███▎      | 116/351 [03:58<06:09,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:01,624] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]#015 33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 117/351 [04:00<06:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]#015 34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]#015 34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]#015 34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]#015 34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 118/351 [04:01<06:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]#015 34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]#015 34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]#015 34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 119/351 [04:02<05:52,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]#015 34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]#015 34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]#015 34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 120/351 [04:04<05:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]#015 34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]#015 34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 121/351 [04:06<06:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]#015 35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]#015 35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]#015 35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]#015 35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 122/351 [04:07<05:52,  1.54s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]#015 35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]#015 35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]#015 35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]#015 35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 123/351 [04:09<05:37,  1.48s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]#015 35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]#015 35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]#015 35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]#015 35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 124/351 [04:10<05:38,  1.49s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:13,444] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]#015 36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]#015 36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]#015 36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 125/351 [04:12<05:39,  1.50s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]#015 36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]#015 36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]#015 36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 126/351 [04:14<06:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]#015 36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]#015 36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 127/351 [04:16<06:33,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]#015 36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]#015 36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]#015 36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 128/351 [04:18<07:10,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:22,024] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]#015 37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 129/351 [04:20<07:30,  2.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]#015 37%|███▋      | 130/351 [04:22<07:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:28,  1.77s/it]#015 37%|███▋      | 131/351 [04:23<06:28,  1.77s/it]#015 37%|███▋      | 131/351 [04:23<06:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:28,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 131/351 [04:23<06:29,  1.77s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]#015 38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]#015 38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]#015 38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 132/351 [04:25<05:59,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:29,288] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]#015 38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 133/351 [04:27<07:14,  1.99s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]#015 38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]#015 38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]#015 38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]#015 38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 134/351 [04:29<06:52,  1.90s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]#015 38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]#015 38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]#015 38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]#015 38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 135/351 [04:30<06:13,  1.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]#015 39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]#015 39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]#015 39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]#015 39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 136/351 [04:32<05:46,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:36,156] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]#015 39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]#015 39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]#015 39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 137/351 [04:34<06:42,  1.88s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]#015 39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]#015 39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]#015 39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 138/351 [04:36<06:38,  1.87s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]#015 40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]#015 40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]#015 40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]#015 40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]#015 40%|███▉      | 139/351 [04:38<06:09,  1.74s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:02,  1.72s/it]#015 40%|███▉      | 140/351 [04:39<06:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:02,  1.72s/it]#015 40%|███▉      | 140/351 [04:39<06:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 140/351 [04:39<06:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:42<06:49,  1.95s/it]#015 40%|████      | 141/351 [04:42<06:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:42<06:49,  1.95s/it]#015 40%|████      | 141/351 [04:42<06:49,  1.95s/it]#015 40%|████      | 141/351 [04:42<06:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:42<06:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:42<06:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 141/351 [04:42<06:49,  1.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 142/351 [04:43<06:11,  1.78s/it]#015 40%|████      | 142/351 [04:43<06:11,  1.78s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:45<05:58,  1.73s/it]#015 41%|████      | 143/351 [04:45<05:58,  1.73s/it]#015 41%|████      | 143/351 [04:45<05:58,  1.73s/it]#015 41%|████      | 143/351 [04:45<05:58,  1.73s/it]#015 41%|████      | 143/351 [04:45<05:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:45<05:58,  1.73s/it]#015 41%|████      | 143/351 [04:45<05:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 143/351 [04:45<05:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:46<05:38,  1.64s/it]#015 41%|████      | 144/351 [04:46<05:38,  1.64s/it]#015 41%|████      | 144/351 [04:46<05:38,  1.64s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:46<05:38,  1.64s/it]#015 41%|████      | 144/351 [04:46<05:38,  1.64s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:46<05:38,  1.64s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 144/351 [04:46<05:38,  1.64s/it]#015 41%|████      | 144/351 [04:46<05:38,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:49,724] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]#015 41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]#015 41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]#015 41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]#015 41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 145/351 [04:48<05:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]#015 42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]#015 42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]#015 42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]#015 42%|████▏     | 146/351 [04:49<05:29,  1.61s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]#015 42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]#015 42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]#015 42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]#015 42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]#015 42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 147/351 [04:51<05:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]#015 42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]#015 42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]#015 42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]#015 42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 148/351 [04:53<05:22,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:58:56,264] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]#015 42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]#015 42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]#015 42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 149/351 [04:54<05:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]#015 43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]#015 43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]#015 43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]#015 43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 150/351 [04:56<05:17,  1.58s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]#015 43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]#015 43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 151/351 [04:57<05:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]#015 43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]#015 43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 152/351 [04:58<04:50,  1.46s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:59:02,806] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]#015 44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]#015 44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]#015 44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 153/351 [05:01<05:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]#015 44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]#015 44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]#015 44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 154/351 [05:03<06:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]#015 44%|████▍     | 155/351 [05:05<05:59,  1.83s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]#015 44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]#015 44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 156/351 [05:07<06:16,  1.93s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]#015 45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:59:11,984] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 157/351 [05:10<07:28,  2.31s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]#015 45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]#015 45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]#015 45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 158/351 [05:12<06:39,  2.07s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]#015 45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]#015 45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]#015 45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]#015 45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 159/351 [05:14<06:35,  2.06s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]#015 46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]#015 46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]#015 46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]#015 46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 160/351 [05:16<06:29,  2.04s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]#015 46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]#015 46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:40,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:41,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 161/351 [05:18<06:41,  2.11s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]#015 46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]#015 46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]#015 46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 162/351 [05:19<06:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]#015 46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]#015 46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 163/351 [05:21<05:39,  1.81s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]#015 47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]#015 47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]#015 47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 164/351 [05:22<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]#015 47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]#015 47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]#015 47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 165/351 [05:25<05:45,  1.86s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]#015 47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]#015 47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]#015 47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 166/351 [05:26<05:26,  1.77s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]#015 48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]#015 48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]#015 48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 167/351 [05:27<05:01,  1.64s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]#015 48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]#015 48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]#015 48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 168/351 [05:29<04:47,  1.57s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]#015 48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]#015 48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]#015 48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]#015 48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]#015 48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 169/351 [05:30<04:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]#015 48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]#015 48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]#015 48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]#015 48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 170/351 [05:32<04:32,  1.51s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]#015 49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]#015 49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]#015 49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 171/351 [05:33<04:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]#015 49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]#015 49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]#015 49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 172/351 [05:35<04:45,  1.59s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:59:39,464] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]#015 49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]#015 49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]#015 49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]#015 49%|████▉     | 173/351 [05:38<05:31,  1.86s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]#015 50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]#015 50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]#015 50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]#015 50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 174/351 [05:39<05:30,  1.87s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]#015 50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]#015 50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]#015 50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 175/351 [05:42<05:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]#015 50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]#015 50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 176/351 [05:43<05:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:59:47,188] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]#015 50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 177/351 [05:45<05:38,  1.95s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:47<05:16,  1.83s/it]#015 51%|█████     | 178/351 [05:47<05:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:47<05:16,  1.83s/it]#015 51%|█████     | 178/351 [05:47<05:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:47<05:16,  1.83s/it]#015 51%|█████     | 178/351 [05:47<05:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:47<05:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 178/351 [05:47<05:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]#015 51%|█████     | 179/351 [05:49<05:07,  1.79s/it]#015 51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 179/351 [05:49<05:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]#015 51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]#015 51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]#015 51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 180/351 [05:50<04:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 18:59:54,011] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 181/351 [05:52<05:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]#015 52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]#015 52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]#015 52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 182/351 [05:54<04:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]#015 52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]#015 52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]#015 52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 183/351 [05:55<04:26,  1.59s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]#015 52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]#015 52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]#015 52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]#015 52%|█████▏    | 184/351 [05:56<04:12,  1.51s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]#015 53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]#015 53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]#015 53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]#015 53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]#015 53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 185/351 [05:58<04:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]#015 53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]#015 53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]#015 53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 186/351 [05:59<04:10,  1.52s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]#015 53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]#015 53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]#015 53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 187/351 [06:01<04:00,  1.47s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]#015 54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]#015 54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 188/351 [06:03<04:38,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:00:07,020] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]#015 54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]#015 54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]#015 54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]#015 54%|█████▍    | 189/351 [06:05<05:00,  1.85s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]#015 54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]#015 54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]#015 54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 190/351 [06:07<05:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]#015 54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]#015 54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]#015 54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]#015 54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 191/351 [06:10<05:22,  2.02s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]#015 55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]#015 55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]#015 55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]#015 55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 192/351 [06:11<05:08,  1.94s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]#015 55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:00:15,480] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]#015 55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]#015 55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 193/351 [06:14<05:23,  2.05s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]#015 55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]#015 55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]#015 55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]#015 55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]#015 55%|█████▌    | 194/351 [06:16<05:31,  2.11s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]#015 56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]#015 56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]#015 56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 195/351 [06:18<05:19,  2.05s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]#015 56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]#015 56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 196/351 [06:19<04:44,  1.84s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]#015 56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]#015 56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]#015 56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 197/351 [06:21<04:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]#015 56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]#015 56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]#015 56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]#015 56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 198/351 [06:23<05:06,  2.01s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]#015 57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]#015 57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]#015 57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]#015 57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 199/351 [06:25<05:08,  2.03s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]#015 57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]#015 57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]#015 57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 200/351 [06:28<05:17,  2.11s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:00:32,223] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 201/351 [06:30<05:38,  2.26s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]#015 58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]#015 58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]#015 58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 202/351 [06:32<04:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]#015 58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]#015 58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]#015 58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]#015 58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]#015 58%|█████▊    | 203/351 [06:33<04:27,  1.80s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]#015 58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]#015 58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]#015 58%|█████▊    | 204/351 [06:35<04:30,  1.84s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]#015 58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]#015 58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:00:39,325] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 205/351 [06:37<04:54,  2.01s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]#015 59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]#015 59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]#015 59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]#015 59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 206/351 [06:39<04:24,  1.83s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]#015 59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]#015 59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]#015 59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]#015 59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 207/351 [06:40<04:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]#015 59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]#015 59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]#015 59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 208/351 [06:42<03:50,  1.61s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]#015 60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]#015 60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]#015 60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]#015 60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 209/351 [06:44<04:10,  1.77s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]#015 60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]#015 60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]#015 60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]#015 60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 210/351 [06:45<04:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:47<03:43,  1.60s/it]#015 60%|██████    | 211/351 [06:47<03:43,  1.60s/it]#015 60%|██████    | 211/351 [06:47<03:43,  1.60s/it]#015 60%|██████    | 211/351 [06:47<03:43,  1.60s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:47<03:43,  1.60s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:47<03:43,  1.60s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:47<03:43,  1.60s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 211/351 [06:47<03:43,  1.60s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]#015 60%|██████    | 212/351 [06:48<03:30,  1.52s/it]#015 60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 212/351 [06:48<03:30,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:50<03:39,  1.59s/it]#015 61%|██████    | 213/351 [06:50<03:39,  1.59s/it]#015 61%|██████    | 213/351 [06:50<03:39,  1.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:50<03:39,  1.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 213/351 [06:50<03:39,  1.59s/it]#015 61%|██████    | 213/351 [06:50<03:39,  1.59s/it]#015 61%|██████    | 213/351 [06:50<03:39,  1.59s/it]#015 61%|██████    | 213/351 [06:50<03:39,  1.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:51<03:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:51<03:28,  1.52s/it]#015 61%|██████    | 214/351 [06:51<03:28,  1.52s/it]#015 61%|██████    | 214/351 [06:51<03:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:51<03:28,  1.52s/it]#015 61%|██████    | 214/351 [06:51<03:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:51<03:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 214/351 [06:51<03:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]#015 61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]#015 61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 215/351 [06:52<03:19,  1.47s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]#015 62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]#015 62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]#015 62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 216/351 [06:54<03:12,  1.43s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:00:57,365] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]#015 62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]#015 62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 217/351 [06:55<03:20,  1.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]#015 62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]#015 62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]#015 62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]#015 62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 218/351 [06:57<03:12,  1.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]#015 62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]#015 62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]#015 62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]#015 62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 219/351 [06:58<03:06,  1.41s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]#015 63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]#015 63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]#015 63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]#015 63%|██████▎   | 220/351 [07:00<03:30,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:05,263] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]#015 63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]#015 63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 221/351 [07:03<04:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]#015 63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]#015 63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]#015 63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 222/351 [07:05<04:07,  1.92s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]#015 64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]#015 64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]#015 64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 223/351 [07:06<03:51,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]#015 64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]#015 64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]#015 64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 224/351 [07:08<03:31,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]#015 64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]#015 64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]#015 64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]#015 64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 225/351 [07:10<03:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]#015 64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]#015 64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]#015 64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 226/351 [07:11<03:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]#015 65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]#015 65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]#015 65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 227/351 [07:13<03:17,  1.60s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]#015 65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]#015 65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]#015 65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]#015 65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]#015 65%|██████▍   | 228/351 [07:14<03:06,  1.52s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]#015 65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]#015 65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]#015 65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]#015 65%|██████▌   | 229/351 [07:16<03:08,  1.55s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]#015 66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]#015 66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]#015 66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]#015 66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]#015 66%|██████▌   | 230/351 [07:17<03:16,  1.62s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]#015 66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]#015 66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]#015 66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]#015 66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 231/351 [07:19<03:04,  1.54s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]#015 66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]#015 66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]#015 66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 232/351 [07:20<03:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:24,360] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]#015 66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]#015 66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 233/351 [07:22<03:21,  1.71s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]#015 67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]#015 67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]#015 67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]#015 67%|██████▋   | 234/351 [07:24<03:08,  1.61s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]#015 67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]#015 67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]#015 67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]#015 67%|██████▋   | 235/351 [07:25<02:57,  1.53s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]#015 67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]#015 67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]#015 67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]#015 67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 236/351 [07:27<02:58,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:30,245] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]#015 68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]#015 68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]#015 68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]#015 68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 237/351 [07:28<02:56,  1.55s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]#015 68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]#015 68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]#015 68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]#015 68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 238/351 [07:30<02:47,  1.49s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]#015 68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]#015 68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]#015 68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]#015 68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 239/351 [07:32<03:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]#015 68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]#015 68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]#015 68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]#015 68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 240/351 [07:33<02:59,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:37,667] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]#015 69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 241/351 [07:36<03:28,  1.89s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]#015 69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]#015 69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]#015 69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]#015 69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]#015 69%|██████▉   | 242/351 [07:37<03:17,  1.81s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]#015 69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]#015 69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]#015 69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]#015 69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 243/351 [07:39<03:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]#015 70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]#015 70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]#015 70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 244/351 [07:40<02:49,  1.59s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]#015 70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]#015 70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]#015 70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]#015 70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]#015 70%|██████▉   | 245/351 [07:42<02:54,  1.65s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:43<02:45,  1.58s/it]#015 70%|███████   | 246/351 [07:43<02:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:43<02:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:43<02:45,  1.58s/it]#015 70%|███████   | 246/351 [07:43<02:45,  1.58s/it]#015 70%|███████   | 246/351 [07:43<02:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:43<02:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 246/351 [07:43<02:45,  1.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:45<02:37,  1.51s/it]#015 70%|███████   | 247/351 [07:45<02:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:45<02:37,  1.51s/it]#015 70%|███████   | 247/351 [07:45<02:37,  1.51s/it]#015 70%|███████   | 247/351 [07:45<02:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:45<02:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:45<02:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 247/351 [07:45<02:37,  1.51s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:46<02:31,  1.47s/it]#015 71%|███████   | 248/351 [07:46<02:31,  1.47s/it]#015 71%|███████   | 248/351 [07:46<02:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:46<02:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:46<02:31,  1.47s/it]#015 71%|███████   | 248/351 [07:46<02:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 248/351 [07:46<02:31,  1.47s/it]#015 71%|███████   | 248/351 [07:46<02:31,  1.47s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:50,444] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:49<03:00,  1.77s/it]#015 71%|███████   | 249/351 [07:49<03:00,  1.77s/it]#015 71%|███████   | 249/351 [07:49<03:00,  1.77s/it]#015 71%|███████   | 249/351 [07:49<03:00,  1.77s/it]#015 71%|███████   | 249/351 [07:49<03:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:49<03:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:49<03:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 249/351 [07:49<03:00,  1.77s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:50<02:51,  1.70s/it]#015 71%|███████   | 250/351 [07:50<02:51,  1.70s/it]#015 71%|███████   | 250/351 [07:50<02:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:50<02:51,  1.70s/it]#015 71%|███████   | 250/351 [07:50<02:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 250/351 [07:50<02:51,  1.70s/it]#015 71%|███████   | 250/351 [07:50<02:51,  1.70s/it]#015 71%|███████   | 250/351 [07:50<02:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]#015 72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]#015 72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]#015 72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 251/351 [07:52<02:42,  1.63s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]#015 72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]#015 72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]#015 72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]#015 72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 252/351 [07:53<02:33,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:01:56,463] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]#015 72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]#015 72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]#015 72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 253/351 [07:55<02:35,  1.58s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]#015 72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]#015 72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]#015 72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]#015 72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]#015 72%|███████▏  | 254/351 [07:56<02:27,  1.52s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]#015 73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]#015 73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]#015 73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]#015 73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]#015 73%|███████▎  | 255/351 [07:58<02:27,  1.53s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]#015 73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]#015 73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]#015 73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]#015 73%|███████▎  | 256/351 [07:59<02:21,  1.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]#015 73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]#015 73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]#015 73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]#015 73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 257/351 [08:00<02:21,  1.51s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]#015 74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]#015 74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]#015 74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]#015 74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 258/351 [08:02<02:32,  1.64s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]#015 74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]#015 74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]#015 74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 259/351 [08:04<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]#015 74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]#015 74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]#015 74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]#015 74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]#015 74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 260/351 [08:05<02:23,  1.57s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:02:09,537] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]#015 74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]#015 74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 261/351 [08:08<02:37,  1.75s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]#015 75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]#015 75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]#015 75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]#015 75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]#015 75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 262/351 [08:09<02:26,  1.64s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]#015 75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]#015 75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]#015 75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]#015 75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]#015 75%|███████▍  | 263/351 [08:10<02:17,  1.56s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]#015 75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]#015 75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]#015 75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]#015 75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 264/351 [08:12<02:16,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]#015 75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]#015 75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]#015 75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]#015 75%|███████▌  | 265/351 [08:14<02:14,  1.57s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]#015 76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]#015 76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]#015 76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]#015 76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]#015 76%|███████▌  | 266/351 [08:15<02:08,  1.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]#015 76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]#015 76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]#015 76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]#015 76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 267/351 [08:17<02:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]#015 76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]#015 76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]#015 76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]#015 76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]#015 76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 268/351 [08:18<02:17,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:02:22,847] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]#015 77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 269/351 [08:21<02:36,  1.90s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]#015 77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]#015 77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]#015 77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]#015 77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]#015 77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 270/351 [08:22<02:24,  1.78s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]#015 77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]#015 77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]#015 77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]#015 77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 271/351 [08:24<02:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]#015 77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]#015 77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]#015 77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]#015 77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]#015 77%|███████▋  | 272/351 [08:25<02:04,  1.57s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]#015 78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]#015 78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]#015 78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 273/351 [08:27<02:07,  1.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]#015 78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]#015 78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]#015 78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]#015 78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]#015 78%|███████▊  | 274/351 [08:29<02:20,  1.82s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]#015 78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]#015 78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]#015 78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]#015 78%|███████▊  | 275/351 [08:31<02:13,  1.75s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]#015 79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]#015 79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]#015 79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]#015 79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]#015 79%|███████▊  | 276/351 [08:32<02:02,  1.64s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:02:36,832] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]#015 79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]#015 79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 277/351 [08:35<02:25,  1.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]#015 79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]#015 79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]#015 79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]#015 79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]#015 79%|███████▉  | 278/351 [08:36<02:13,  1.83s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]#015 79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]#015 79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]#015 79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]#015 79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 279/351 [08:38<02:02,  1.70s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]#015 80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]#015 80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]#015 80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 280/351 [08:39<01:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]#015 80%|████████  | 281/351 [08:41<01:59,  1.71s/it]#015 80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 281/351 [08:41<01:59,  1.71s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:43<01:51,  1.61s/it]#015 80%|████████  | 282/351 [08:43<01:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:43<01:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:43<01:51,  1.61s/it]#015 80%|████████  | 282/351 [08:43<01:51,  1.61s/it]#015 80%|████████  | 282/351 [08:43<01:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:43<01:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 282/351 [08:43<01:51,  1.61s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:44<01:47,  1.58s/it]#015 81%|████████  | 283/351 [08:44<01:47,  1.58s/it]#015 81%|████████  | 283/351 [08:44<01:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:44<01:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:44<01:47,  1.58s/it]#015 81%|████████  | 283/351 [08:44<01:47,  1.58s/it]#015 81%|████████  | 283/351 [08:44<01:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 283/351 [08:44<01:47,  1.58s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:46<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:46<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:46<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:46<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:46<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:46<01:41,  1.52s/it]#015 81%|████████  | 284/351 [08:46<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 284/351 [08:46<01:41,  1.52s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:47<01:41,  1.53s/it]#015 81%|████████  | 285/351 [08:47<01:41,  1.53s/it]#015 81%|████████  | 285/351 [08:47<01:41,  1.53s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:47<01:41,  1.53s/it]#015 81%|████████  | 285/351 [08:47<01:41,  1.53s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:47<01:41,  1.53s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:47<01:41,  1.53s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 285/351 [08:47<01:41,  1.54s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]#015 81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]#015 81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]#015 81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]#015 81%|████████▏ | 286/351 [08:49<01:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]#015 82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]#015 82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]#015 82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]#015 82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]#015 82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 287/351 [08:50<01:40,  1.57s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]#015 82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]#015 82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]#015 82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]#015 82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]#015 82%|████████▏ | 288/351 [08:52<01:35,  1.51s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]#015 82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]#015 82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]#015 82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]#015 82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 289/351 [08:53<01:34,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]#015 83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]#015 83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]#015 83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 290/351 [08:55<01:32,  1.52s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]#015 83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]#015 83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]#015 83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]#015 83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 291/351 [08:56<01:28,  1.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]#015 83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]#015 83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]#015 83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]#015 83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 292/351 [08:57<01:25,  1.44s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:01,651] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]#015 83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]#015 83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]#015 83%|████████▎ | 293/351 [09:00<01:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]#015 84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]#015 84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]#015 84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]#015 84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]#015 84%|████████▍ | 294/351 [09:01<01:33,  1.64s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]#015 84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]#015 84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]#015 84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]#015 84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]#015 84%|████████▍ | 295/351 [09:03<01:27,  1.56s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]#015 84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]#015 84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]#015 84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 296/351 [09:04<01:25,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:07,851] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]#015 85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 297/351 [09:06<01:27,  1.62s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]#015 85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]#015 85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]#015 85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]#015 85%|████████▍ | 298/351 [09:07<01:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]#015 85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]#015 85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 299/351 [09:09<01:20,  1.56s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]#015 85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]#015 85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]#015 85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]#015 85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 300/351 [09:11<01:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:15,657] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 301/351 [09:14<01:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]#015 86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]#015 86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]#015 86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]#015 86%|████████▌ | 302/351 [09:15<01:31,  1.87s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]#015 86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]#015 86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]#015 86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]#015 86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 303/351 [09:17<01:31,  1.91s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]#015 87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]#015 87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]#015 87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]#015 87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 304/351 [09:19<01:34,  2.01s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:24,424] [WARNING] [stage3.py:1832:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]#015 87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 305/351 [09:23<01:46,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]#015 87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]#015 87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]#015 87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]#015 87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 306/351 [09:25<01:41,  2.25s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]#015 87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]#015 87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]#015 87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]#015 87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]#015 87%|████████▋ | 307/351 [09:26<01:33,  2.11s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]#015 88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]#015 88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]#015 88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]#015 88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 308/351 [09:28<01:25,  1.99s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]#015 88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]#015 88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]#015 88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:32,872] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]#015 88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 309/351 [09:31<01:34,  2.26s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]#015 88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]#015 88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]#015 88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]#015 88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 310/351 [09:32<01:22,  2.01s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]#015 89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]#015 89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]#015 89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 311/351 [09:34<01:12,  1.82s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]#015 89%|████████▉ | 312/351 [09:35<01:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]#015 89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:39,532] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]#015 89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 313/351 [09:38<01:12,  1.90s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]#015 89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]#015 89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]#015 89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]#015 89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]#015 89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 314/351 [09:39<01:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]#015 90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]#015 90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]#015 90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]#015 90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]#015 90%|████████▉ | 315/351 [09:41<01:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]#015 90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]#015 90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]#015 90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]#015 90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 316/351 [09:42<00:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]#015 90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]#015 90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]#015 90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]#015 90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 317/351 [09:44<00:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]#015 91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]#015 91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]#015 91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]#015 91%|█████████ | 318/351 [09:45<00:53,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]#015 91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]#015 91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 319/351 [09:47<00:52,  1.65s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]#015 91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]#015 91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]#015 91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]#015 91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]#015 91%|█████████ | 320/351 [09:49<00:50,  1.62s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:52,196] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]#015 91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]#015 91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]#015 91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 321/351 [09:50<00:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]#015 92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]#015 92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]#015 92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]#015 92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 322/351 [09:52<00:51,  1.78s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]#015 92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]#015 92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]#015 92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]#015 92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 323/351 [09:54<00:49,  1.75s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]#015 92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]#015 92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 324/351 [09:56<00:44,  1.64s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]#015 93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:03:59,850] [WARNING] [stage3.py:1832:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]#015 93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]#015 93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 325/351 [09:58<00:48,  1.87s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]#015 93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]#015 93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]#015 93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]#015 93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]#015 93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 326/351 [09:59<00:43,  1.74s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]#015 93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]#015 93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]#015 93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 327/351 [10:01<00:39,  1.63s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]#015 93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]#015 93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]#015 93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]#015 93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 328/351 [10:02<00:36,  1.60s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]#015 94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]#015 94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]#015 94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]#015 94%|█████████▎| 329/351 [10:04<00:35,  1.62s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]#015 94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 330/351 [10:05<00:32,  1.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]#015 94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]#015 94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 331/351 [10:07<00:31,  1.59s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]#015 95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]#015 95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]#015 95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]#015 95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 332/351 [10:08<00:28,  1.52s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]#015 95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]#015 95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]#015 95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]#015 95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 333/351 [10:10<00:28,  1.57s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]#015 95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]#015 95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]#015 95%|█████████▌| 334/351 [10:12<00:26,  1.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]#015 95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]#015 95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 335/351 [10:13<00:23,  1.49s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]#015 96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]#015 96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]#015 96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]#015 96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 336/351 [10:14<00:21,  1.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]#015 96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]#015 96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]#015 96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]#015 96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 337/351 [10:16<00:21,  1.53s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]#015 96%|█████████▋| 338/351 [10:18<00:21,  1.64s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]#015 97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]#015 97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]#015 97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 339/351 [10:19<00:19,  1.60s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]#015 97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]#015 97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]#015 97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]#015 97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]#015 97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 340/351 [10:21<00:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:04:25,216] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]#015 97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]#015 97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]#015 97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 341/351 [10:23<00:18,  1.84s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]#015 97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]#015 97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]#015 97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]#015 97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 342/351 [10:25<00:15,  1.74s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]#015 98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]#015 98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]#015 98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]#015 98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]#015 98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 343/351 [10:26<00:12,  1.62s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]#015 98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]#015 98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]#015 98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]#015 98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 344/351 [10:28<00:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[2024-04-03 19:04:32,372] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]#015 98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]#015 98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]#015 98%|█████████▊| 345/351 [10:30<00:11,  1.92s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]#015 99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]#015 99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]#015 99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]#015 99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]#015 99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 346/351 [10:32<00:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]#015 99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]#015 99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]#015 99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]#015 99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 347/351 [10:33<00:06,  1.72s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]#015 99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]#015 99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]#015 99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]#015 99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 348/351 [10:35<00:04,  1.64s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]#015 99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]#015 99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]#015 99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]#015 99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]#015 99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 349/351 [10:37<00:03,  1.70s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]#015100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]#015100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]#015100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]#015100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 350/351 [10:38<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.65s/it]#015100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.65s/it]#015100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 351/351 [10:40<00:00,  1.82s/it]\u001b[0m\n",
      "\u001b[34mepoch=1: train_epoch_perplexity=tensor(5.3300, device='cuda:0') train_epoch_loss=tensor(1.6734, device='cuda:0')\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]#015  0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:15<01:00, 15.05s/it]#015 20%|██        | 1/5 [00:15<01:00, 15.05s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:29<00:44, 14.86s/it]#015 40%|████      | 2/5 [00:29<00:44, 14.86s/it]#015 40%|████      | 2/5 [00:29<00:44, 14.86s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:29<00:44, 14.86s/it]#015 40%|████      | 2/5 [00:29<00:44, 14.86s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:29<00:44, 14.86s/it]#015 40%|████      | 2/5 [00:29<00:44, 14.86s/it]#015 40%|████      | 2/5 [00:29<00:44, 14.86s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:44<00:29, 14.81s/it]#015 60%|██████    | 3/5 [00:44<00:29, 14.81s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:59<00:14, 14.76s/it]#015 80%|████████  | 4/5 [00:59<00:14, 14.76s/it]#015 80%|████████  | 4/5 [00:59<00:14, 14.76s/it]#015 80%|████████  | 4/5 [00:59<00:14, 14.76s/it]#015 80%|████████  | 4/5 [00:59<00:14, 14.76s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:59<00:14, 14.76s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:59<00:14, 14.76s/it]#015 80%|████████  | 4/5 [00:59<00:14, 14.76s/it]\u001b[0m\n",
      "\u001b[34mInvalidate trace cache @ step 0: expected module 0, but got module 2\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:13<00:00, 14.74s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [01:13<00:00, 14.74s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34m{'rouge1': 20.198, 'rouge2': 1.8688, 'rougeL': 16.1626, 'rougeLsum': 17.9093, 'gen_len': 36.06875}\u001b[0m\n",
      "\u001b[34m2024-04-03 19:06:01,534 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-03 19:06:01,534 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-03 19:06:01,534 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-03 19:06:19 Uploading - Uploading generated training model\n",
      "2024-04-03 19:06:19 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 1732\n",
      "Billable seconds: 1732\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"train\": s3_data_path}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "cur_dir = os.getcwd().replace(os.environ[\"HOME\"],\"\")\n",
    "HTML(f'''1. Paste the following command into the Studio Terminal <code style=\"background-color:gray;\">tensorboard --logdir {tensorboard_output_config.s3_output_path}</code><br>\n",
    "2. Click <a href='/jupyter/default/proxy/6006/'>here</a> to open TensorBoard''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to wait for the job to finish before we can deploy the model \n",
    "estimator.latest_training_job.wait(logs=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job has completed, we can deploy the model to a SageMaker Endpoint.\n",
    "We will use a [Deep learning container for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) for deployment which is optimized for serving large models in excess of 100B parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need a few additional imports for model deployment\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the docker image that will be used for inference\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a model deploymnent packages which will be used to deploy our model to a SageMaker Endpoint. The model deployment package is a tarball that contains the model artifacts, [inference code](src/inference/model.py), and any [additional dependencies](src/inference/requirements.txt) required to run the inference code. We'll go through the following steps to create the model deployment package:\n",
    "1. Download the trained model artifact from S3 to the local filesystem\n",
    "2. Cretae a `serving.properties` file that will configure our hosting environment\n",
    "3. Combine the trained model, the inference code, and the `serving.properties` file into a tarball with the following structure:\n",
    "```\n",
    "|-- model.py         # inference code\n",
    "|-- requirements.txt    # additional dependencies\n",
    "|-- serving.properties  # configuration file\n",
    "|-- <model_id>\\         # model artifacts\n",
    "    |-- config.json\n",
    "    |-- pytorch_model.bin\n",
    "    |-- special_tokens_map.json\n",
    "    |-- tokenizer_config.json\n",
    "    |-- tokenizer.json\n",
    "    |-- vocab.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model was trained using Low Rank Adaptation (LoRA), and as a result the model artifact is small (~10Mb) allowing us to repackage it along with our inference code. At deployment time, the base model will be downloaded from Hugging Face Hub and the LoRA weights will be applied to the base model. For deployment of larger models with LoRA weights, it is recommended to store the based model weights in your own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model artifacts into the inference code directory \n",
    "with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "    contents = tar.getnames()\n",
    "    model_id = os.path.dirname(contents[-1]) # model id is the name of the folder containing the model files as generated by the training job\n",
    "    tar.extractall(\"src/inference/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the serving.properties file\n",
    "# We'll use the python engine for inference and specify the model_id for the base model we want to use\n",
    "with open(\"src/inference/serving.properties\", \"w\") as f:\n",
    "    f.write(\n",
    "f\"\"\"engine=Python\n",
    "option.model_id={model_id}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything needed to create the model package. We'll combine the contents of the `src/inference` directory with the model artifact and create a tarball. We'll then upload the tarball to S3 and use the S3 URI to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd src/\n",
    "tar czvf model.tar.gz inference/\n",
    "mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, f\"{s3_key_prefix}/model\")\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {hf_s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(image_uri, model_data, role, endpoint_name, instance_type, sagemaker_session):\n",
    "    \"\"\"Helper function to create the SageMaker Endpoint resources and return a predictor\"\"\"\n",
    "    model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "    model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name)\n",
    "\n",
    "    # our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=serializers.JSONSerializer(),        # will convert python dict to json\n",
    "        deserializer=deserializers.JSONDeserializer(),  # will convert json to python dict\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique endpoint name\n",
    "hf_endpoint_name = sagemaker.utils.name_from_base(\"t5-summarization\")\n",
    "print(f\"Our endpoint will be called {hf_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment will take 5 to 10 minutes\n",
    "hf_predictor = deploy_model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=hf_s3_code_artifact,\n",
    "    role=role,\n",
    "    endpoint_name=hf_endpoint_name,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the endpoint deployed, we can generate summaries on dialogues from the test dataset. We'll randomly select an examples and generate summaries. You can also provide your own dialogue to generate summaries just be sure to use the same format as the examples in the train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dialogue_idx = randint(0, test_data.shape[0])\n",
    "random_dialogue = test_data[\"dialogue\"][random_dialogue_idx]\n",
    "\n",
    "output = hf_predictor.predict({\"inputs\": [random_dialogue], \"parameters\":{\"max_length\": 100}})\n",
    "output_summary = output[\"outputs\"][0][\"summary_text\"]\n",
    "\n",
    "print(\"#####DIALOGUE######\\n\", random_dialogue)\n",
    "print(\"\\n#####GENERATED SUMMARY######\\n\", output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint when finished experimenting\n",
    "hf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usMJlDcRmpXX"
      },
      "source": [
        "## Amazon SageMaker Initialization\n",
        "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role. Upgrade SageMaker SDK to the latest version.\n",
        "\n",
        "**NOTE:** This step might require a kernel restart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "bF5RKMf8mpXY"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiF7hSdPmpXY"
      },
      "outputs": [],
      "source": [
        "FILE_SYSTEM_ID = \"...\"\n",
        "FSX_SECURITY_GROUP_ID = \"...\"\n",
        "FSX_SUBNET = \"...\"\n",
        "BASE_PATH = \"...\"\n",
        "PRETRAINED_MODEL = \"...\"\n",
        "PRETRAINED_DIR = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBX9eQxjmpXY",
        "outputId": "72403373-3de0-451b-fe8c-378c3c6555e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sagemaker>=2.212 in /opt/conda/lib/python3.10/site-packages (2.217.0)\n",
            "Collecting sagemaker>=2.212\n",
            "  Downloading sagemaker-2.218.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (23.2.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (1.34.84)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (2.2.1)\n",
            "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (1.26.4)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (4.25.3)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (6.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (23.2)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (2.2.2)\n",
            "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (0.3.2)\n",
            "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (0.7.5)\n",
            "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (6.0.1)\n",
            "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (4.21.1)\n",
            "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (4.1.0)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (3.0.0)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (2.2.1)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (2.31.0)\n",
            "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (6.1.3)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (4.66.1)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.212) (5.9.8)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.84 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.212) (1.34.84)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.212) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.212) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.212) (3.17.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker>=2.212) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.212) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.212) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.212) (2024.2.2)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.212) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.212) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.212) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.212) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.212) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.212) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.212) (2024.1)\n",
            "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.212) (1.7.6.8)\n",
            "Collecting dill>=0.3.8 (from pathos->sagemaker>=2.212)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.212) (0.3.4)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->sagemaker>=2.212)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.212) (21.6.0)\n",
            "Downloading sagemaker-2.218.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Installing collected packages: dill, multiprocess, sagemaker\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.7\n",
            "    Uninstalling dill-0.3.7:\n",
            "      Successfully uninstalled dill-0.3.7\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.15\n",
            "    Uninstalling multiprocess-0.70.15:\n",
            "      Successfully uninstalled multiprocess-0.70.15\n",
            "  Attempting uninstall: sagemaker\n",
            "    Found existing installation: sagemaker 2.217.0\n",
            "    Uninstalling sagemaker-2.217.0:\n",
            "      Successfully uninstalled sagemaker-2.217.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.15.0 requires dill<0.3.8,>=0.3.0, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.8 multiprocess-0.70.16 sagemaker-2.218.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.10/site-packages (0.1.45)\n",
            "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker-experiments) (1.34.84)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.84 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.34.84)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.84->boto3>=1.16.27->sagemaker-experiments) (2.9.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.84->boto3>=1.16.27->sagemaker-experiments) (2.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.84->boto3>=1.16.27->sagemaker-experiments) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade \"sagemaker>=2.212\"\n",
        "%pip install sagemaker-experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5FXzLzVmpXZ",
        "outputId": "68f80637-db9f-46dc-cbf4-aea408a04e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
            "SageMaker Execution Role: arn:aws:iam::905418125508:role/service-role/AmazonSageMaker-ExecutionRole-20240317T151227\n",
            "AWS account: 905418125508\n",
            "AWS region: us-east-1\n",
            "Default bucket for this session:  sagemaker-us-east-1-905418125508\n",
            "CPU times: user 1.95 s, sys: 224 ms, total: 2.18 s\n",
            "Wall time: 2.84 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "role = (\n",
        "    get_execution_role()\n",
        ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
        "print(f\"SageMaker Execution Role: {role}\")\n",
        "\n",
        "client = boto3.client(\"sts\")\n",
        "account = client.get_caller_identity()[\"Account\"]\n",
        "print(f\"AWS account: {account}\")\n",
        "\n",
        "session = boto3.session.Session()\n",
        "region = session.region_name\n",
        "print(f\"AWS region: {region}\")\n",
        "\n",
        "sm_boto_client = boto3.client(\"sagemaker\")\n",
        "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
        "\n",
        "# get default bucket\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "print(\"Default bucket for this session: \", default_bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPE95lWrmpXZ"
      },
      "source": [
        "## Download and prepare GLUE/SST2 data\n",
        "Here you will download, prepare the GLUE/SST2 dataset and then copy the files to S3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7jrbzIlmpXZ"
      },
      "source": [
        "### Install the Hugging Face Transformers and Datasets libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V5fZOu2mpXZ",
        "outputId": "5f3fd7f5-4934-4292-bfab-334297bbbb17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\n",
            "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -q datasets==2.15.0 transformers pytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I1kOwpsmpXZ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vg7e7EMmpXZ",
        "outputId": "fe93f9ad-4343-41da-9ae5-bea8d8c04f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "import transformers\n",
        "import logging\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "from transformers.testing_utils import CaptureLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mhWFGClmpXZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP0ZKkSlmpXZ"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "5JyODFJDmpXZ"
      },
      "source": [
        "# Config Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QLOa2JowmpXZ"
      },
      "source": [
        "### Choose Model\n",
        "Choose to train either the GPT-NeoX or Llama-v2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-KHv-jmmpXZ"
      },
      "outputs": [],
      "source": [
        "model_type = \"llama_v2\"  # [gpt_neox, llama_v2]\n",
        "max_context_width = 4096  # For Llama v2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "YHUvcafnmpXZ"
      },
      "source": [
        "### Load data\n",
        "This section loads the [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset and splits it to training and validation datasets. You can update this section to load any HuggingFace dataset you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Yauw5XmpXZ"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"dataset_name\": \"glue\",\n",
        "    \"dataset_config_name\": \"sst2\",\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"cache_dir\": \"tmp\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa8DrRdnmpXZ"
      },
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset(\n",
        "    hyperparameters[\"dataset_name\"],\n",
        "    hyperparameters[\"dataset_config_name\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZUzTHW8mpXZ"
      },
      "outputs": [],
      "source": [
        "# Remove existing validation dataset as it is too small\n",
        "# to shard across all ranks.\n",
        "del raw_datasets[\"validation\"]\n",
        "if \"validation\" not in raw_datasets.keys():\n",
        "    validation_percentage = \"10%\"\n",
        "    raw_datasets[\"validation\"] = load_dataset(\n",
        "        hyperparameters[\"dataset_name\"],\n",
        "        hyperparameters[\"dataset_config_name\"],\n",
        "        split=f\"train[:{validation_percentage}]\",\n",
        "        cache_dir=hyperparameters[\"cache_dir\"],\n",
        "    )\n",
        "\n",
        "    raw_datasets[\"train\"] = load_dataset(\n",
        "        hyperparameters[\"dataset_name\"],\n",
        "        hyperparameters[\"dataset_config_name\"],\n",
        "        split=f\"train[{validation_percentage}:]\",\n",
        "        cache_dir=hyperparameters[\"cache_dir\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "eNm_0594mpXZ"
      },
      "source": [
        "### Load tokenizer\n",
        "Nearly every NLP task begins with a tokenizer. A tokenizer converts your text data into a format (token) that can be processed by the NLP model.\n",
        "The following cell loads a tokenizer for GPT-NeoX-7B using [AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.19.4/en/autoclass_tutorial#autotokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4yrYAa1mpXa"
      },
      "outputs": [],
      "source": [
        "tokenizer_kwargs = {\n",
        "    \"cache_dir\": hyperparameters[\"cache_dir\"],\n",
        "}\n",
        "\n",
        "# Pretrained meta-llama/Llama-2-7b-hf requires HuggingFace access, https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
        "# There also exist pretrained models without special access requirement e.g., https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
        "PRETRAINED_MODEL='NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, **tokenizer_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "eqY6g708mpXa"
      },
      "source": [
        "### Preprocess data\n",
        "\n",
        "The following two cells set up a function to run the tokenizer and group texts into chunks smaller than the block size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0aKSuR9mpXa"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
        "\n",
        "    with CaptureLogger(tok_logger) as cl:\n",
        "        output = tokenizer(examples[text_column_name])\n",
        "        # clm input could be much much longer than block_size\n",
        "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
        "            tok_logger.warning(\n",
        "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
        "            )\n",
        "    return output\n",
        "\n",
        "\n",
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "def group_texts(block_size, examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFK5sYz-mpXa"
      },
      "outputs": [],
      "source": [
        "column_names = raw_datasets[\"train\"].column_names\n",
        "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
        "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "import functools\n",
        "\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    functools.partial(group_texts, max_context_width),\n",
        "    batched=True,\n",
        "    #     num_proc=args.preprocessing_num_workers,\n",
        "    desc=f\"Grouping texts in chunks of {max_context_width}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMzhQcvEmpXa"
      },
      "outputs": [],
      "source": [
        "if hyperparameters[\"do_train\"]:\n",
        "    if \"train\" not in tokenized_datasets:\n",
        "        raise ValueError(\"--do_train requires a train dataset\")\n",
        "    train_dataset = lm_datasets[\"train\"]\n",
        "\n",
        "\n",
        "if hyperparameters[\"do_eval\"]:\n",
        "    if \"validation\" not in tokenized_datasets:\n",
        "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "    eval_dataset = lm_datasets[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e1031f53c8d34df8bb631da0ff73ada1",
            "ccc309a1836942a8b8392dbd4748a202"
          ]
        },
        "id": "UtiXJ9iNmpXa",
        "outputId": "4be8ecbd-f637-41af-9bb6-53606f124aef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1031f53c8d34df8bb631da0ff73ada1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccc309a1836942a8b8392dbd4748a202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_dataset_location = None\n",
        "validation_dataset_location = None\n",
        "\n",
        "\n",
        "if hyperparameters[\"do_train\"]:\n",
        "    train_dataset.to_json(\"./training.json\")\n",
        "    training_dataset_location = \"s3://{}/dataset/train/\".format(default_bucket)\n",
        "\n",
        "if hyperparameters[\"do_eval\"]:\n",
        "    eval_dataset.to_json(\"./validation.json\")\n",
        "    validation_dataset_location = \"s3://{}/dataset/validation/\".format(default_bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F54qzHiAmpXa",
        "outputId": "74f02fe4-b0dc-4aa8-c5d4-1d8a08b9b53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "upload: ./training.json to s3://sagemaker-us-east-1-905418125508/dataset/train/training.json\n",
            "upload: ./validation.json to s3://sagemaker-us-east-1-905418125508/dataset/validation/validation.json\n"
          ]
        }
      ],
      "source": [
        "if training_dataset_location is not None:\n",
        "    command = \"aws s3 cp ./training.json {}\".format(training_dataset_location)\n",
        "    os.system(command)\n",
        "\n",
        "if validation_dataset_location is not None:\n",
        "    command = \"aws s3 cp ./validation.json {}\".format(validation_dataset_location)\n",
        "    os.system(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKpCB9cOmpXa"
      },
      "outputs": [],
      "source": [
        "if hyperparameters[\"do_train\"]:\n",
        "    command = \"rm ./training.json\"\n",
        "    os.system(command)\n",
        "\n",
        "if hyperparameters[\"do_eval\"]:\n",
        "    command = \"rm ./validation.json\"\n",
        "    os.system(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWPgX7j8mpXa",
        "outputId": "7cd2085d-e6b7-4b5b-ebcf-e2ced96dd482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'training_dataset_location' (str)\n",
            "Stored 'validation_dataset_location' (str)\n"
          ]
        }
      ],
      "source": [
        "%store training_dataset_location\n",
        "%store validation_dataset_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxYIufdjmpXa",
        "outputId": "4c49b539-d7c0-4cb8-880f-9b5a9fab7671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored variables and their in-db values:\n",
            "training_dataset_location               -> 's3://sagemaker-us-east-1-905418125508/dataset/tra\n",
            "validation_dataset_location             -> 's3://sagemaker-us-east-1-905418125508/dataset/val\n"
          ]
        }
      ],
      "source": [
        "%store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvzkjLQompXa"
      },
      "source": [
        "## Specify Amazon S3 Bucket Paths\n",
        "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. In the cells above you downloaded the GLUE/SST2 training and validation split datasets and uploaded the json files in an S3 bucket in your account. This example will train on those json files.\n",
        "\n",
        "After you successfully run this example tensor parallel + fully sharded data parallel training job, you can modify the S3 bucket to where your own dataset is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MekM8zAmpXa"
      },
      "outputs": [],
      "source": [
        "%store -r training_dataset_location\n",
        "%store -r validation_dataset_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpsak9_nmpXa"
      },
      "outputs": [],
      "source": [
        "s3_train_bucket = training_dataset_location\n",
        "s3_test_bucket = validation_dataset_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsqVDbY_mpXa"
      },
      "source": [
        "The following S3 bucket will store the output artifacts of the training job. You can modify this as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOzWZJa7mpXa"
      },
      "outputs": [],
      "source": [
        "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-fsdp/{model_type}-outputdir/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOLvwIWYmpXa"
      },
      "source": [
        "## Define Data Channels for SageMaker Training Using Amazon S3\n",
        "In this step, define SageMaker training data channels to the S3 buckets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aKisN5XmpXa"
      },
      "outputs": [],
      "source": [
        "# Set below var to True if you want to use fsx (see next cell)\n",
        "use_fsx = False\n",
        "if not use_fsx:\n",
        "    if s3_train_bucket != None:\n",
        "        train = sagemaker.inputs.TrainingInput(\n",
        "            s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
        "        )\n",
        "        data_channels = {\"train\": train}\n",
        "    else:\n",
        "        data_channels = {\"train\": mock_data}\n",
        "    if s3_test_bucket != None:\n",
        "        test = sagemaker.inputs.TrainingInput(\n",
        "            s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
        "        )\n",
        "        data_channels[\"test\"] = test\n",
        "    else:\n",
        "        data_channels[\"test\"] = mock_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrXaZHwDmpXq"
      },
      "outputs": [],
      "source": [
        "tensor_parallel_degree = 8  # An integer in [1, world_size]. Note: we recommend using TP_DEGREE in [1,8] for intra-node communication as inter-node TP communication is slow.\n",
        "hybrid_shard_degree = (\n",
        "    0  # An integer in [0, world_size // tensor_parallel_degree] and its default value is 0.\n",
        ")\n",
        "offload_activations = True  # Enables SM activation offloading implementation.\n",
        "activation_loading_horizon = (\n",
        "    2  # Activation loading horizon, a positive integer and its default value is 2.\n",
        ")\n",
        "save_steps = 50  # Save step interval.\n",
        "max_steps = 50  # Maximum training steps.\n",
        "\n",
        "hyperparameters = {\n",
        "    \"train_batch_size\": 2,\n",
        "    \"val_batch_size\": 4,\n",
        "    \"fast_validation\": 0,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 12345,\n",
        "    \"bf16\": 1,\n",
        "    \"fp8\":0,\n",
        "    \"lr\": 0.0001,\n",
        "    \"min_lr\": 1e-05,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"lr_decay_style\": \"cosine\",\n",
        "    \"lr_decay_iters\": 47683,\n",
        "    \"warmup\": 0.0032,\n",
        "    \"plateau\": 0.0,\n",
        "    \"delayed_param\": 1,\n",
        "    \"num_kept_checkpoints\": 2,\n",
        "    \"checkpoint_freq\": save_steps,\n",
        "    \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
        "    \"validation_freq\": save_steps,\n",
        "    \"logging_freq\": 1,\n",
        "    \"weight_decay\": 0.2,\n",
        "    \"clean_cache\": 0,\n",
        "    \"activation_checkpointing\": 1,\n",
        "    \"enable_memory_profiling\": 0,\n",
        "    \"forward_prefetch\": 1,\n",
        "    \"vocab_size\": 50257,\n",
        "    \"limit_all_gathers\": 1,\n",
        "    \"backward_fetch_policy\": \"backward_pre\",\n",
        "    \"sharding_strategy\": \"hybrid_shard\",\n",
        "    \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
        "    \"model_type\": model_type,\n",
        "    \"use_smp_flash_attn\": 1,\n",
        "    \"use_smp_implementation\": 1,\n",
        "    \"distributed_backend\": \"nccl\",\n",
        "}\n",
        "\n",
        "if use_fsx:\n",
        "    # make sure to update paths for training_dir and test_dir based on the paths of datasets in fsx\n",
        "    # If you want to resume training, set checkpoint_dir to the same path as a previous job.\n",
        "    SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
        "    hyperparameters[\"checkpoint_dir\"] = f\"{SM_TRAIN_DIR}/smp-v2/{model_type}/checkpointdir\"\n",
        "    hyperparameters[\"training_dir\"] = f\"{SM_TRAIN_DIR}/datasets/c4/en/hf-tokenized/llama/train\"\n",
        "    hyperparameters[\"test_dir\"] = f\"{SM_TRAIN_DIR}/datasets/c4/en/hf-tokenized/llama/val\"\n",
        "    hyperparameters[\"zipped_data\"] = 1\n",
        "    hyperparameters[\"dataset_type\"] = \"hf\"\n",
        "else:\n",
        "    hyperparameters[\"zipped_data\"] = 0\n",
        "    hyperparameters[\"dataset_type\"] = \"gpt_jsonl\"\n",
        "\n",
        "# The checkpoint path (hyperparameters['checkpoint_dir'] or checkpoint_s3_uri) is not unique per job.\n",
        "# You need to modify as needed for different runs.\n",
        "# If same path is used for unrelated runs, this may increase time when downloading unnecessary checkpoints,\n",
        "# and cause conflicts when loading checkpoints.\n",
        "\n",
        "metric_definitions = [\n",
        "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
        "]  # Add your custom metric definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gKJ8Zn3mpXr"
      },
      "outputs": [],
      "source": [
        "if use_fsx:\n",
        "    hyperparameters[\"hf_pretrained_model_name_or_dir\"] = f\"{SM_TRAIN_DIR}{PRETRAINED_DIR}\"\n",
        "else:\n",
        "    hyperparameters[\"hf_pretrained_model_name_or_dir\"] = PRETRAINED_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD7pS8cJmpXr"
      },
      "outputs": [],
      "source": [
        "# Select your model size.\n",
        "model_config = \"7b\"  # [7b, 65b]\n",
        "\n",
        "if model_type == \"llama_v2\":\n",
        "    if model_config == \"7b\":\n",
        "        model_params = {\n",
        "            \"max_context_width\": 4096,\n",
        "            \"hidden_width\": 4096,\n",
        "            \"num_layers\": 32,\n",
        "            \"num_heads\": 32,\n",
        "            \"llama_intermediate_size\": 11008,\n",
        "        }\n",
        "    elif model_config == \"65b\":\n",
        "        model_params = {\n",
        "            \"max_context_width\": 4096,\n",
        "            \"hidden_width\": 8192,\n",
        "            \"num_layers\": 80,\n",
        "            \"num_heads\": 64,\n",
        "            \"llama_intermediate_size\": 22016,\n",
        "        }\n",
        "    else:\n",
        "        raise RuntimeError(\"Unknown model config\")\n",
        "\n",
        "for k, v in model_params.items():\n",
        "    hyperparameters[k] = v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJqHL4GrmpXr"
      },
      "source": [
        "## Specify Essential Parameters for a SageMaker Training Job\n",
        "Next, you use the `SageMaker Estimator class` to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances.\n",
        "\n",
        "- `instance_count`\n",
        "- `instance_type`\n",
        "- `volume_size`\n",
        "- `base_job_name`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmBNN6PympXr"
      },
      "source": [
        "### Update the Type and Number of EC2 Instance to Use\n",
        "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, determine the total number of GPUs (world size).\n",
        "$$\\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instances)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gyY9qWumpXr"
      },
      "outputs": [],
      "source": [
        "instance_type = \"ml.p4d.24xlarge\"\n",
        "\n",
        "# You need >= 1 p4d for 7b model.\n",
        "# You need >= 8 p4d for 65b model.\n",
        "instance_count = 1\n",
        "\n",
        "# set to the number of GPUs on that instance\n",
        "processes_per_host = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaycaSR2mpXr"
      },
      "source": [
        "### Specify a Base Job Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcZu3sohmpXr"
      },
      "outputs": [],
      "source": [
        "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
        "base_job_name = f'smp-{model_config}-{machine_str}-hs{hybrid_shard_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv-jn2LCmpXr"
      },
      "outputs": [],
      "source": [
        "if not use_fsx:\n",
        "    checkpoint_bucket = f\"s3://sagemaker-{region}-{account}/\"\n",
        "    checkpoint_s3_uri = (\n",
        "        f\"{checkpoint_bucket}/experiments/smp_fsdp-{model_type}-checkpoints/{base_job_name}/\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qDHOGJrmpXr"
      },
      "outputs": [],
      "source": [
        "kwargs = {}\n",
        "if use_fsx:\n",
        "    # Use the security group and subnet that was used to create the fsx filesystem\n",
        "    kwargs[\"security_group_ids\"] = [fsx_security_group_id]\n",
        "    kwargs[\"subnets\"] = [fsx_subnet]\n",
        "\n",
        "smp_estimator = PyTorch(\n",
        "    entry_point=\"train.py\",\n",
        "    hyperparameters=hyperparameters,\n",
        "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
        "    role=role,\n",
        "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
        "    checkpoint_local_path=hyperparameters[\"checkpoint_dir\"] if use_fsx else None,\n",
        "    instance_type=instance_type,\n",
        "    volume_size=400,\n",
        "    instance_count=instance_count,\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    distribution={\n",
        "        \"torch_distributed\": {\"enabled\": True},  # Use torchrun.\n",
        "        \"smdistributed\": {\n",
        "            \"modelparallel\": {\n",
        "                \"enabled\": True,\n",
        "                \"parameters\": {\n",
        "                    \"tensor_parallel_degree\": tensor_parallel_degree,\n",
        "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
        "                    \"sm_activation_offloading\": offload_activations,\n",
        "                    \"activation_loading_horizon\": activation_loading_horizon,\n",
        "                },\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    py_version=\"py310\",\n",
        "    framework_version=\"2.2.0\",\n",
        "    # image_uri=$IMAGE,  # Either provide `framework_version` or `image_uri`\n",
        "    output_path=s3_output_bucket,\n",
        "    max_run=86400,\n",
        "    debugger_hook_config=False,\n",
        "    base_job_name=base_job_name,\n",
        "    metric_definitions=metric_definitions,\n",
        "    **kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "s-cEVKG8mpXr"
      },
      "source": [
        "# FIT ESTIMATOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRd_FNnnmpXr"
      },
      "source": [
        "Finally, run the estimator.fit method to launch the SageMaker fine-tuning job of the model with hybrid sharding and activation offloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5fr6y2QmpXr",
        "outputId": "cfccb7b2-f40e-486e-9b91-8d8159f4f4dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
            "INFO:sagemaker:Creating training-job with name: smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-27 23:51:12 Starting - Starting the training job...\n",
            "2024-03-27 23:51:31 Pending - Training job waiting for capacity...\n",
            "2024-03-27 23:51:58 Pending - Preparing the instances for training........................\n",
            "2024-03-27 23:56:03 Downloading - Downloading input data.........\n",
            "2024-03-27 23:57:33 Downloading - Downloading the training image............\n",
            "2024-03-27 23:59:24 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
            "\u001b[34mbash: no job control in this shell\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:15,447 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:15,555 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:15,564 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:15,565 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:15,565 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:16,842 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
            "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: datasets>=2.16.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.16.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.0)\u001b[0m\n",
            "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
            "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: expecttest in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.2.1)\u001b[0m\n",
            "\u001b[34mCollecting flash-attn>=2.3.6 (from -r requirements.txt (line 6))\u001b[0m\n",
            "\u001b[34mDownloading flash_attn-2.5.6.tar.gz (2.5 MB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 43.2 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
            "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.10.0)\u001b[0m\n",
            "\u001b[34mCollecting humanize (from -r requirements.txt (line 8))\u001b[0m\n",
            "\u001b[34mDownloading humanize-4.9.0-py3-none-any.whl.metadata (7.9 kB)\u001b[0m\n",
            "\u001b[34mCollecting hypothesis (from -r requirements.txt (line 9))\u001b[0m\n",
            "\u001b[34mDownloading hypothesis-6.99.13-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.11.1.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (3.20.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (1.4.1.post1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.1.99)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.13.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers>=4.37.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (4.37.1)\u001b[0m\n",
            "\u001b[34mCollecting peft (from -r requirements.txt (line 18))\u001b[0m\n",
            "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
            "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 19))\u001b[0m\n",
            "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.16.1->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.1->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
            "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
            "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from hypothesis->-r requirements.txt (line 9)) (23.2.0)\u001b[0m\n",
            "\u001b[34mCollecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis->-r requirements.txt (line 9))\u001b[0m\n",
            "\u001b[34mDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from hypothesis->-r requirements.txt (line 9)) (1.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 14)) (1.12.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 14)) (1.3.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 14)) (3.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (2.1.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (1.62.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (2.28.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (1.0.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (3.5.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (68.1.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (0.7.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (3.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 16)) (0.41.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.1->-r requirements.txt (line 17)) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.1->-r requirements.txt (line 17)) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.1->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.1->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.1->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.1->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (5.3.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (0.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (4.7.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 16)) (1.3.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate>=0.12.0->-r requirements.txt (line 1)) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.1->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.1->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.1->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 16)) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (0.5.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.1->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 16)) (3.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
            "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mDownloading humanize-4.9.0-py3-none-any.whl (126 kB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.8/126.8 kB 22.4 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mDownloading hypothesis-6.99.13-py3-none-any.whl (457 kB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 458.0/458.0 kB 46.0 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 33.5 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\u001b[0m\n",
            "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 MB 24.1 MB/s eta 0:00:00\u001b[0m\n",
            "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
            "\u001b[34mDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\u001b[0m\n",
            "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
            "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
            "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
            "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120592258 sha256=d8cf54adda65f59820221d329d274e124972d7fdc05ab3b1130253c64eee6c8a\u001b[0m\n",
            "\u001b[34mStored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\u001b[0m\n",
            "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
            "\u001b[34mInstalling collected packages: sortedcontainers, hypothesis, humanize, responses, flash-attn, bitsandbytes, peft, evaluate\u001b[0m\n",
            "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
            "\u001b[34mFound existing installation: flash-attn 2.3.3\u001b[0m\n",
            "\u001b[34mUninstalling flash-attn-2.3.3:\u001b[0m\n",
            "\u001b[34mSuccessfully uninstalled flash-attn-2.3.3\u001b[0m\n",
            "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
            "\u001b[34mtransformer-engine 1.2.1+bbafb02 requires flash-attn!=2.0.9,!=2.1.0,<=2.3.3,>=1.0.6, but you have flash-attn 2.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[34mSuccessfully installed bitsandbytes-0.43.0 evaluate-0.4.1 flash-attn-2.5.6 humanize-4.9.0 hypothesis-6.99.13 peft-0.10.0 responses-0.18.0 sortedcontainers-2.4.0\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,076 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,076 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,196 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,301 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,311 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,406 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:35,417 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
            "\u001b[34mTraining Env:\u001b[0m\n",
            "\u001b[34m{\n",
            "    \"additional_framework_parameters\": {\n",
            "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
            "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
            "        \"sagemaker_torch_distributed_enabled\": true\n",
            "    },\n",
            "    \"channel_input_dirs\": {\n",
            "        \"test\": \"/opt/ml/input/data/test\",\n",
            "        \"train\": \"/opt/ml/input/data/train\"\n",
            "    },\n",
            "    \"current_host\": \"algo-1\",\n",
            "    \"current_instance_group\": \"homogeneousCluster\",\n",
            "    \"current_instance_group_hosts\": [\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
            "    \"distribution_hosts\": [\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"distribution_instance_groups\": [\n",
            "        \"homogeneousCluster\"\n",
            "    ],\n",
            "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
            "    \"hosts\": [\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"hyperparameters\": {\n",
            "        \"activation_checkpointing\": 1,\n",
            "        \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
            "        \"backward_fetch_policy\": \"backward_pre\",\n",
            "        \"beta1\": 0.9,\n",
            "        \"beta2\": 0.95,\n",
            "        \"bf16\": 1,\n",
            "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
            "        \"checkpoint_freq\": 50,\n",
            "        \"clean_cache\": 0,\n",
            "        \"dataset_type\": \"gpt_jsonl\",\n",
            "        \"delayed_param\": 1,\n",
            "        \"distributed_backend\": \"nccl\",\n",
            "        \"enable_memory_profiling\": 0,\n",
            "        \"epochs\": 100,\n",
            "        \"fast_validation\": 0,\n",
            "        \"forward_prefetch\": 1,\n",
            "        \"fp8\": 0,\n",
            "        \"hf_pretrained_model_name_or_dir\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
            "        \"hidden_width\": 4096,\n",
            "        \"limit_all_gathers\": 1,\n",
            "        \"llama_intermediate_size\": 11008,\n",
            "        \"logging_freq\": 1,\n",
            "        \"lr\": 0.0001,\n",
            "        \"lr_decay_iters\": 47683,\n",
            "        \"lr_decay_style\": \"cosine\",\n",
            "        \"max_context_width\": 4096,\n",
            "        \"max_steps\": 50,\n",
            "        \"min_lr\": 1e-05,\n",
            "        \"model_type\": \"llama_v2\",\n",
            "        \"mp_parameters\": {\n",
            "            \"tensor_parallel_degree\": 8,\n",
            "            \"hybrid_shard_degree\": 0,\n",
            "            \"sm_activation_offloading\": true,\n",
            "            \"activation_loading_horizon\": 2\n",
            "        },\n",
            "        \"num_heads\": 32,\n",
            "        \"num_kept_checkpoints\": 2,\n",
            "        \"num_layers\": 32,\n",
            "        \"plateau\": 0.0,\n",
            "        \"seed\": 12345,\n",
            "        \"sharding_strategy\": \"hybrid_shard\",\n",
            "        \"train_batch_size\": 2,\n",
            "        \"use_smp_flash_attn\": 1,\n",
            "        \"use_smp_implementation\": 1,\n",
            "        \"val_batch_size\": 4,\n",
            "        \"validation_freq\": 50,\n",
            "        \"vocab_size\": 50257,\n",
            "        \"warmup\": 0.0032,\n",
            "        \"weight_decay\": 0.2,\n",
            "        \"zipped_data\": 0\n",
            "    },\n",
            "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
            "    \"input_data_config\": {\n",
            "        \"test\": {\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        },\n",
            "        \"train\": {\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        }\n",
            "    },\n",
            "    \"input_dir\": \"/opt/ml/input\",\n",
            "    \"instance_groups\": [\n",
            "        \"homogeneousCluster\"\n",
            "    ],\n",
            "    \"instance_groups_dict\": {\n",
            "        \"homogeneousCluster\": {\n",
            "            \"instance_group_name\": \"homogeneousCluster\",\n",
            "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
            "            \"hosts\": [\n",
            "                \"algo-1\"\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    \"is_hetero\": false,\n",
            "    \"is_master\": true,\n",
            "    \"is_modelparallel_enabled\": true,\n",
            "    \"is_smddpmprun_installed\": false,\n",
            "    \"is_smddprun_installed\": true,\n",
            "    \"job_name\": \"smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911\",\n",
            "    \"log_level\": 20,\n",
            "    \"master_hostname\": \"algo-1\",\n",
            "    \"model_dir\": \"/opt/ml/model\",\n",
            "    \"module_dir\": \"s3://sagemaker-us-east-1-905418125508/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911/source/sourcedir.tar.gz\",\n",
            "    \"module_name\": \"train\",\n",
            "    \"network_interface_name\": \"eth0\",\n",
            "    \"num_cpus\": 96,\n",
            "    \"num_gpus\": 8,\n",
            "    \"num_neurons\": 0,\n",
            "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
            "    \"output_dir\": \"/opt/ml/output\",\n",
            "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
            "    \"resource_config\": {\n",
            "        \"current_host\": \"algo-1\",\n",
            "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
            "        \"current_group_name\": \"homogeneousCluster\",\n",
            "        \"hosts\": [\n",
            "            \"algo-1\"\n",
            "        ],\n",
            "        \"instance_groups\": [\n",
            "            {\n",
            "                \"instance_group_name\": \"homogeneousCluster\",\n",
            "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
            "                \"hosts\": [\n",
            "                    \"algo-1\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"network_interface_name\": \"eth0\"\n",
            "    },\n",
            "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
            "\u001b[34m}\u001b[0m\n",
            "\u001b[34mEnvironment variables:\u001b[0m\n",
            "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
            "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
            "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":50,\"clean_cache\":0,\"dataset_type\":\"gpt_jsonl\",\"delayed_param\":1,\"distributed_backend\":\"nccl\",\"enable_memory_profiling\":0,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"NousResearch/Llama-2-7b-chat-hf\",\"hidden_width\":4096,\"limit_all_gathers\":1,\"llama_intermediate_size\":11008,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":4096,\"max_steps\":50,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_layers\":32,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":2,\"use_smp_flash_attn\":1,\"use_smp_implementation\":1,\"val_batch_size\":4,\"validation_freq\":50,\"vocab_size\":50257,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0}\u001b[0m\n",
            "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
            "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
            "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
            "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
            "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
            "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
            "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
            "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
            "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
            "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
            "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
            "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
            "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
            "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
            "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
            "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
            "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
            "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905418125508/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911/source/sourcedir.tar.gz\u001b[0m\n",
            "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":50,\"clean_cache\":0,\"dataset_type\":\"gpt_jsonl\",\"delayed_param\":1,\"distributed_backend\":\"nccl\",\"enable_memory_profiling\":0,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"NousResearch/Llama-2-7b-chat-hf\",\"hidden_width\":4096,\"limit_all_gathers\":1,\"llama_intermediate_size\":11008,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":4096,\"max_steps\":50,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_layers\":32,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":2,\"use_smp_flash_attn\":1,\"use_smp_implementation\":1,\"val_batch_size\":4,\"validation_freq\":50,\"vocab_size\":50257,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
            "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--auto_wrap_policy\",\"transformer_auto_wrap_policy\",\"--backward_fetch_policy\",\"backward_pre\",\"--beta1\",\"0.9\",\"--beta2\",\"0.95\",\"--bf16\",\"1\",\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--checkpoint_freq\",\"50\",\"--clean_cache\",\"0\",\"--dataset_type\",\"gpt_jsonl\",\"--delayed_param\",\"1\",\"--distributed_backend\",\"nccl\",\"--enable_memory_profiling\",\"0\",\"--epochs\",\"100\",\"--fast_validation\",\"0\",\"--forward_prefetch\",\"1\",\"--fp8\",\"0\",\"--hf_pretrained_model_name_or_dir\",\"NousResearch/Llama-2-7b-chat-hf\",\"--hidden_width\",\"4096\",\"--limit_all_gathers\",\"1\",\"--llama_intermediate_size\",\"11008\",\"--logging_freq\",\"1\",\"--lr\",\"0.0001\",\"--lr_decay_iters\",\"47683\",\"--lr_decay_style\",\"cosine\",\"--max_context_width\",\"4096\",\"--max_steps\",\"50\",\"--min_lr\",\"1e-05\",\"--model_type\",\"llama_v2\",\"--mp_parameters\",\"activation_loading_horizon=2,hybrid_shard_degree=0,sm_activation_offloading=True,tensor_parallel_degree=8\",\"--num_heads\",\"32\",\"--num_kept_checkpoints\",\"2\",\"--num_layers\",\"32\",\"--plateau\",\"0.0\",\"--seed\",\"12345\",\"--sharding_strategy\",\"hybrid_shard\",\"--train_batch_size\",\"2\",\"--use_smp_flash_attn\",\"1\",\"--use_smp_implementation\",\"1\",\"--val_batch_size\",\"4\",\"--validation_freq\",\"50\",\"--vocab_size\",\"50257\",\"--warmup\",\"0.0032\",\"--weight_decay\",\"0.2\",\"--zipped_data\",\"0\"]\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
            "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
            "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
            "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
            "\u001b[34mSM_HP_AUTO_WRAP_POLICY=transformer_auto_wrap_policy\u001b[0m\n",
            "\u001b[34mSM_HP_BACKWARD_FETCH_POLICY=backward_pre\u001b[0m\n",
            "\u001b[34mSM_HP_BETA1=0.9\u001b[0m\n",
            "\u001b[34mSM_HP_BETA2=0.95\u001b[0m\n",
            "\u001b[34mSM_HP_BF16=1\u001b[0m\n",
            "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
            "\u001b[34mSM_HP_CHECKPOINT_FREQ=50\u001b[0m\n",
            "\u001b[34mSM_HP_CLEAN_CACHE=0\u001b[0m\n",
            "\u001b[34mSM_HP_DATASET_TYPE=gpt_jsonl\u001b[0m\n",
            "\u001b[34mSM_HP_DELAYED_PARAM=1\u001b[0m\n",
            "\u001b[34mSM_HP_DISTRIBUTED_BACKEND=nccl\u001b[0m\n",
            "\u001b[34mSM_HP_ENABLE_MEMORY_PROFILING=0\u001b[0m\n",
            "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
            "\u001b[34mSM_HP_FAST_VALIDATION=0\u001b[0m\n",
            "\u001b[34mSM_HP_FORWARD_PREFETCH=1\u001b[0m\n",
            "\u001b[34mSM_HP_FP8=0\u001b[0m\n",
            "\u001b[34mSM_HP_HF_PRETRAINED_MODEL_NAME_OR_DIR=NousResearch/Llama-2-7b-chat-hf\u001b[0m\n",
            "\u001b[34mSM_HP_HIDDEN_WIDTH=4096\u001b[0m\n",
            "\u001b[34mSM_HP_LIMIT_ALL_GATHERS=1\u001b[0m\n",
            "\u001b[34mSM_HP_LLAMA_INTERMEDIATE_SIZE=11008\u001b[0m\n",
            "\u001b[34mSM_HP_LOGGING_FREQ=1\u001b[0m\n",
            "\u001b[34mSM_HP_LR=0.0001\u001b[0m\n",
            "\u001b[34mSM_HP_LR_DECAY_ITERS=47683\u001b[0m\n",
            "\u001b[34mSM_HP_LR_DECAY_STYLE=cosine\u001b[0m\n",
            "\u001b[34mSM_HP_MAX_CONTEXT_WIDTH=4096\u001b[0m\n",
            "\u001b[34mSM_HP_MAX_STEPS=50\u001b[0m\n",
            "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
            "\u001b[34mSM_HP_MODEL_TYPE=llama_v2\u001b[0m\n",
            "\u001b[34mSM_HP_MP_PARAMETERS={\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8}\u001b[0m\n",
            "\u001b[34mSM_HP_NUM_HEADS=32\u001b[0m\n",
            "\u001b[34mSM_HP_NUM_KEPT_CHECKPOINTS=2\u001b[0m\n",
            "\u001b[34mSM_HP_NUM_LAYERS=32\u001b[0m\n",
            "\u001b[34mSM_HP_PLATEAU=0.0\u001b[0m\n",
            "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
            "\u001b[34mSM_HP_SHARDING_STRATEGY=hybrid_shard\u001b[0m\n",
            "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=2\u001b[0m\n",
            "\u001b[34mSM_HP_USE_SMP_FLASH_ATTN=1\u001b[0m\n",
            "\u001b[34mSM_HP_USE_SMP_IMPLEMENTATION=1\u001b[0m\n",
            "\u001b[34mSM_HP_VAL_BATCH_SIZE=4\u001b[0m\n",
            "\u001b[34mSM_HP_VALIDATION_FREQ=50\u001b[0m\n",
            "\u001b[34mSM_HP_VOCAB_SIZE=50257\u001b[0m\n",
            "\u001b[34mSM_HP_WARMUP=0.0032\u001b[0m\n",
            "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
            "\u001b[34mSM_HP_ZIPPED_DATA=0\u001b[0m\n",
            "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
            "\u001b[34mInvoking script with the following command:\u001b[0m\n",
            "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 8 train.py --activation_checkpointing 1 --auto_wrap_policy transformer_auto_wrap_policy --backward_fetch_policy backward_pre --beta1 0.9 --beta2 0.95 --bf16 1 --checkpoint_dir /opt/ml/checkpoints --checkpoint_freq 50 --clean_cache 0 --dataset_type gpt_jsonl --delayed_param 1 --distributed_backend nccl --enable_memory_profiling 0 --epochs 100 --fast_validation 0 --forward_prefetch 1 --fp8 0 --hf_pretrained_model_name_or_dir NousResearch/Llama-2-7b-chat-hf --hidden_width 4096 --limit_all_gathers 1 --llama_intermediate_size 11008 --logging_freq 1 --lr 0.0001 --lr_decay_iters 47683 --lr_decay_style cosine --max_context_width 4096 --max_steps 50 --min_lr 1e-05 --model_type llama_v2 --mp_parameters activation_loading_horizon=2,hybrid_shard_degree=0,sm_activation_offloading=True,tensor_parallel_degree=8 --num_heads 32 --num_kept_checkpoints 2 --num_layers 32 --plateau 0.0 --seed 12345 --sharding_strategy hybrid_shard --train_batch_size 2 --use_smp_flash_attn 1 --use_smp_implementation 1 --val_batch_size 4 --validation_freq 50 --vocab_size 50257 --warmup 0.0032 --weight_decay 0.2 --zipped_data 0\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:36,480] torch.distributed.run: [WARNING] \u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:36,480] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:36,480] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:36,480] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.37.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.27.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:48.941: I torch/sagemaker/state_handler.py:148] Sizes (pp, rep, sdp, tp, world) = (1, None, 0, 8, 8).\u001b[0m\n",
            "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:00:48.942: I torch/sagemaker/state_handler.py:177] Rewrite sizes (pp, rep, sdp, tp, world) = (1, None, 1, 8, 8).\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:51] Env variables (len = 156):\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [000/156] AWS_CONTAINER_CREDENTIALS_RELATIVE_URI: `/v2/credentials/proxy-44777dbc86a7ac7571ce36fdf753ce2cc1193192c9916405bdabad43c38bd9f9-customer`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [001/156] AWS_REGION          : `us-east-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [002/156] BOOST_VERSION       : `1.84.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [003/156] CMAKE_PREFIX_PATH   : `$(dirname $(which conda))/../`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [004/156] CUDA_HOME           : `/opt/conda`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [005/156] CUDA_MODULE_LOADING : `LAZY`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [006/156] CUDA_VERSION        : `12.1.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [007/156] CURRENT_HOST        : `algo-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [008/156] DEBIAN_FRONTEND     : `noninteractive`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [009/156] DGLBACKEND          : `pytorch`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [010/156] DLC_CONTAINER_TYPE  : `training`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [011/156] DMLC_INTERFACE      : `eth0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [012/156] EFA_VERSION         : `1.30.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [013/156] FI_EFA_USE_DEVICE_RDMA: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [014/156] FI_PROVIDER         : `efa`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [015/156] GDRCOPY_VERSION     : `2.3.1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [016/156] GROUP_RANK          : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [017/156] GROUP_WORLD_SIZE    : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [018/156] HOME                : `/root`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [019/156] HOSTNAME            : `ip-10-0-253-105.ec2.internal`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [020/156] KMP_DUPLICATE_LIB_OK: `True`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [021/156] KMP_INIT_AT_FORK    : `FALSE`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [022/156] LANG                : `C.UTF-8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [023/156] LC_ALL              : `C.UTF-8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [024/156] LD_LIBRARY_PATH     : `/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [025/156] LD_PRELOAD          : `/libchangehostname.so`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [026/156] LOCAL_RANK          : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [027/156] LOCAL_WORLD_SIZE    : `8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [028/156] MANUAL_BUILD        : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [029/156] MASTER_ADDR         : `127.0.0.1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [030/156] MASTER_PORT         : `29500`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [031/156] NCCL_ASYNC_ERROR_HANDLING: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [032/156] NCCL_DEBUG          : `WARN`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [033/156] NCCL_IB_DISABLE     : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [034/156] NCCL_PROTO          : `simple`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [035/156] NCCL_SOCKET_IFNAME  : `eth0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [036/156] NCCL_VERSION        : `2.19.4`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [037/156] NVARCH              : `x86_64`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [038/156] NVIDIA_DRIVER_CAPABILITIES: `compute,utility,compat32,graphics,video`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [039/156] NVIDIA_REQUIRE_CUDA : `cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [040/156] NVIDIA_VISIBLE_DEVICES: `all`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [041/156] NVTE_FRAMEWORK      : `pytorch`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [042/156] NVTE_TORCH_COMPILE  : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [043/156] NV_CUDA_COMPAT_PACKAGE: `cuda-compat-12-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [044/156] NV_CUDA_CUDART_VERSION: `12.1.55-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [045/156] OMP_NUM_THREADS     : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [046/156] OPEN_MPI_PATH       : `/opt/amazon/openmpi`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [047/156] PATH                : `/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [048/156] PWD                 : `/opt/ml/code`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [049/156] PYTHONDONTWRITEBYTECODE: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [050/156] PYTHONIOENCODING    : `UTF-8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [051/156] PYTHONPATH          : `/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [052/156] PYTHONUNBUFFERED    : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [053/156] RANK                : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [054/156] RDMAV_FORK_SAFE     : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [055/156] REQUESTS_CA_BUNDLE  : `/etc/ssl/certs/ca-certificates.crt`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [056/156] ROLE_NAME           : `default`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [057/156] ROLE_RANK           : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [058/156] ROLE_WORLD_SIZE     : `8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [059/156] SAGEMAKER_JOB_NAME  : `smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [060/156] SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY: `/opt/ml/sagemaker/warmpoolcache`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [061/156] SAGEMAKER_METRICS_DIRECTORY: `/opt/ml/output/metrics/sagemaker`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [062/156] SAGEMAKER_REGION    : `us-east-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [063/156] SAGEMAKER_TRAINING_MODULE: `sagemaker_pytorch_container.training:main`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [064/156] SHLVL               : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [065/156] SM_CHANNELS         : `[\"test\",\"train\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [066/156] SM_CHANNEL_TEST     : `/opt/ml/input/data/test`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [067/156] SM_CHANNEL_TRAIN    : `/opt/ml/input/data/train`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [068/156] SM_CURRENT_HOST     : `algo-1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [069/156] SM_CURRENT_INSTANCE_GROUP: `homogeneousCluster`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [070/156] SM_CURRENT_INSTANCE_GROUP_HOSTS: `[\"algo-1\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [071/156] SM_CURRENT_INSTANCE_TYPE: `ml.p4d.24xlarge`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [072/156] SM_DISTRIBUTION_INSTANCE_GROUPS: `[\"homogeneousCluster\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [073/156] SM_DLC_TORCH_VERSION: `2.2.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [074/156] SM_FRAMEWORK_MODULE : `sagemaker_pytorch_container.training:main`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [075/156] SM_FRAMEWORK_PARAMS : `{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [076/156] SM_HOSTS            : `[\"algo-1\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [077/156] SM_HPS              : `{\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":50,\"clean_cache\":0,\"dataset_type\":\"gpt_jsonl\",\"delayed_param\":1,\"distributed_backend\":\"nccl\",\"enable_memory_profiling\":0,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"NousResearch/Llama-2-7b-chat-hf\",\"hidden_width\":4096,\"limit_all_gathers\":1,\"llama_intermediate_size\":11008,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":4096,\"max_steps\":50,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_layers\":32,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":2,\"use_smp_flash_attn\":1,\"use_smp_implementation\":1,\"val_batch_size\":4,\"validation_freq\":50,\"vocab_size\":50257,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [078/156] SM_HP_ACTIVATION_CHECKPOINTING: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [079/156] SM_HP_AUTO_WRAP_POLICY: `transformer_auto_wrap_policy`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [080/156] SM_HP_BACKWARD_FETCH_POLICY: `backward_pre`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [081/156] SM_HP_BETA1         : `0.9`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [082/156] SM_HP_BETA2         : `0.95`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [083/156] SM_HP_BF16          : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [084/156] SM_HP_CHECKPOINT_DIR: `/opt/ml/checkpoints`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [085/156] SM_HP_CHECKPOINT_FREQ: `50`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [086/156] SM_HP_CLEAN_CACHE   : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [087/156] SM_HP_DATASET_TYPE  : `gpt_jsonl`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [088/156] SM_HP_DELAYED_PARAM : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [089/156] SM_HP_DISTRIBUTED_BACKEND: `nccl`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [090/156] SM_HP_ENABLE_MEMORY_PROFILING: `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [091/156] SM_HP_EPOCHS        : `100`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [092/156] SM_HP_FAST_VALIDATION: `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [093/156] SM_HP_FORWARD_PREFETCH: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [094/156] SM_HP_FP8           : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [095/156] SM_HP_HF_PRETRAINED_MODEL_NAME_OR_DIR: `NousResearch/Llama-2-7b-chat-hf`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [096/156] SM_HP_HIDDEN_WIDTH  : `4096`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [097/156] SM_HP_LIMIT_ALL_GATHERS: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [098/156] SM_HP_LLAMA_INTERMEDIATE_SIZE: `11008`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [099/156] SM_HP_LOGGING_FREQ  : `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [100/156] SM_HP_LR            : `0.0001`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [101/156] SM_HP_LR_DECAY_ITERS: `47683`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [102/156] SM_HP_LR_DECAY_STYLE: `cosine`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [103/156] SM_HP_MAX_CONTEXT_WIDTH: `4096`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [104/156] SM_HP_MAX_STEPS     : `50`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [105/156] SM_HP_MIN_LR        : `1e-05`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [106/156] SM_HP_MODEL_TYPE    : `llama_v2`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [107/156] SM_HP_MP_PARAMETERS : `{\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [108/156] SM_HP_NUM_HEADS     : `32`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [109/156] SM_HP_NUM_KEPT_CHECKPOINTS: `2`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [110/156] SM_HP_NUM_LAYERS    : `32`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [111/156] SM_HP_PLATEAU       : `0.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [112/156] SM_HP_SEED          : `12345`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [113/156] SM_HP_SHARDING_STRATEGY: `hybrid_shard`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [114/156] SM_HP_TRAIN_BATCH_SIZE: `2`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [115/156] SM_HP_USE_SMP_FLASH_ATTN: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [116/156] SM_HP_USE_SMP_IMPLEMENTATION: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [117/156] SM_HP_VALIDATION_FREQ: `50`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [118/156] SM_HP_VAL_BATCH_SIZE: `4`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [119/156] SM_HP_VOCAB_SIZE    : `50257`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [120/156] SM_HP_WARMUP        : `0.0032`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [121/156] SM_HP_WEIGHT_DECAY  : `0.2`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [122/156] SM_HP_ZIPPED_DATA   : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [123/156] SM_INPUT_CONFIG_DIR : `/opt/ml/input/config`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [124/156] SM_INPUT_DATA_CONFIG: `{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [125/156] SM_INPUT_DIR        : `/opt/ml/input`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [126/156] SM_INSTANCE_GROUPS  : `[\"homogeneousCluster\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [127/156] SM_INSTANCE_GROUPS_DICT: `{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [128/156] SM_IS_HETERO        : `false`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [129/156] SM_LOG_LEVEL        : `20`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [130/156] SM_MODEL_DIR        : `/opt/ml/model`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [131/156] SM_MODULE_DIR       : `s3://sagemaker-us-east-1-905418125508/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911/source/sourcedir.tar.gz`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [132/156] SM_MODULE_NAME      : `train`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [133/156] SM_NETWORK_INTERFACE_NAME: `eth0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [134/156] SM_NUM_CPUS         : `96`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [135/156] SM_NUM_GPUS         : `8`\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [136/156] SM_NUM_NEURONS      : `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [137/156] SM_OUTPUT_DATA_DIR  : `/opt/ml/output/data`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [138/156] SM_OUTPUT_DIR       : `/opt/ml/output`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [139/156] SM_OUTPUT_INTERMEDIATE_DIR: `/opt/ml/output/intermediate`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [140/156] SM_RESOURCE_CONFIG  : `{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [141/156] SM_TRAINING_ENV     : `{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":50,\"clean_cache\":0,\"dataset_type\":\"gpt_jsonl\",\"delayed_param\":1,\"distributed_backend\":\"nccl\",\"enable_memory_profiling\":0,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"NousResearch/Llama-2-7b-chat-hf\",\"hidden_width\":4096,\"limit_all_gathers\":1,\"llama_intermediate_size\":11008,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":4096,\"max_steps\":50,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"activation_loading_horizon\":2,\"hybrid_shard_degree\":0,\"sm_activation_offloading\":true,\"tensor_parallel_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_layers\":32,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":2,\"use_smp_flash_attn\":1,\"use_smp_implementation\":1,\"val_batch_size\":4,\"validation_freq\":50,\"vocab_size\":50257,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905418125508/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [142/156] SM_USER_ARGS        : `[\"--activation_checkpointing\",\"1\",\"--auto_wrap_policy\",\"transformer_auto_wrap_policy\",\"--backward_fetch_policy\",\"backward_pre\",\"--beta1\",\"0.9\",\"--beta2\",\"0.95\",\"--bf16\",\"1\",\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--checkpoint_freq\",\"50\",\"--clean_cache\",\"0\",\"--dataset_type\",\"gpt_jsonl\",\"--delayed_param\",\"1\",\"--distributed_backend\",\"nccl\",\"--enable_memory_profiling\",\"0\",\"--epochs\",\"100\",\"--fast_validation\",\"0\",\"--forward_prefetch\",\"1\",\"--fp8\",\"0\",\"--hf_pretrained_model_name_or_dir\",\"NousResearch/Llama-2-7b-chat-hf\",\"--hidden_width\",\"4096\",\"--limit_all_gathers\",\"1\",\"--llama_intermediate_size\",\"11008\",\"--logging_freq\",\"1\",\"--lr\",\"0.0001\",\"--lr_decay_iters\",\"47683\",\"--lr_decay_style\",\"cosine\",\"--max_context_width\",\"4096\",\"--max_steps\",\"50\",\"--min_lr\",\"1e-05\",\"--model_type\",\"llama_v2\",\"--mp_parameters\",\"activation_loading_horizon=2,hybrid_shard_degree=0,sm_activation_offloading=True,tensor_parallel_degree=8\",\"--num_heads\",\"32\",\"--num_kept_checkpoints\",\"2\",\"--num_layers\",\"32\",\"--plateau\",\"0.0\",\"--seed\",\"12345\",\"--sharding_strategy\",\"hybrid_shard\",\"--train_batch_size\",\"2\",\"--use_smp_flash_attn\",\"1\",\"--use_smp_implementation\",\"1\",\"--val_batch_size\",\"4\",\"--validation_freq\",\"50\",\"--vocab_size\",\"50257\",\"--warmup\",\"0.0032\",\"--weight_decay\",\"0.2\",\"--zipped_data\",\"0\"]`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [143/156] SM_USER_ENTRY_POINT : `train.py`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [144/156] TORCHELASTIC_ERROR_FILE: `/tmp/torchelastic_losv00ca/none_1wv7sggw/attempt_0/0/error.json`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [145/156] TORCHELASTIC_MAX_RESTARTS: `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [146/156] TORCHELASTIC_RESTART_COUNT: `0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [147/156] TORCHELASTIC_RUN_ID : `none`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [148/156] TORCHELASTIC_USE_AGENT_STORE: `True`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [149/156] TORCH_CUDA_ARCH_LIST: `5.2;7.0+PTX;7.5;8.0;8.6;9.0`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [150/156] TORCH_NCCL_ASYNC_ERROR_HANDLING: `1`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [151/156] TORCH_NVCC_FLAGS    : `-Xfatbin -compress-all`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [152/156] TRAINING_JOB_ARN    : `arn:aws:sagemaker:us-east-1:905418125508:training-job/smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [153/156] TRAINING_JOB_NAME   : `smp-7b-p4d24x-hs0-aoTrue-bs2-2024-03-27-23-51-11-911`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [154/156] WORLD_SIZE          : `8`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:55] [000] env [155/156] _                   : `/opt/conda/bin/train`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [logging_utils.py:66] [000] env from all nodes `('HOSTNAME', 'SLURM_PROCID')`: `('ip-10-0-253-105.ec2.internal', 'None')`\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [001/075] activation_checkpointing      : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [002/075] attn_pdrop                    : 0.1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [003/075] auto_wrap_policy              : transformer_auto_wrap_policy\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [004/075] backward_fetch_policy         : backward_pre\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [005/075] beta1                         : 0.9\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [006/075] beta2                         : 0.95\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [007/075] bf16                          : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [008/075] checkpoint_dir                : ['/opt/ml/checkpoints']\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [009/075] checkpoint_freq               : [50]\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [010/075] checkpoint_type               : sharded\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [011/075] clean_cache                   : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [012/075] data_num_workers              : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [013/075] data_type                     : gpt\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [014/075] dataset_type                  : gpt_jsonl\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [015/075] delayed_param                 : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [016/075] distributed_backend           : nccl\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [017/075] embd_pdrop                    : 0.1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [018/075] enable_memory_profiling       : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [019/075] epochs                        : 100\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [020/075] fast_validation               : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [021/075] forward_prefetch              : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [022/075] fp8                           : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [023/075] fp8_amax_compute_algo         : max\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [024/075] fp8_amax_history_len          : 1024\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [025/075] framework                     : fsdp\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [026/075] grad_clip                     : 1.0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [027/075] hf_pretrained_model_name_or_dir: NousResearch/Llama-2-7b-chat-hf\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [028/075] hidden_width                  : 4096\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [029/075] initializer_range             : 0.02\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [030/075] limit_all_gathers             : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [031/075] llama_intermediate_size       : 11008\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [032/075] log_reduced_training_loss     : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [033/075] logging_freq                  : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [034/075] logging_freq_for_avg          : 50\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [035/075] lr                            : 0.0001\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [036/075] lr_decay_iters                : 47683\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [037/075] lr_decay_style                : cosine\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [038/075] max_context_width             : 4096\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [039/075] max_steps                     : 50\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [040/075] min_lr                        : 1e-05\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [041/075] model_dir                     : None\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [042/075] model_type                    : llama_v2\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [043/075] nccl_test_log                 : \u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [044/075] num_heads                     : 32\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [045/075] num_kept_checkpoints          : [2]\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [046/075] num_key_value_heads           : None\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [047/075] num_layers                    : 32\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [048/075] patch_neox_rope               : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [049/075] plateau                       : 0.0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [050/075] preserve_np_state             : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [051/075] profile_nsys                  : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [052/075] resid_pdrop                   : 0.1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [053/075] resume_from_checkpoint        : None\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [054/075] rotary_emb_base               : 10000\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [055/075] rotary_pct                    : 0.25\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [056/075] same_seed                     : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [057/075] save_final_model              : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [058/075] seed                          : 12345\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [059/075] sharding_strategy             : hybrid_shard\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [060/075] summary_first_pdrop           : 0.1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [061/075] tensorboard_dir               : None\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [062/075] test_dir                      : /opt/ml/input/data/test\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [063/075] train_batch_size              : 2\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [064/075] training_dir                  : /opt/ml/input/data/train\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [065/075] use_orig_params               : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [066/075] use_smp_flash_attn            : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [067/075] use_smp_implementation        : 1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [068/075] use_synthetic_data            : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [069/075] val_batch_size                : 4\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [070/075] validation_batches            : 10\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [071/075] validation_freq               : 50\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [072/075] vocab_size                    : 50257\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [073/075] warmup                        : 0.0032\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [074/075] weight_decay                  : 0.2\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:442] Arguments [075/075] zipped_data                   : 0\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:443] Transformers version: 4.37.1\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:444] World size = 8: # nodes = 1.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_lib.py:452] Global batch size in tokens:       8192 ( 0.01M).\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:49 I [train_utils.py:112] Loading pretrained weights from NousResearch/Llama-2-7b-chat-hf.\u001b[0m\n",
            "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:00:50 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:37<00:37, 37.60s/it]\u001b[0m\n",
            "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:50<00:00, 23.16s/it]\u001b[0m\n",
            "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:50<00:00, 25.33s/it]\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\u001b[0m\n",
            "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\u001b[0m\n",
            "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.65s/it]\u001b[0m\n",
            "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.24s/it]\u001b[0m\n",
            "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.90s/it]\u001b[0m\n",
            "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\u001b[0m\n",
            "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\u001b[0m\n",
            "\u001b[34m2024-03-28 00:01:56 I [train_utils.py:125] For transformers greater than or equal to 4.37.1, automatically use integrated flash attn.\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34malgo-1:98:213 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:100:217 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:101:216 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:103:220 [5] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:104:219 [6] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:102:214 [4] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:105:215 [7] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34malgo-1:99:218 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:23.923: I torch/sagemaker/tensor_parallel/parallelize.py:73] Created transformed model\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:32.853: I torch/sagemaker/comm.py:127] Scattered slices of params to other ranks in tp group 0\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.335: I torch/sagemaker/tensor_parallel/parallelize.py:107] [0] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.453: I torch/sagemaker/utils/utils.py:52] [ 0] Runtime is      105.1 seconds: Model creation.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:34 I [train_lib.py:525] Created model with total parameters: 6738415616 (6.74 B)\u001b[0m\n",
            "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.665: I torch/sagemaker/tensor_parallel/parallelize.py:107] [6] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.676: I torch/sagemaker/tensor_parallel/parallelize.py:107] [5] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.679: I torch/sagemaker/tensor_parallel/parallelize.py:107] [1] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.680: I torch/sagemaker/tensor_parallel/parallelize.py:107] [4] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.848: I torch/sagemaker/tensor_parallel/parallelize.py:107] [7] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.865: I torch/sagemaker/tensor_parallel/parallelize.py:107] [3] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:34.883: I torch/sagemaker/tensor_parallel/parallelize.py:107] [2] Loaded transformed state dict\u001b[0m\n",
            "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:35.537: I torch/sagemaker/utils/utils.py:52] [ 0] Runtime is      1.082 seconds: FSDP constructor.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:36 I [train_lib.py:589] Wrapped model with FSDP\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:02:36.244: W torch/sagemaker/offload.py:160] Using SM offloader. Ensure offload_wrapper is only called on the root model\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:36 I [train_lib.py:617] Created optimizer\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:36 I [train_lib.py:229] Creating train dataloader\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:36 I [train_lib.py:243] Starting training with epoch 0.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:40 I [logging_utils.py:135] Batch 0 Loss: 5.125, Speed: 0.43 samples/sec, Model TFLOPS/GPU: 8.83, lr: 0.000001, gradnorm: 40.2140\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:41 I [logging_utils.py:135] Batch 1 Loss: 5.03125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.78, lr: 0.000001, gradnorm: 36.4508\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:42 I [logging_utils.py:135] Batch 2 Loss: 5.0, Speed: 3.25 samples/sec, Model TFLOPS/GPU: 67.35, lr: 0.000002, gradnorm: 42.0351\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:42 I [logging_utils.py:135] Batch 3 Loss: 4.9375, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.74, lr: 0.000003, gradnorm: 41.9803\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:43 I [logging_utils.py:135] Batch 4 Loss: 4.875, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.53, lr: 0.000003, gradnorm: 36.8082\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:44 I [logging_utils.py:135] Batch 5 Loss: 4.65625, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.67, lr: 0.000004, gradnorm: 35.5719\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:44 I [logging_utils.py:135] Batch 6 Loss: 4.21875, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.40, lr: 0.000005, gradnorm: 35.2843\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:45 I [logging_utils.py:135] Batch 7 Loss: 4.03125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.78, lr: 0.000005, gradnorm: 37.8609\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:45 I [logging_utils.py:135] Batch 8 Loss: 3.578125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.79, lr: 0.000006, gradnorm: 28.0719\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:46 I [logging_utils.py:135] Batch 9 Loss: 3.453125, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.48, lr: 0.000007, gradnorm: 28.9580\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:47 I [logging_utils.py:135] Batch 10 Loss: 3.453125, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.46, lr: 0.000007, gradnorm: 26.8257\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:47 I [logging_utils.py:135] Batch 11 Loss: 3.375, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.87, lr: 0.000008, gradnorm: 23.3213\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:48 I [logging_utils.py:135] Batch 12 Loss: 3.265625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.86, lr: 0.000009, gradnorm: 11.2322\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:48 I [logging_utils.py:135] Batch 13 Loss: 3.28125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.81, lr: 0.000009, gradnorm: 12.4183\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:49 I [logging_utils.py:135] Batch 14 Loss: 3.09375, Speed: 3.24 samples/sec, Model TFLOPS/GPU: 67.15, lr: 0.000010, gradnorm: 8.9073\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:50 I [logging_utils.py:135] Batch 15 Loss: 3.125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.88, lr: 0.000010, gradnorm: 9.7903\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:50 I [logging_utils.py:135] Batch 16 Loss: 3.046875, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.87, lr: 0.000011, gradnorm: 11.3048\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:51 I [logging_utils.py:135] Batch 17 Loss: 2.953125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.69, lr: 0.000012, gradnorm: 4.4535\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:52 I [logging_utils.py:135] Batch 18 Loss: 2.9375, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.60, lr: 0.000012, gradnorm: 5.3293\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:52 I [logging_utils.py:135] Batch 19 Loss: 2.890625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.94, lr: 0.000013, gradnorm: 8.9350\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:53 I [logging_utils.py:135] Batch 20 Loss: 2.984375, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.83, lr: 0.000014, gradnorm: 11.9355\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:53 I [logging_utils.py:135] Batch 21 Loss: 2.890625, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.78, lr: 0.000014, gradnorm: 8.2558\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:54 I [logging_utils.py:135] Batch 22 Loss: 2.859375, Speed: 3.25 samples/sec, Model TFLOPS/GPU: 67.27, lr: 0.000015, gradnorm: 4.6224\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:55 I [logging_utils.py:135] Batch 23 Loss: 2.828125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.93, lr: 0.000016, gradnorm: 2.8973\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:55 I [logging_utils.py:135] Batch 24 Loss: 2.8125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.81, lr: 0.000016, gradnorm: 3.5439\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:56 I [logging_utils.py:135] Batch 25 Loss: 2.78125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.73, lr: 0.000017, gradnorm: 5.8463\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:56 I [logging_utils.py:135] Batch 26 Loss: 2.75, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.53, lr: 0.000018, gradnorm: 4.0866\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:57 I [logging_utils.py:135] Batch 27 Loss: 2.71875, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.66, lr: 0.000018, gradnorm: 3.2313\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:58 I [logging_utils.py:135] Batch 28 Loss: 2.734375, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.87, lr: 0.000019, gradnorm: 5.4272\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:58 I [logging_utils.py:135] Batch 29 Loss: 2.734375, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.80, lr: 0.000020, gradnorm: 3.3056\u001b[0m\n",
            "\u001b[34m2024-03-28 00:02:59 I [logging_utils.py:135] Batch 30 Loss: 2.75, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.54, lr: 0.000020, gradnorm: 2.7126\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:00 I [logging_utils.py:135] Batch 31 Loss: 2.640625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.89, lr: 0.000021, gradnorm: 3.8950\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:00 I [logging_utils.py:135] Batch 32 Loss: 2.640625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.85, lr: 0.000022, gradnorm: 2.7880\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:01 I [logging_utils.py:135] Batch 33 Loss: 2.65625, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.74, lr: 0.000022, gradnorm: 3.9386\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:01 I [logging_utils.py:135] Batch 34 Loss: 2.625, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.50, lr: 0.000023, gradnorm: 2.7383\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:02 I [logging_utils.py:135] Batch 35 Loss: 2.640625, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.67, lr: 0.000024, gradnorm: 2.8962\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:03 I [logging_utils.py:135] Batch 36 Loss: 2.546875, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.87, lr: 0.000024, gradnorm: 2.3895\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:03 I [logging_utils.py:135] Batch 37 Loss: 2.546875, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.66, lr: 0.000025, gradnorm: 3.0959\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:04 I [logging_utils.py:135] Batch 38 Loss: 2.5625, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.51, lr: 0.000026, gradnorm: 2.9078\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:04 I [logging_utils.py:135] Batch 39 Loss: 2.5625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.89, lr: 0.000026, gradnorm: 2.8839\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:05 I [logging_utils.py:135] Batch 40 Loss: 2.453125, Speed: 3.25 samples/sec, Model TFLOPS/GPU: 67.32, lr: 0.000027, gradnorm: 2.6455\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:06 I [logging_utils.py:135] Batch 41 Loss: 2.453125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.60, lr: 0.000028, gradnorm: 2.6954\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:06 I [logging_utils.py:135] Batch 42 Loss: 2.484375, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.49, lr: 0.000028, gradnorm: 2.9013\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:07 I [logging_utils.py:135] Batch 43 Loss: 2.453125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.80, lr: 0.000029, gradnorm: 2.6873\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:08 I [logging_utils.py:135] Batch 44 Loss: 2.390625, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.86, lr: 0.000029, gradnorm: 2.8287\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:08 I [logging_utils.py:135] Batch 45 Loss: 2.390625, Speed: 3.25 samples/sec, Model TFLOPS/GPU: 67.34, lr: 0.000030, gradnorm: 2.7117\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:09 I [logging_utils.py:135] Batch 46 Loss: 2.390625, Speed: 3.26 samples/sec, Model TFLOPS/GPU: 67.52, lr: 0.000031, gradnorm: 2.5780\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:09 I [logging_utils.py:135] Batch 47 Loss: 2.3125, Speed: 3.28 samples/sec, Model TFLOPS/GPU: 67.81, lr: 0.000031, gradnorm: 2.7116\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:10 I [logging_utils.py:135] Batch 48 Loss: 2.328125, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.77, lr: 0.000032, gradnorm: 2.7160\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:11 I [logging_utils.py:135] Batch 49 Loss: 2.390625, Speed: 3.27 samples/sec, Model TFLOPS/GPU: 67.73, lr: 0.000033, gradnorm: 5.9859\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:11 I [logging_utils.py:152] Batch 49 Running Avg Speed: 3.27 samples/sec, Running Avg Model TFLOPS/GPU: 67.67\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:12 I [logging_utils.py:166] Batch 49 Validation loss: 2.29375\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:12 I [logging_utils.py:171] Batch 49 Validation perplexity: 9.912038217506028\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:12 I [checkpoints.py:268] Checkpointing to /opt/ml/checkpoints/llama_v2-50steps ...\u001b[0m\n",
            "\u001b[34m[rank0]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.02263427299999421, 'preprocessing_with_comm': 0.004921276000004582, 'state_converting': 0.000187153000069884, <Type.ALL: 'all'>: 0.029680940000048395})\u001b[0m\n",
            "\u001b[34m[rank6]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.022840361000021403, 'preprocessing_with_comm': 0.004148181999994449, 'state_converting': 0.0002049199999873963, <Type.ALL: 'all'>: 0.02917964000005213})\u001b[0m\n",
            "\u001b[34m[rank3]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.022747187000049962, 'preprocessing_with_comm': 0.005370066999944356, 'state_converting': 0.00019500299993069348, <Type.ALL: 'all'>: 0.030255716000056054})\u001b[0m\n",
            "\u001b[34m[rank5]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.02271965399995679, 'preprocessing_with_comm': 0.0040047019999747135, 'state_converting': 0.00020356200002424885, <Type.ALL: 'all'>: 0.028975733000038417})\u001b[0m\n",
            "\u001b[34m[rank4]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.02279435499997362, 'preprocessing_with_comm': 0.004165398000054665, 'state_converting': 0.00020401000006131653, <Type.ALL: 'all'>: 0.02912775300001158})\u001b[0m\n",
            "\u001b[34m[rank2]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.022740408999993633, 'preprocessing_with_comm': 0.0053609409999353375, 'state_converting': 0.00020158699999228702, <Type.ALL: 'all'>: 0.030260188000056587})\u001b[0m\n",
            "\u001b[34m[rank1]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.022912783000037962, 'preprocessing_with_comm': 0.0036604469999019784, 'state_converting': 0.00020954199999323464, <Type.ALL: 'all'>: 0.02884144400002242})\u001b[0m\n",
            "\u001b[34m[rank7]:[2024-03-28 00:03:12,595] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.022828199000059612, 'preprocessing_with_comm': 0.004131513999936942, 'state_converting': 0.00020329400001628528, <Type.ALL: 'all'>: 0.02913848000002872})\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:12 I [checkpoints.py:166] Processed state dict to save. Starting write to disk now.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:32 I [checkpoints.py:303] Finished checkpointing to /opt/ml/checkpoints/llama_v2-50steps/tp8-0.\u001b[0m\n",
            "\u001b[34m[2024-03-28 00:03:32.876: I torch/sagemaker/checkpoint/utils.py:38] Number of dirs matching regex (`^.*\\d+steps$`) is 1: `/opt/ml/checkpoints`.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:32 I [train_lib.py:705] FSDP training finished successfully 56.634421s (0.943907min) out of (2.733489min).\u001b[0m\n",
            "\n",
            "2024-03-28 00:03:50 Uploading - Uploading generated training model\u001b[34m2024-03-28 00:03:41,919 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:41,919 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
            "\u001b[34m2024-03-28 00:03:41,919 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
            "\n",
            "2024-03-28 00:05:46 Completed - Training job completed\n",
            "Training seconds: 583\n",
            "Billable seconds: 583\n"
          ]
        }
      ],
      "source": [
        "smp_estimator.fit(inputs=data_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70j7wrwYmpXr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "2_4i009AmpXr"
      },
      "source": [
        "## Accessing the launched SM training job\n",
        "You can access the launched training job from [SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html).  \n",
        "Go to `Amazon SageMaker -> Training -> Training jobs`.  \n",
        "You can also access the training logs from here with `View Logs` which opens CloudWatch directly.\n",
        "\n",
        "## Accessing the Training Logs\n",
        "\n",
        "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html).\n",
        "\n",
        "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see [SageMaker Jobs and Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) in the Amazon SageMaker Developer Guide.\n",
        "\n",
        "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html).\n",
        "\n",
        "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
        "\n",
        "## Deploying Trained Model for Inference\n",
        "\n",
        "In most cases, a trained model can be deployed on a single device for inference because inference only requires a small amount of memory.\n",
        "\n",
        "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
        "\n",
        "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
        "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker Batch Transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
        "\n",
        "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLS9-tejmpXs"
      },
      "source": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (Data Science 3.0)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}